<!DOCTYPE html><script type="module" src="/_astro/index.astro_astro_type_script_index_0_lang.CoXd8L9s.js"></script> <html lang="en-us" dir="ltr"> <head><script async src="https://ot.www.cloudflare.com/public/vendor/onetrust/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" data-domain-script="b1e05d49-f072-4bae-9116-bdb78af15448"></script><script type="text/javascript">
    window.loadScript = (url, { charset = 'UTF-8', ...attributes } = {}, location = 'head') => {
      const existingScript = document.querySelector('script[src="' + url + '"]');
      if (existingScript) {
        return;
      }

      const script = document.createElement('script');
      script.src = url;
      script.type = 'text/javascript';

      script.async = true;
      script.charset = charset;

      if (attributes) {
        Object.entries(attributes).forEach(([key, value]) => {
          script.setAttribute(key, value);
        });
      }

      if (location === 'head') {
        document.head.appendChild(script);
      } else {
        document.body.appendChild(script);
      }

      return script;
    };
  </script><meta name="HandheldFriendly" content="True"><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="KeThzeyMOr"><meta name="baidu-site-verification" content="code-NIlrS7gNhx"><meta charset="UTF-8"><meta name="description" content="Cloudflare's AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications."><title>Block unsafe prompts targeting your LLM endpoints with Firewall for AI</title><meta name="title" content="Block unsafe prompts targeting your LLM endpoints with Firewall for AI"><meta name="msvalidate.01" content="CF295E1604697F9CAD18B5A232E871F6"><meta class="swiftype" name="language" data-type="string" content="en"><script type="text/javascript">
    function OptanonWrapper() {
      if (!window.zarazTagManager.loaded && window.OnetrustActiveGroups.includes('C0004')) {
        window.zarazTagManager.loadZaraz();
      }

      if (window.zaraz && !window.OnetrustActiveGroups.includes('C0004')) {
        window.zaraz = undefined;
        window.zarazData = undefined;
        window.zarazTagManager.loaded = false;
      }

      if (!window.isCfCookieEventAttached) {
        window.isCfCookieEventAttached = true;
        window.addEventListener('consent.onetrust', async () => {
          await fetch('/', { method: 'HEAD' });
        });
      }
    }
    window.zarazTagManager = {
      loaded: false,
      getEndPoint() {
        return '/static/z/i.js';
      },
      loadZaraz() {
        window.loadScript(window.zarazTagManager.getEndPoint());
        window.zarazTagManager.loaded = true;
      },
    };
  </script><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-32x32.png"><link rel="mask-icon" href="/images/favicon-32x32.png" color="#f78100"><link rel="stylesheet" href="/themes/ashes.min.css"><link rel="sitemap" href="/sitemap.xml"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"><link rel="canonical" href="https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/"><link rel="alternate" hreflang="en-us" href="https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/"><!-- General Meta Tags --><meta property="article:published_time" content="2025-08-26T14:00+00:00"><meta property="article:modified_time" content="2025-08-26T13:00:37.849Z"><meta property="article:tag" content="AI Week"><meta property="article:tag" content="Security"><meta property="article:tag" content="LLM"><meta property="article:tag" content="WAF"><meta property="article:tag" content="AI"><meta property="article:publisher" content="https://www.facebook.com/cloudflare"><!-- Facebook Meta Tags --><meta property="og:site_name" content="The Cloudflare Blog"><meta property="og:type" content="article"><meta property="og:title" content="Block unsafe prompts targeting your LLM endpoints with Firewall for AI"><meta property="og:description" content="Cloudflare's AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications."><meta property="og:url" content="https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="628"><!-- Twitter/X Meta Tags --><meta name="twitter:title" content="Block unsafe prompts targeting your LLM endpoints with Firewall for AI"><meta name="twitter:description" content="Cloudflare's AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications."><meta name="twitter:url" content="https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:label1" content="Written by"><meta name="twitter:data1" content="Radwa Radwan"><meta name="twitter:creator" content="@RadwaRadwan__"><meta name="twitter:label2" content="Filed under"><meta name="twitter:data2" content="AI Week,Security,LLM,WAF,AI"><meta name="twitter:site" content="@cloudflare"><meta property="og:image" content="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5q7ynAaKyGB8eed0B3AXNv/921572b388e22cde586f440cb70109e8/OG_Share_2024__78_.png"><meta name="twitter:image" content="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5q7ynAaKyGB8eed0B3AXNv/921572b388e22cde586f440cb70109e8/OG_Share_2024__78_.png"><link rel="stylesheet" href="/_astro/index.CgcMBmOJ.css"></head><style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).only=e;window.dispatchEvent(new Event("astro:only"));})();;(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="ZQVHGD" component-url="/_astro/GoogleAnalytics.CXpa9INe.js" component-export="GoogleAnalytics" renderer-url="/_astro/client.CZEEmA8m.js" props="{&quot;title&quot;:[0,&quot;Block unsafe prompts targeting your LLM endpoints with Firewall for AI&quot;],&quot;canonical&quot;:[0,&quot;https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;info&quot;:[0,{&quot;id&quot;:[0,&quot;59hk6A3nH3YcLMjXhYnNof&quot;],&quot;title&quot;:[0,&quot;Block unsafe prompts targeting your LLM endpoints with Firewall for AI&quot;],&quot;slug&quot;:[0,&quot;block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;excerpt&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. &quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;Security teams are racing to &lt;a href=\&quot;https://www.cloudflare.com/the-net/vulnerable-llm-ai/\&quot;&gt;&lt;u&gt;secure a new attack surface&lt;/u&gt;&lt;/a&gt;: AI-powered applications. From chatbots to search assistants, LLMs are already shaping customer experience, but they also open the door to new risks. A single malicious prompt can exfiltrate sensitive data, &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/data-poisoning/\&quot;&gt;&lt;u&gt;poison a model&lt;/u&gt;&lt;/a&gt;, or inject toxic content into customer-facing interactions, undermining user trust. Without guardrails, even the best-trained model can be turned against the business.&lt;/p&gt;&lt;p&gt;Today, as part of AI Week, weâre expanding our AI security offerings by introducing unsafe content moderation, now integrated directly into Cloudflare &lt;a href=\&quot;https://developers.cloudflare.com/waf/detections/firewall-for-ai/\&quot;&gt;&lt;u&gt;Firewall for AI&lt;/u&gt;&lt;/a&gt;. Built with Llama, this new feature allows customers to leverage their existing Firewall for AI engine for unified detection, analytics, and topic enforcement, providing real-time protection for &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/what-is-large-language-model/\&quot;&gt;&lt;u&gt;Large Language Models (LLMs)&lt;/u&gt;&lt;/a&gt; at the network level. Now with just a few clicks, security and application teams can detect and block harmful prompts or topics at the edge â eliminating the need to modify application code or infrastructure.\n\nThis feature is immediately available to current Firewall for AI users. Those not yet onboarded can contact their account team to participate in the beta program.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;ai-protection-in-application-security\&quot;&gt;AI protection in application security&lt;/h2&gt;\n      &lt;a href=\&quot;#ai-protection-in-application-security\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Cloudflare&amp;#39;s Firewall for AI protects user-facing LLM applications from abuse and data leaks, addressing several of the &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/owasp-top-10-risks-for-llms/\&quot;&gt;&lt;u&gt;OWASP Top 10 LLM risks&lt;/u&gt;&lt;/a&gt; such as prompt injection, PII disclosure, and unbound consumption. It also extends protection to other risks such as unsafe or harmful content.&lt;/p&gt;&lt;p&gt;Unlike built-in controls that vary between model providers, Firewall for AI is model-agnostic. It sits in front of any model you choose, whether itâs from a third party like OpenAI or Gemini, one you run in-house, or a custom model you have built, and applies the same consistent protections.&lt;/p&gt;&lt;p&gt;Just like our origin-agnostic &lt;a href=\&quot;https://www.cloudflare.com/application-services/#application-services-case-products\&quot;&gt;&lt;u&gt;Application Security suite&lt;/u&gt;&lt;/a&gt;, Firewall for AI enforces policies at scale across all your models, creating a unified security layer. That means you can define guardrails once and apply them everywhere. For example, a financial services company might require its LLM to only respond to finance-related questions, while blocking prompts about unrelated or sensitive topics, enforced consistently across every model in use.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;unsafe-content-moderation-protects-businesses-and-users\&quot;&gt;Unsafe content moderation protects businesses and users&lt;/h2&gt;\n      &lt;a href=\&quot;#unsafe-content-moderation-protects-businesses-and-users\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Effective AI moderation is more than blocking âbad wordsâ, itâs about setting boundaries that protect users, meeting legal obligations, and preserving brand integrity, without over-moderating in ways that silence important voices.&lt;/p&gt;&lt;p&gt;Because LLMs cannot be fully scripted, their interactions are inherently unpredictable. This flexibility enables rich user experiences but also opens the door to abuse.&lt;/p&gt;&lt;p&gt;Key risks from unsafe prompts include misinformation, biased or offensive content, and model poisoning, where repeated harmful prompts degrade the quality and safety of future outputs. Blocking these prompts aligns with the OWASP Top 10 for LLMs, preventing both immediate misuse and long-term degradation.&lt;/p&gt;&lt;p&gt;One example of this is&lt;a href=\&quot;https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\&quot;&gt; &lt;b&gt;&lt;u&gt;Microsoftâs Tay chatbot&lt;/u&gt;&lt;/b&gt;&lt;/a&gt;. Trolls deliberately submitted toxic, racist, and offensive prompts, which Tay quickly began repeating. The failure was not only in Tayâs responses; it was in the lack of moderation on the inputs it accepted.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;detecting-unsafe-prompts-before-reaching-the-model\&quot;&gt;Detecting unsafe prompts before reaching the model&lt;/h2&gt;\n      &lt;a href=\&quot;#detecting-unsafe-prompts-before-reaching-the-model\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Cloudflare has integrated &lt;a href=\&quot;https://huggingface.co/meta-llama/Llama-Guard-3-8B\&quot;&gt;&lt;u&gt;Llama Guard&lt;/u&gt;&lt;/a&gt; directly into Firewall for AI. This brings AI input moderation into the same rules engine our customers already use to protect their applications. It uses the same approach that we created for developers building with AI in our &lt;a href=\&quot;https://blog.cloudflare.com/guardrails-in-ai-gateway/\&quot;&gt;&lt;u&gt;AI Gateway&lt;/u&gt;&lt;/a&gt; product.&lt;/p&gt;&lt;p&gt;Llama Guard analyzes prompts in real time and flags them across multiple safety categories, including hate, violence, sexual content, criminal planning, self-harm, and more.&lt;/p&gt;&lt;p&gt;With this integration, Firewall for AI not only &lt;a href=\&quot;https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#discovering-llm-powered-applications\&quot;&gt;&lt;u&gt;discovers LLM traffic&lt;/u&gt;&lt;/a&gt; endpoints automatically, but also enables security and AI teams to take immediate action. Unsafe prompts can be blocked before they reach the model, while flagged content can be logged or reviewed for oversight and tuning. Content safety checks can also be combined with other Application Security protections, such as &lt;a href=\&quot;https://www.cloudflare.com/application-services/products/bot-management/\&quot;&gt;&lt;u&gt;Bot Management&lt;/u&gt; &lt;/a&gt;and &lt;a href=\&quot;https://www.cloudflare.com/application-services/products/rate-limiting/\&quot;&gt;&lt;u&gt;Rate Limiting&lt;/u&gt;&lt;/a&gt;, to create layered defenses when protecting your model.&lt;/p&gt;&lt;p&gt;The result is a single, edge-native policy layer that enforces guardrails before unsafe prompts ever reach your infrastructure â without needing complex integrations.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;how-it-works-under-the-hood\&quot;&gt;How it works under the hood&lt;/h2&gt;\n      &lt;a href=\&quot;#how-it-works-under-the-hood\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Before diving into the architecture of Firewall for AI engine and how it fits within our previously mentioned module to detect &lt;a href=\&quot;https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#using-workers-ai-to-deploy-presidio\&quot;&gt;&lt;u&gt;PII in the prompts&lt;/u&gt;&lt;/a&gt;, letâs start with how we detect unsafe topics.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;detection-of-unsafe-topics\&quot;&gt;Detection of unsafe topics&lt;/h3&gt;\n      &lt;a href=\&quot;#detection-of-unsafe-topics\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;A key challenge in building safety guardrails is balancing a good detection with model helpfulness. If detection is too broad, it can prevent a model from answering legitimate user questions, hurting its utility. This is especially difficult for topic detection because of the ambiguity and dynamic nature of human language, where context is fundamental to meaning.Â &lt;/p&gt;&lt;p&gt;Simple approaches like keyword blocklists are interesting for precise subjects â but insufficient. They are easily bypassed and fail to understand the context in which words are used, leading to poor recall. Older probabilistic models such as &lt;a href=\&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\&quot;&gt;&lt;u&gt;Latent Dirichlet Allocation (LDA)&lt;/u&gt;&lt;/a&gt; were an improvement, but did not properly account for word ordering and other contextual nuances. \n\nRecent advancements in LLMs introduced a new paradigm. Their ability to perform zero-shot or few-shot classification is uniquely suited for the task of topic detection. For this reason, we chose &lt;a href=\&quot;https://huggingface.co/meta-llama/Llama-Guard-3-8B\&quot;&gt;&lt;u&gt;Llama Guard 3&lt;/u&gt;&lt;/a&gt;, an open-source model based on the Llama architecture that is specifically fine-tuned for content safety classification. When it analyzes a prompt, it answers whether the text is safe or unsafe, and provides a specific category. We are showing the default categories, as listed &lt;a href=\&quot;http://developers.cloudflare.com/ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_categories/\&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;. Because Llama 3 has a fixed knowledge cutoff, certain categories â like defamation or elections â are time-sensitive. As a result, the model may not fully capture events or context that emerged after it was trained, and thatâs important to keep in mind when relying on it.&lt;/p&gt;&lt;p&gt;For now, we cover the 13 default categories. We plan to expand coverage in the future, leveraging the modelâs zero-shot capabilities.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;a-scalable-architecture-for-future-detections\&quot;&gt;A scalable architecture for future detections&lt;/h3&gt;\n      &lt;a href=\&quot;#a-scalable-architecture-for-future-detections\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;We designed Firewall for AI to scale without adding noticeable latency, including Llama Guard, and this remains true even as we add new detection models.&lt;/p&gt;&lt;p&gt;To achieve this, we built a new asynchronous architecture. When a request is sent to an application protected by Firewall for AI, a Cloudflare Worker makes parallel, non-blocking requests to our different detection modules â one for PII, one for unsafe topics, and others as we add them.Â &lt;/p&gt;&lt;p&gt;Thanks to the Cloudflare network, this design scales to handle high request volumes out of the box, and latency does not increase as we add new detections. It will only be bounded by the slowest model used.Â &lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4Y2gTP6teVR2263UIEWHc9/9a31fb394cee6c437c1d4af6f71d867c/image3.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1924\&quot; height=\&quot;1300\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;We optimize to keep the model utility at its maximum while keeping the guardrail detection broad enough.&lt;/p&gt;&lt;p&gt;Llama Guard is a rather large model, so running it at scale with minimal latency is a challenge. We deploy it on &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt;, leveraging our large fleet of high performance GPUs. This infrastructure ensures we can offer fast, reliable inference throughout our network.&lt;/p&gt;&lt;p&gt;To ensure the system remains fast and reliable as adoption grows, we ran extensive load tests simulating the requests per second (RPS) we anticipate, using a wide range of prompt sizes to prepare for real-world traffic. To handle this, the number of model instances deployed on our network scales automatically with the load. We employ concurrency to minimize latency and optimize for hardware utilization. We also enforce a hard 2-second threshold for each analysis; if this time limit is reached, we fall back to any detections already completed, ensuring your application&amp;#39;s requests latency is never further impacted.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;from-detection-to-security-rules-enforcement\&quot;&gt;From detection to security rules enforcement&lt;/h3&gt;\n      &lt;a href=\&quot;#from-detection-to-security-rules-enforcement\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Firewall for AI follows the same familiar pattern as other Application Security features like Bot Management and WAF Attack Score, making it easy to adopt.&lt;/p&gt;&lt;p&gt;Once enabled, the &lt;a href=\&quot;https://developers.cloudflare.com/waf/detections/firewall-for-ai/#fields\&quot;&gt;&lt;u&gt;new fields&lt;/u&gt;&lt;/a&gt; appear in &lt;a href=\&quot;https://developers.cloudflare.com/waf/analytics/security-analytics/\&quot;&gt;&lt;u&gt;Security Analytics&lt;/u&gt;&lt;/a&gt; and expanded logs. From there, you can filter by unsafe topics, track trends over time, and drill into the results of individual requests to see all detection outcomes, for example: did we detect unsafe topics, and what are the categories. The request body itself (the prompt text) is not stored or exposed; only the results of the analysis are logged.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/722JxyLvT6DFQxFpQhHMYP/3f1a6aa8ef1dafe4ad1a8277578fd7ae/image2.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1125\&quot; height=\&quot;818\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;After reviewing the analytics, you can enforce unsafe topic moderation by creating rules to log or block based on prompt categories in &lt;a href=\&quot;https://developers.cloudflare.com/waf/custom-rules/\&quot;&gt;&lt;u&gt;Custom rules&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For example, you might log prompts flagged as sexual content or hate speech for review.Â &lt;/p&gt;&lt;p&gt;You can use this expression: \n&lt;code&gt;If (any(cf.llm.prompt.unsafe_topic_categories[*] in {&amp;quot;S10&amp;quot; &amp;quot;S12&amp;quot;})) then Log&lt;/code&gt;\n\nOr deploy the rule with the categories field in the dashboard as in the below screenshot.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2CUsVjjpCEqv2UQMU6cMmt/5307235338c1b58856c0685585347537/image4.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1113\&quot; height=\&quot;498\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;You can also take a broader approach by blocking all unsafe prompts outright:\n&lt;code&gt;If (cf.llm.prompt.unsafe_topic_detected)then Block&lt;/code&gt;&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3uRT9YlRlRPsL5bNyBFA3i/54eb171ecb48aaecc7876b972789bf15/image5.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1169\&quot; height=\&quot;628\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;These rules are applied automatically to all discovered HTTP requests containing prompts, ensuring guardrails are enforced consistently across your AI traffic.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;whats-next\&quot;&gt;Whatâs Next&lt;/h2&gt;\n      &lt;a href=\&quot;#whats-next\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;In the coming weeks, Firewall for AI will expand to detect prompt injection and jailbreak attempts. We are also exploring how to add more visibility in the analytics and logs, so teams can better validate detection results. A major part of our roadmap is adding model response handling, giving you control over not only what goes into the LLM but also what comes out. Additional abuse controls, such as rate limiting on tokens and support for more safety categories, are also on the way.&lt;/p&gt;&lt;p&gt;Firewall for AI is available in beta today. If youâre new to Cloudflare and want to explore how to implement these AI protections, &lt;a href=\&quot;https://www.cloudflare.com/plans/enterprise/contact/?utm_medium=referral&amp;utm_source=blog&amp;utm_campaign=2025-q3-acq-gbl-connectivity-ge-ge-general-ai_week_blog\&quot;&gt;&lt;u&gt;reach out for a consultation&lt;/u&gt;&lt;/a&gt;. If youâre already with Cloudflare, contact your account team to get access and start testing with real traffic.&lt;/p&gt;&lt;p&gt;Cloudflare is also opening up a user research program focused on AI security. If you are curious about previews of new functionality or want to help shape our roadmap, &lt;a href=\&quot;https://www.cloudflare.com/lp/ai-security-user-research-program-2025\&quot;&gt;&lt;u&gt;express your interest here&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-26T14:00+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-26T13:00:37.849Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1VoeWz4uYyUO0N70CkENqB/eaf25c3382f436ba3d3e1b27da334cbc/image1.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Mp7ouACN2rT3YjL1xaXJx&quot;],&quot;name&quot;:[0,&quot;Security&quot;],&quot;slug&quot;:[0,&quot;security&quot;]}],[0,{&quot;id&quot;:[0,&quot;6gMpGK5HugYKaxJbvTMOHp&quot;],&quot;name&quot;:[0,&quot;LLM&quot;],&quot;slug&quot;:[0,&quot;llm&quot;]}],[0,{&quot;id&quot;:[0,&quot;lGCLqAT2SMojMzw5b6aio&quot;],&quot;name&quot;:[0,&quot;WAF&quot;],&quot;slug&quot;:[0,&quot;waf&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Radwa Radwan&quot;],&quot;slug&quot;:[0,&quot;radwa&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/26ps7sSkjvnatLHbGNWuI9/42c3618ee2392f7d7a0dc5f335615fda/radwa.jpg&quot;],&quot;location&quot;:[0,&quot;London&quot;],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@RadwaRadwan__&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Mathias Deschamps&quot;],&quot;slug&quot;:[0,&quot;mathias-deschamps&quot;],&quot;bio&quot;:[0],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/43EtC0dFgs1vhmJxGLXQ5O/c625b7599d66afcf1890624b8108345a/Mathias_Deschamps.jpg&quot;],&quot;location&quot;:[0],&quot;website&quot;:[0],&quot;twitter&quot;:[0],&quot;facebook&quot;:[0],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications.&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;Block unsafe prompts targeting your LLM endpoints with Firewall for AI&quot;],&quot;description&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications.&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5q7ynAaKyGB8eed0B3AXNv/921572b388e22cde586f440cb70109e8/OG_Share_2024__78_.png&quot;]}],&quot;publicly_index&quot;:[0,true]}],&quot;tagInfo&quot;:[0],&quot;authorInfo&quot;:[0],&quot;translatedPosts&quot;:[1,[]]}" ssr client="only" opts="{&quot;name&quot;:&quot;GoogleAnalytics&quot;,&quot;value&quot;:&quot;react&quot;}"></astro-island><script>(()=>{var l=(n,t)=>{let i=async()=>{await(await n())()},e=typeof t.value=="object"?t.value:void 0,s={timeout:e==null?void 0:e.timeout};"requestIdleCallback"in window?window.requestIdleCallback(i,s):setTimeout(i,s.timeout||200)};(self.Astro||(self.Astro={})).idle=l;window.dispatchEvent(new Event("astro:idle"));})();</script><astro-island uid="Z2vASfy" prefix="r7" component-url="/_astro/Navigation.OM2TpgVQ.js" component-export="Navigation" renderer-url="/_astro/client.CZEEmA8m.js" props="{&quot;title&quot;:[0,&quot;The Cloudflare Blog&quot;],&quot;logo&quot;:[0,&quot;//images.ctfassets.net/zkvhlag99gkb/69RwBidpiEHCDZ9rFVVk7T/092507edbed698420b89658e5a6d5105/CF_logo_stacked_blktype.png&quot;],&quot;pagesStore&quot;:[0,{&quot;page&quot;:[0,&quot;Post&quot;],&quot;slug&quot;:[0,&quot;block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;translationsAvailable&quot;:[1,[]],&quot;navData&quot;:[1,[[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T22:44:28.803Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-08-26T14:03:51.430Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,201],&quot;revision&quot;:[0,55],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;AI&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;4HIPcb68qM0e26fIxyfzwQ&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:43:21.536Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-08-25T09:38:21.024Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,152],&quot;revision&quot;:[0,51],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Developers&quot;],&quot;name&quot;:[0,&quot;Developers&quot;],&quot;slug&quot;:[0,&quot;developers&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;5kZtWqjqa7aOUoZr8NFGwI&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:43:26.040Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-07-24T05:03:10.471Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,213],&quot;revision&quot;:[0,51],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Cloudflare Radar&quot;],&quot;name&quot;:[0,&quot;Radar&quot;],&quot;slug&quot;:[0,&quot;cloudflare-radar&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;6QktrXeEFcl4e2dZUTZVGl&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:43:20.198Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-04-09T11:02:56.665Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,68],&quot;revision&quot;:[0,28],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Product News&quot;],&quot;name&quot;:[0,&quot;Product News&quot;],&quot;slug&quot;:[0,&quot;product-news&quot;],&quot;featured&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;6Mp7ouACN2rT3YjL1xaXJx&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:42:46.231Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-08-26T08:03:01.509Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,162],&quot;revision&quot;:[0,47],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Security&quot;],&quot;name&quot;:[0,&quot;Security&quot;],&quot;slug&quot;:[0,&quot;security&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;16yk8DVbNNifxov5cWvAov&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:56:23.848Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-07-25T01:09:42.347Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,67],&quot;revision&quot;:[0,30],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Policy &amp; Legal&quot;],&quot;name&quot;:[0,&quot;Policy &amp; Legal&quot;],&quot;slug&quot;:[0,&quot;policy&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;J61Eszqn98amrYHq4IhTx&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:43:46.068Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-08-22T01:01:37.998Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,141],&quot;revision&quot;:[0,45],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Zero Trust&quot;],&quot;name&quot;:[0,&quot;Zero Trust&quot;],&quot;slug&quot;:[0,&quot;zero-trust&quot;],&quot;featured&quot;:[0,true],&quot;publiclyIndex&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;48r7QV00gLMWOIcM1CSDRy&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:54:22.790Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-04-07T23:03:52.422Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,66],&quot;revision&quot;:[0,28],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Speed &amp; Reliability&quot;],&quot;name&quot;:[0,&quot;Speed &amp; Reliability&quot;],&quot;slug&quot;:[0,&quot;speed-and-reliability&quot;],&quot;featured&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;4g8tPriKOAUwdUT4jNPebe&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:46:40.927Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-04-07T23:03:51.235Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,77],&quot;revision&quot;:[0,29],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Life at Cloudflare&quot;],&quot;name&quot;:[0,&quot;Life at Cloudflare&quot;],&quot;slug&quot;:[0,&quot;life-at-cloudflare&quot;],&quot;featured&quot;:[0,true]}]}],[0,{&quot;metadata&quot;:[0,{&quot;tags&quot;:[1,[]],&quot;concepts&quot;:[1,[]]}],&quot;sys&quot;:[0,{&quot;space&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Space&quot;],&quot;id&quot;:[0,&quot;zkvhlag99gkb&quot;]}]}],&quot;id&quot;:[0,&quot;V86khSc459Yi1AhTlvtY7&quot;],&quot;type&quot;:[0,&quot;Entry&quot;],&quot;createdAt&quot;:[0,&quot;2024-10-09T19:46:53.657Z&quot;],&quot;updatedAt&quot;:[0,&quot;2025-02-04T17:12:59.473Z&quot;],&quot;environment&quot;:[0,{&quot;sys&quot;:[0,{&quot;id&quot;:[0,&quot;master&quot;],&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;Environment&quot;]}]}],&quot;publishedVersion&quot;:[0,57],&quot;revision&quot;:[0,21],&quot;contentType&quot;:[0,{&quot;sys&quot;:[0,{&quot;type&quot;:[0,&quot;Link&quot;],&quot;linkType&quot;:[0,&quot;ContentType&quot;],&quot;id&quot;:[0,&quot;blogTag&quot;]}]}],&quot;locale&quot;:[0,&quot;en-US&quot;]}],&quot;fields&quot;:[0,{&quot;entryTitle&quot;:[0,&quot;Partners&quot;],&quot;name&quot;:[0,&quot;Partners&quot;],&quot;slug&quot;:[0,&quot;partners&quot;],&quot;featured&quot;:[0,true]}]}]]]}],&quot;locale&quot;:[0,&quot;en-us&quot;],&quot;translations&quot;:[0,{&quot;posts.by&quot;:[0,&quot;By&quot;],&quot;footer.gdpr&quot;:[0,&quot;GDPR&quot;],&quot;lang_blurb1&quot;:[0,&quot;This post is also available in {lang1}.&quot;],&quot;lang_blurb2&quot;:[0,&quot;This post is also available in {lang1} and {lang2}.&quot;],&quot;lang_blurb3&quot;:[0,&quot;This post is also available in {lang1}, {lang2} and {lang3}.&quot;],&quot;footer.press&quot;:[0,&quot;Press&quot;],&quot;header.title&quot;:[0,&quot;The Cloudflare Blog&quot;],&quot;search.clear&quot;:[0,&quot;Clear&quot;],&quot;search.filter&quot;:[0,&quot;Filter&quot;],&quot;search.source&quot;:[0,&quot;Source&quot;],&quot;footer.careers&quot;:[0,&quot;Careers&quot;],&quot;footer.company&quot;:[0,&quot;Company&quot;],&quot;footer.support&quot;:[0,&quot;Support&quot;],&quot;footer.the_net&quot;:[0,&quot;theNet&quot;],&quot;search.filters&quot;:[0,&quot;Filters&quot;],&quot;footer.our_team&quot;:[0,&quot;Our team&quot;],&quot;footer.webinars&quot;:[0,&quot;Webinars&quot;],&quot;page.more_posts&quot;:[0,&quot;More posts&quot;],&quot;posts.time_read&quot;:[0,&quot;{time} min read&quot;],&quot;search.language&quot;:[0,&quot;Language&quot;],&quot;footer.community&quot;:[0,&quot;Community&quot;],&quot;footer.resources&quot;:[0,&quot;Resources&quot;],&quot;footer.solutions&quot;:[0,&quot;Solutions&quot;],&quot;footer.trademark&quot;:[0,&quot;Trademark&quot;],&quot;header.subscribe&quot;:[0,&quot;Subscribe&quot;],&quot;footer.compliance&quot;:[0,&quot;Compliance&quot;],&quot;footer.free_plans&quot;:[0,&quot;Free plans&quot;],&quot;footer.impact_ESG&quot;:[0,&quot;Impact/ESG&quot;],&quot;posts.follow_on_X&quot;:[0,&quot;Follow on X&quot;],&quot;footer.help_center&quot;:[0,&quot;Help center&quot;],&quot;footer.network_map&quot;:[0,&quot;Network Map&quot;],&quot;header.please_wait&quot;:[0,&quot;Please Wait&quot;],&quot;page.related_posts&quot;:[0,&quot;Related posts&quot;],&quot;search.result_stat&quot;:[0,&quot;Results &lt;strong&gt;{search_range}&lt;/strong&gt; of &lt;strong&gt;{search_total}&lt;/strong&gt; for &lt;strong&gt;{search_keyword}&lt;/strong&gt;&quot;],&quot;footer.case_studies&quot;:[0,&quot;Case Studies&quot;],&quot;footer.connect_2024&quot;:[0,&quot;Connect 2024&quot;],&quot;footer.terms_of_use&quot;:[0,&quot;Terms of Use&quot;],&quot;footer.white_papers&quot;:[0,&quot;White Papers&quot;],&quot;footer.cloudflare_tv&quot;:[0,&quot;Cloudflare TV&quot;],&quot;footer.community_hub&quot;:[0,&quot;Community Hub&quot;],&quot;footer.compare_plans&quot;:[0,&quot;Compare plans&quot;],&quot;footer.contact_sales&quot;:[0,&quot;Contact Sales&quot;],&quot;header.contact_sales&quot;:[0,&quot;Contact Sales&quot;],&quot;header.email_address&quot;:[0,&quot;Email Address&quot;],&quot;page.error.not_found&quot;:[0,&quot;Page not found&quot;],&quot;footer.developer_docs&quot;:[0,&quot;Developer docs&quot;],&quot;footer.privacy_policy&quot;:[0,&quot;Privacy Policy&quot;],&quot;footer.request_a_demo&quot;:[0,&quot;Request a demo&quot;],&quot;page.continue_reading&quot;:[0,&quot;Continue reading&quot;],&quot;footer.analysts_report&quot;:[0,&quot;Analyst reports&quot;],&quot;footer.for_enterprises&quot;:[0,&quot;For enterprises&quot;],&quot;footer.getting_started&quot;:[0,&quot;Getting Started&quot;],&quot;footer.learning_center&quot;:[0,&quot;Learning Center&quot;],&quot;footer.project_galileo&quot;:[0,&quot;Project Galileo&quot;],&quot;pagination.newer_posts&quot;:[0,&quot;Newer Posts&quot;],&quot;pagination.older_posts&quot;:[0,&quot;Older Posts&quot;],&quot;posts.social_buttons.x&quot;:[0,&quot;Discuss on X&quot;],&quot;search.icon_aria_label&quot;:[0,&quot;Search&quot;],&quot;search.source_location&quot;:[0,&quot;Source/Location&quot;],&quot;footer.about_cloudflare&quot;:[0,&quot;About Cloudflare&quot;],&quot;footer.athenian_project&quot;:[0,&quot;Athenian Project&quot;],&quot;footer.become_a_partner&quot;:[0,&quot;Become a partner&quot;],&quot;footer.cloudflare_radar&quot;:[0,&quot;Cloudflare Radar&quot;],&quot;footer.network_services&quot;:[0,&quot;Network services&quot;],&quot;footer.trust_and_safety&quot;:[0,&quot;Trust &amp; Safety&quot;],&quot;header.get_started_free&quot;:[0,&quot;Get Started Free&quot;],&quot;page.search.placeholder&quot;:[0,&quot;Search Cloudflare&quot;],&quot;footer.cloudflare_status&quot;:[0,&quot;Cloudflare Status&quot;],&quot;footer.cookie_preference&quot;:[0,&quot;Cookie Preferences&quot;],&quot;header.valid_email_error&quot;:[0,&quot;Must be valid email.&quot;],&quot;search.result_stat_empty&quot;:[0,&quot;Results &lt;strong&gt;{search_range}&lt;/strong&gt; of &lt;strong&gt;{search_total}&lt;/strong&gt;&quot;],&quot;footer.connectivity_cloud&quot;:[0,&quot;Connectivity cloud&quot;],&quot;footer.developer_services&quot;:[0,&quot;Developer services&quot;],&quot;footer.investor_relations&quot;:[0,&quot;Investor relations&quot;],&quot;page.not_found.error_code&quot;:[0,&quot;Error Code: 404&quot;],&quot;search.autocomplete_title&quot;:[0,&quot;Insert a query. Press enter to send&quot;],&quot;footer.logos_and_press_kit&quot;:[0,&quot;Logos &amp; press kit&quot;],&quot;footer.application_services&quot;:[0,&quot;Application services&quot;],&quot;footer.get_a_recommendation&quot;:[0,&quot;Get a recommendation&quot;],&quot;posts.social_buttons.reddit&quot;:[0,&quot;Discuss on Reddit&quot;],&quot;footer.sse_and_sase_services&quot;:[0,&quot;SSE and SASE services&quot;],&quot;page.not_found.outdated_link&quot;:[0,&quot;You may have used an outdated link, or you may have typed the address incorrectly.&quot;],&quot;footer.report_security_issues&quot;:[0,&quot;Report Security Issues&quot;],&quot;page.error.error_message_page&quot;:[0,&quot;Sorry, we can&#39;t find the page you are looking for.&quot;],&quot;header.subscribe_notifications&quot;:[0,&quot;Subscribe to receive notifications of new posts:&quot;],&quot;footer.cloudflare_for_campaigns&quot;:[0,&quot;Cloudflare for Campaigns&quot;],&quot;header.subscription_confimation&quot;:[0,&quot;Subscription confirmed. Thank you for subscribing!&quot;],&quot;posts.social_buttons.hackernews&quot;:[0,&quot;Discuss on Hacker News&quot;],&quot;footer.diversity_equity_inclusion&quot;:[0,&quot;Diversity, equity &amp; inclusion&quot;],&quot;footer.critical_infrastructure_defense_project&quot;:[0,&quot;Critical Infrastructure Defense Project&quot;]}]}" ssr client="idle" opts="{&quot;name&quot;:&quot;NavigationComponent&quot;,&quot;value&quot;:true}" await-children><header class="flex flex-row flex-wrap justify-between items-flex-end mw8 center mv3 pl3 pr1"><div class="w-100 flex items-flex-end justify-between justify-start-l"><div class="w-100 tr flex justify-end"><div class="flex justify-between items-center"><span class="dn di-l pr1"><a href="https://dash.cloudflare.com/sign-up" class="f1 blue1 dn di-l b no-underline underline-hover" target="_blank" rel="noreferrer">Get Started Free</a></span><span class="f1 gray4 dn di-l pr1">|</span><span class="dn di-l"><a target="_blank" href="https://www.cloudflare.com/plans/enterprise/contact/" class="f1 gray4 no-underline underline-hover pr1" rel="noreferrer">Contact Sales</a></span></div></div></div><div class="w-100 w-50-l flex items-end nb5 nb1-l"><a href="/" class="header-logo mr4 dn db-l"><img class="header-logo" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/69RwBidpiEHCDZ9rFVVk7T/092507edbed698420b89658e5a6d5105/CF_logo_stacked_blktype.png" alt="The Cloudflare Blog" width="170" height="57"/></a><h2 class="mt0 mb1 dn di-l"><a href="/" class="fw5 f5 gray3 no-underline"><span class="dn di-l">The Cloudflare Blog</span></a></h2></div><div class="w-100 w-50-l dn db-l"><div class="w-100 tr mkto-sub-message"><p class="f2">Subscribe to receive notifications of new posts:</p></div><div class="w-100 tr"><div class="marketo-form-container"><form id="mktoForm_1653"><div class="top-subscribe-form-container"><div class="top-subscribe-form-field"><input placeholder="Email Address" class="top-subscribe-form-input" name="email" type="email" title="Must be valid email."/></div><button class="top-subscribe-form-button" type="button">Subscribe</button></div></form></div></div></div></header><nav dir="ltr" class="bb b--black-10 db dn-l w-100 ph3 "><div class=" flex justify-between items-center" style="height:44px"><a href="/search/"><img class="h-6 w-6" src="/images/magnifier.svg" alt="magnifier icon"/></a><button type="button" style="background:transparent;border:none"><img src="/images/hamburger.svg" alt="hamburger menu"/></button></div><div class="js-mobile-nav-container dn"><div class="flex flex-column flex-wrap bg-gray9 o-95 absolute ph3 z-1 left-0 right-0 mx-4"><div class="pv3 ph2 tl"><a href="/tag/ai/" class="no-underline gray1 f4 fw7">AI</a></div><div class="pv3 ph2 tl"><a href="/tag/developers/" class="no-underline gray1 f4 fw7">Developers</a></div><div class="pv3 ph2 tl"><a href="/tag/cloudflare-radar/" class="no-underline gray1 f4 fw7">Radar</a></div><div class="pv3 ph2 tl"><a href="/tag/product-news/" class="no-underline gray1 f4 fw7">Product News</a></div><div class="pv3 ph2 tl"><a href="/tag/security/" class="no-underline gray1 f4 fw7">Security</a></div><div class="pv3 ph2 tl"><a href="/tag/policy/" class="no-underline gray1 f4 fw7">Policy &amp; Legal</a></div><div class="pv3 ph2 tl"><a href="/tag/zero-trust/" class="no-underline gray1 f4 fw7">Zero Trust</a></div><div class="pv3 ph2 tl"><a href="/tag/speed-and-reliability/" class="no-underline gray1 f4 fw7">Speed &amp; Reliability</a></div><div class="pv3 ph2 tl"><a href="/tag/life-at-cloudflare/" class="no-underline gray1 f4 fw7">Life at Cloudflare</a></div><div class="pv3 ph2 tl"><a href="/tag/partners/" class="no-underline gray1 f4 fw7">Partners</a></div></div></div></nav><nav id="nav" class="w-100 bb-0 bb-l b--black-10 z-1"><div id="desktop-nav-items-container" class="flex flex-wrap justify-between items-center mw8 center mv3 mv0-l"><div data-tag="ai" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/ai/" class="no-underline gray1 f2 fw5 pv3">AI</a></div><div data-tag="developers" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/developers/" class="no-underline gray1 f2 fw5 pv3">Developers</a></div><div data-tag="cloudflare-radar" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/cloudflare-radar/" class="no-underline gray1 f2 fw5 pv3">Radar</a></div><div data-tag="product-news" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/product-news/" class="no-underline gray1 f2 fw5 pv3">Product News</a></div><div data-tag="security" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/security/" class="no-underline gray1 f2 fw5 pv3">Security</a></div><div data-tag="policy" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/policy/" class="no-underline gray1 f2 fw5 pv3">Policy &amp; Legal</a></div><div data-tag="zero-trust" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/zero-trust/" class="no-underline gray1 f2 fw5 pv3">Zero Trust</a></div><div data-tag="speed-and-reliability" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/speed-and-reliability/" class="no-underline gray1 f2 fw5 pv3">Speed &amp; Reliability</a></div><div data-tag="life-at-cloudflare" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/life-at-cloudflare/" class="no-underline gray1 f2 fw5 pv3">Life at Cloudflare</a></div><div data-tag="partners" class="nav-item nav-item-desktop ml3 mr2 dn db-l pv3"><a href="/tag/partners/" class="no-underline gray1 f2 fw5 pv3">Partners</a></div><div class="nav-item ml2 mr3 dn db-l pv3" data-tag="search icon"><a href="/search/"><img id="search-icon" class="h-6 w-6" src="/images/magnifier.svg" alt="magnifier icon"/></a></div></div></nav><!--astro:end--></astro-island><progress class="reading-progress" value="0" max="100" aria-label="Reading progress"></progress><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();</script><astro-island uid="2bbdYn" prefix="r3" component-url="/_astro/Post.DwgEWwGA.js" component-export="Post" renderer-url="/_astro/client.CZEEmA8m.js" props="{&quot;post&quot;:[0,{&quot;id&quot;:[0,&quot;59hk6A3nH3YcLMjXhYnNof&quot;],&quot;title&quot;:[0,&quot;Block unsafe prompts targeting your LLM endpoints with Firewall for AI&quot;],&quot;slug&quot;:[0,&quot;block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;excerpt&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. &quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;Security teams are racing to &lt;a href=\&quot;https://www.cloudflare.com/the-net/vulnerable-llm-ai/\&quot;&gt;&lt;u&gt;secure a new attack surface&lt;/u&gt;&lt;/a&gt;: AI-powered applications. From chatbots to search assistants, LLMs are already shaping customer experience, but they also open the door to new risks. A single malicious prompt can exfiltrate sensitive data, &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/data-poisoning/\&quot;&gt;&lt;u&gt;poison a model&lt;/u&gt;&lt;/a&gt;, or inject toxic content into customer-facing interactions, undermining user trust. Without guardrails, even the best-trained model can be turned against the business.&lt;/p&gt;&lt;p&gt;Today, as part of AI Week, weâre expanding our AI security offerings by introducing unsafe content moderation, now integrated directly into Cloudflare &lt;a href=\&quot;https://developers.cloudflare.com/waf/detections/firewall-for-ai/\&quot;&gt;&lt;u&gt;Firewall for AI&lt;/u&gt;&lt;/a&gt;. Built with Llama, this new feature allows customers to leverage their existing Firewall for AI engine for unified detection, analytics, and topic enforcement, providing real-time protection for &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/what-is-large-language-model/\&quot;&gt;&lt;u&gt;Large Language Models (LLMs)&lt;/u&gt;&lt;/a&gt; at the network level. Now with just a few clicks, security and application teams can detect and block harmful prompts or topics at the edge â eliminating the need to modify application code or infrastructure.\n\nThis feature is immediately available to current Firewall for AI users. Those not yet onboarded can contact their account team to participate in the beta program.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;ai-protection-in-application-security\&quot;&gt;AI protection in application security&lt;/h2&gt;\n      &lt;a href=\&quot;#ai-protection-in-application-security\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Cloudflare&amp;#39;s Firewall for AI protects user-facing LLM applications from abuse and data leaks, addressing several of the &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/owasp-top-10-risks-for-llms/\&quot;&gt;&lt;u&gt;OWASP Top 10 LLM risks&lt;/u&gt;&lt;/a&gt; such as prompt injection, PII disclosure, and unbound consumption. It also extends protection to other risks such as unsafe or harmful content.&lt;/p&gt;&lt;p&gt;Unlike built-in controls that vary between model providers, Firewall for AI is model-agnostic. It sits in front of any model you choose, whether itâs from a third party like OpenAI or Gemini, one you run in-house, or a custom model you have built, and applies the same consistent protections.&lt;/p&gt;&lt;p&gt;Just like our origin-agnostic &lt;a href=\&quot;https://www.cloudflare.com/application-services/#application-services-case-products\&quot;&gt;&lt;u&gt;Application Security suite&lt;/u&gt;&lt;/a&gt;, Firewall for AI enforces policies at scale across all your models, creating a unified security layer. That means you can define guardrails once and apply them everywhere. For example, a financial services company might require its LLM to only respond to finance-related questions, while blocking prompts about unrelated or sensitive topics, enforced consistently across every model in use.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;unsafe-content-moderation-protects-businesses-and-users\&quot;&gt;Unsafe content moderation protects businesses and users&lt;/h2&gt;\n      &lt;a href=\&quot;#unsafe-content-moderation-protects-businesses-and-users\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Effective AI moderation is more than blocking âbad wordsâ, itâs about setting boundaries that protect users, meeting legal obligations, and preserving brand integrity, without over-moderating in ways that silence important voices.&lt;/p&gt;&lt;p&gt;Because LLMs cannot be fully scripted, their interactions are inherently unpredictable. This flexibility enables rich user experiences but also opens the door to abuse.&lt;/p&gt;&lt;p&gt;Key risks from unsafe prompts include misinformation, biased or offensive content, and model poisoning, where repeated harmful prompts degrade the quality and safety of future outputs. Blocking these prompts aligns with the OWASP Top 10 for LLMs, preventing both immediate misuse and long-term degradation.&lt;/p&gt;&lt;p&gt;One example of this is&lt;a href=\&quot;https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\&quot;&gt; &lt;b&gt;&lt;u&gt;Microsoftâs Tay chatbot&lt;/u&gt;&lt;/b&gt;&lt;/a&gt;. Trolls deliberately submitted toxic, racist, and offensive prompts, which Tay quickly began repeating. The failure was not only in Tayâs responses; it was in the lack of moderation on the inputs it accepted.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;detecting-unsafe-prompts-before-reaching-the-model\&quot;&gt;Detecting unsafe prompts before reaching the model&lt;/h2&gt;\n      &lt;a href=\&quot;#detecting-unsafe-prompts-before-reaching-the-model\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Cloudflare has integrated &lt;a href=\&quot;https://huggingface.co/meta-llama/Llama-Guard-3-8B\&quot;&gt;&lt;u&gt;Llama Guard&lt;/u&gt;&lt;/a&gt; directly into Firewall for AI. This brings AI input moderation into the same rules engine our customers already use to protect their applications. It uses the same approach that we created for developers building with AI in our &lt;a href=\&quot;https://blog.cloudflare.com/guardrails-in-ai-gateway/\&quot;&gt;&lt;u&gt;AI Gateway&lt;/u&gt;&lt;/a&gt; product.&lt;/p&gt;&lt;p&gt;Llama Guard analyzes prompts in real time and flags them across multiple safety categories, including hate, violence, sexual content, criminal planning, self-harm, and more.&lt;/p&gt;&lt;p&gt;With this integration, Firewall for AI not only &lt;a href=\&quot;https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#discovering-llm-powered-applications\&quot;&gt;&lt;u&gt;discovers LLM traffic&lt;/u&gt;&lt;/a&gt; endpoints automatically, but also enables security and AI teams to take immediate action. Unsafe prompts can be blocked before they reach the model, while flagged content can be logged or reviewed for oversight and tuning. Content safety checks can also be combined with other Application Security protections, such as &lt;a href=\&quot;https://www.cloudflare.com/application-services/products/bot-management/\&quot;&gt;&lt;u&gt;Bot Management&lt;/u&gt; &lt;/a&gt;and &lt;a href=\&quot;https://www.cloudflare.com/application-services/products/rate-limiting/\&quot;&gt;&lt;u&gt;Rate Limiting&lt;/u&gt;&lt;/a&gt;, to create layered defenses when protecting your model.&lt;/p&gt;&lt;p&gt;The result is a single, edge-native policy layer that enforces guardrails before unsafe prompts ever reach your infrastructure â without needing complex integrations.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;how-it-works-under-the-hood\&quot;&gt;How it works under the hood&lt;/h2&gt;\n      &lt;a href=\&quot;#how-it-works-under-the-hood\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Before diving into the architecture of Firewall for AI engine and how it fits within our previously mentioned module to detect &lt;a href=\&quot;https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#using-workers-ai-to-deploy-presidio\&quot;&gt;&lt;u&gt;PII in the prompts&lt;/u&gt;&lt;/a&gt;, letâs start with how we detect unsafe topics.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;detection-of-unsafe-topics\&quot;&gt;Detection of unsafe topics&lt;/h3&gt;\n      &lt;a href=\&quot;#detection-of-unsafe-topics\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;A key challenge in building safety guardrails is balancing a good detection with model helpfulness. If detection is too broad, it can prevent a model from answering legitimate user questions, hurting its utility. This is especially difficult for topic detection because of the ambiguity and dynamic nature of human language, where context is fundamental to meaning.Â &lt;/p&gt;&lt;p&gt;Simple approaches like keyword blocklists are interesting for precise subjects â but insufficient. They are easily bypassed and fail to understand the context in which words are used, leading to poor recall. Older probabilistic models such as &lt;a href=\&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\&quot;&gt;&lt;u&gt;Latent Dirichlet Allocation (LDA)&lt;/u&gt;&lt;/a&gt; were an improvement, but did not properly account for word ordering and other contextual nuances. \n\nRecent advancements in LLMs introduced a new paradigm. Their ability to perform zero-shot or few-shot classification is uniquely suited for the task of topic detection. For this reason, we chose &lt;a href=\&quot;https://huggingface.co/meta-llama/Llama-Guard-3-8B\&quot;&gt;&lt;u&gt;Llama Guard 3&lt;/u&gt;&lt;/a&gt;, an open-source model based on the Llama architecture that is specifically fine-tuned for content safety classification. When it analyzes a prompt, it answers whether the text is safe or unsafe, and provides a specific category. We are showing the default categories, as listed &lt;a href=\&quot;http://developers.cloudflare.com/ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_categories/\&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;. Because Llama 3 has a fixed knowledge cutoff, certain categories â like defamation or elections â are time-sensitive. As a result, the model may not fully capture events or context that emerged after it was trained, and thatâs important to keep in mind when relying on it.&lt;/p&gt;&lt;p&gt;For now, we cover the 13 default categories. We plan to expand coverage in the future, leveraging the modelâs zero-shot capabilities.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;a-scalable-architecture-for-future-detections\&quot;&gt;A scalable architecture for future detections&lt;/h3&gt;\n      &lt;a href=\&quot;#a-scalable-architecture-for-future-detections\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;We designed Firewall for AI to scale without adding noticeable latency, including Llama Guard, and this remains true even as we add new detection models.&lt;/p&gt;&lt;p&gt;To achieve this, we built a new asynchronous architecture. When a request is sent to an application protected by Firewall for AI, a Cloudflare Worker makes parallel, non-blocking requests to our different detection modules â one for PII, one for unsafe topics, and others as we add them.Â &lt;/p&gt;&lt;p&gt;Thanks to the Cloudflare network, this design scales to handle high request volumes out of the box, and latency does not increase as we add new detections. It will only be bounded by the slowest model used.Â &lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4Y2gTP6teVR2263UIEWHc9/9a31fb394cee6c437c1d4af6f71d867c/image3.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1924\&quot; height=\&quot;1300\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;We optimize to keep the model utility at its maximum while keeping the guardrail detection broad enough.&lt;/p&gt;&lt;p&gt;Llama Guard is a rather large model, so running it at scale with minimal latency is a challenge. We deploy it on &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt;, leveraging our large fleet of high performance GPUs. This infrastructure ensures we can offer fast, reliable inference throughout our network.&lt;/p&gt;&lt;p&gt;To ensure the system remains fast and reliable as adoption grows, we ran extensive load tests simulating the requests per second (RPS) we anticipate, using a wide range of prompt sizes to prepare for real-world traffic. To handle this, the number of model instances deployed on our network scales automatically with the load. We employ concurrency to minimize latency and optimize for hardware utilization. We also enforce a hard 2-second threshold for each analysis; if this time limit is reached, we fall back to any detections already completed, ensuring your application&amp;#39;s requests latency is never further impacted.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;from-detection-to-security-rules-enforcement\&quot;&gt;From detection to security rules enforcement&lt;/h3&gt;\n      &lt;a href=\&quot;#from-detection-to-security-rules-enforcement\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Firewall for AI follows the same familiar pattern as other Application Security features like Bot Management and WAF Attack Score, making it easy to adopt.&lt;/p&gt;&lt;p&gt;Once enabled, the &lt;a href=\&quot;https://developers.cloudflare.com/waf/detections/firewall-for-ai/#fields\&quot;&gt;&lt;u&gt;new fields&lt;/u&gt;&lt;/a&gt; appear in &lt;a href=\&quot;https://developers.cloudflare.com/waf/analytics/security-analytics/\&quot;&gt;&lt;u&gt;Security Analytics&lt;/u&gt;&lt;/a&gt; and expanded logs. From there, you can filter by unsafe topics, track trends over time, and drill into the results of individual requests to see all detection outcomes, for example: did we detect unsafe topics, and what are the categories. The request body itself (the prompt text) is not stored or exposed; only the results of the analysis are logged.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/722JxyLvT6DFQxFpQhHMYP/3f1a6aa8ef1dafe4ad1a8277578fd7ae/image2.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1125\&quot; height=\&quot;818\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;After reviewing the analytics, you can enforce unsafe topic moderation by creating rules to log or block based on prompt categories in &lt;a href=\&quot;https://developers.cloudflare.com/waf/custom-rules/\&quot;&gt;&lt;u&gt;Custom rules&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For example, you might log prompts flagged as sexual content or hate speech for review.Â &lt;/p&gt;&lt;p&gt;You can use this expression: \n&lt;code&gt;If (any(cf.llm.prompt.unsafe_topic_categories[*] in {&amp;quot;S10&amp;quot; &amp;quot;S12&amp;quot;})) then Log&lt;/code&gt;\n\nOr deploy the rule with the categories field in the dashboard as in the below screenshot.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2CUsVjjpCEqv2UQMU6cMmt/5307235338c1b58856c0685585347537/image4.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1113\&quot; height=\&quot;498\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;You can also take a broader approach by blocking all unsafe prompts outright:\n&lt;code&gt;If (cf.llm.prompt.unsafe_topic_detected)then Block&lt;/code&gt;&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3uRT9YlRlRPsL5bNyBFA3i/54eb171ecb48aaecc7876b972789bf15/image5.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1169\&quot; height=\&quot;628\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;These rules are applied automatically to all discovered HTTP requests containing prompts, ensuring guardrails are enforced consistently across your AI traffic.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;whats-next\&quot;&gt;Whatâs Next&lt;/h2&gt;\n      &lt;a href=\&quot;#whats-next\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;In the coming weeks, Firewall for AI will expand to detect prompt injection and jailbreak attempts. We are also exploring how to add more visibility in the analytics and logs, so teams can better validate detection results. A major part of our roadmap is adding model response handling, giving you control over not only what goes into the LLM but also what comes out. Additional abuse controls, such as rate limiting on tokens and support for more safety categories, are also on the way.&lt;/p&gt;&lt;p&gt;Firewall for AI is available in beta today. If youâre new to Cloudflare and want to explore how to implement these AI protections, &lt;a href=\&quot;https://www.cloudflare.com/plans/enterprise/contact/?utm_medium=referral&amp;utm_source=blog&amp;utm_campaign=2025-q3-acq-gbl-connectivity-ge-ge-general-ai_week_blog\&quot;&gt;&lt;u&gt;reach out for a consultation&lt;/u&gt;&lt;/a&gt;. If youâre already with Cloudflare, contact your account team to get access and start testing with real traffic.&lt;/p&gt;&lt;p&gt;Cloudflare is also opening up a user research program focused on AI security. If you are curious about previews of new functionality or want to help shape our roadmap, &lt;a href=\&quot;https://www.cloudflare.com/lp/ai-security-user-research-program-2025\&quot;&gt;&lt;u&gt;express your interest here&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-26T14:00+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-26T13:00:37.849Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1VoeWz4uYyUO0N70CkENqB/eaf25c3382f436ba3d3e1b27da334cbc/image1.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Mp7ouACN2rT3YjL1xaXJx&quot;],&quot;name&quot;:[0,&quot;Security&quot;],&quot;slug&quot;:[0,&quot;security&quot;]}],[0,{&quot;id&quot;:[0,&quot;6gMpGK5HugYKaxJbvTMOHp&quot;],&quot;name&quot;:[0,&quot;LLM&quot;],&quot;slug&quot;:[0,&quot;llm&quot;]}],[0,{&quot;id&quot;:[0,&quot;lGCLqAT2SMojMzw5b6aio&quot;],&quot;name&quot;:[0,&quot;WAF&quot;],&quot;slug&quot;:[0,&quot;waf&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Radwa Radwan&quot;],&quot;slug&quot;:[0,&quot;radwa&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/26ps7sSkjvnatLHbGNWuI9/42c3618ee2392f7d7a0dc5f335615fda/radwa.jpg&quot;],&quot;location&quot;:[0,&quot;London&quot;],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@RadwaRadwan__&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Mathias Deschamps&quot;],&quot;slug&quot;:[0,&quot;mathias-deschamps&quot;],&quot;bio&quot;:[0],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/43EtC0dFgs1vhmJxGLXQ5O/c625b7599d66afcf1890624b8108345a/Mathias_Deschamps.jpg&quot;],&quot;location&quot;:[0],&quot;website&quot;:[0],&quot;twitter&quot;:[0],&quot;facebook&quot;:[0],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications.&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;Block unsafe prompts targeting your LLM endpoints with Firewall for AI&quot;],&quot;description&quot;:[0,&quot;Cloudflare&#39;s AI security suite now includes unsafe content moderation, integrated into the Application Security Suite via Firewall for AI. Built with Llama, it detects and blocks harmful prompts before they reach your AI applications.&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5q7ynAaKyGB8eed0B3AXNv/921572b388e22cde586f440cb70109e8/OG_Share_2024__78_.png&quot;]}],&quot;publicly_index&quot;:[0,true]}],&quot;initialReadingTime&quot;:[0,&quot;7&quot;],&quot;relatedPosts&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;6O1tkxTcxxG9hgxI8X9kFH&quot;],&quot;title&quot;:[0,&quot;AI Gateway now gives you access to your favorite AI models, dynamic routing and more â through just one endpoint&quot;],&quot;slug&quot;:[0,&quot;ai-gateway-aug-2025-refresh&quot;],&quot;excerpt&quot;:[0,&quot;AI Gateway now gives you access to your favorite AI models, dynamic routing and more â through just one endpoint.&quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;Getting the observability you need is challenging enough when the code is deterministic, but AI presents a new challenge âÂ a core part of your userâs experience now relies on a non-deterministic engine that provides unpredictable outputs. On top of that, there are many factors that can influence the results: the model, the system prompt. And on top of that, you still have to worry about performance, reliability, and costs.Â &lt;/p&gt;&lt;p&gt;Solving performance, reliability and observability challenges is exactly what Cloudflare was built for, and two years ago, with the introduction of AI Gateway, we wanted to extend to our users the same levels of control in the age of AI.Â &lt;/p&gt;&lt;p&gt;Today, weâre excited to announce several features to make building AI applications easier and more manageable: unified billing, secure key storage, dynamic routing, security controls with Data Loss Prevention (DLP). This means that AI Gateway becomes your go-to place to control costs and API keys, route between different models and providers, and manage your AI traffic. Check out our new &lt;a href=\&quot;https://ai.cloudflare.com/gateway\&quot;&gt;&lt;u&gt;AI Gateway landing page&lt;/u&gt;&lt;/a&gt; for more information at a glance.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;connect-to-all-your-favorite-ai-providers\&quot;&gt;Connect to all your favorite AI providers&lt;/h2&gt;\n      &lt;a href=\&quot;#connect-to-all-your-favorite-ai-providers\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;When using an AI provider, you typically have to sign up for an account, get an API key, manage rate limits, top up credits â all within an individual providerâs dashboard. Multiply that for each of the different providers you might use, and youâll soon be left with an administrative headache of bills and keys to manage.&lt;/p&gt;&lt;p&gt;With &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/ai-gateway/\&quot;&gt;&lt;u&gt;AI Gateway&lt;/u&gt;&lt;/a&gt;, you can now connect to major AI providers directly through Cloudflare and manage everything through one single plane. Weâre excited to partner with Anthropic, Google, Groq, OpenAI, and xAI to provide Cloudflare users with access to their models directly through Cloudflare. With this, youâll have access to over 350+ models across 6 different providers.&lt;/p&gt;&lt;p&gt;You can now get billed for usage across different providers directly through your Cloudflare account. This feature is available for Workers Paid users, where youâll be able to add credits to your Cloudflare account and use them for &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/inference-vs-training/\&quot;&gt;&lt;u&gt;AI inference&lt;/u&gt;&lt;/a&gt; to all the supported providers. Youâll be able to see real-time usage statistics and manage your credits through the AI Gateway dashboard. Your AI Gateway inference usage will also be documented in your monthly Cloudflare invoice. No more signing up and paying for each individual model provider account.Â &lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4t2j5frheaYOLznprTL58p/f0fb4c6de2aad70c82a23bc35873ea50/image1.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;1586\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;Usage rates are based on then-current list prices from model providers â all you will need to cover is the transaction fee as you load credits into your account. Since this is one of the first times weâre launching a credits based billing system at Cloudflare, weâre releasing this feature in Closed Beta â sign up for access &lt;a href=\&quot;https://forms.gle/3LGAzN2NDXqtbjKR9\&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;byo-provider-keys-now-with-cloudflare-secrets-store\&quot;&gt;BYO Provider Keys, now with Cloudflare Secrets Store&lt;/h3&gt;\n      &lt;a href=\&quot;#byo-provider-keys-now-with-cloudflare-secrets-store\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Although weâve introduced unified billing, some users might still want to manage their own accounts and keys with providers. Weâre happy to say that AI Gateway will continue supporting our &lt;a href=\&quot;https://developers.cloudflare.com/ai-gateway/configuration/bring-your-own-keys/\&quot;&gt;&lt;u&gt;BYO Key feature, &lt;/u&gt;&lt;/a&gt;improving the experience of BYO Provider Keys by integrating with Cloudflareâs secrets management product &lt;a href=\&quot;https://developers.cloudflare.com/secrets-store/\&quot;&gt;&lt;u&gt;Secrets Store&lt;/u&gt;&lt;/a&gt;. Now, you can seamlessly and securely store your keys in one centralized location and distribute them without relying on plain text. Secrets Store uses a two level key hierarchy with AES encryption to ensure that your secret stays safe, while maintaining low latency through our global configuration system, &lt;a href=\&quot;https://blog.cloudflare.com/quicksilver-v2-evolution-of-a-globally-distributed-key-value-store-part-1/\&quot;&gt;&lt;u&gt;Quicksilver&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;You can now save and manage keys directly through your AI Gateway dashboard or through the Secrets Store &lt;a href=\&quot;http://dash.cloudflare.com/?to=/:account/secrets-store\&quot;&gt;&lt;u&gt;dashboard&lt;/u&gt;&lt;/a&gt;, &lt;a href=\&quot;https://developers.cloudflare.com/api/resources/secrets_store/subresources/stores/subresources/secrets/methods/create/\&quot;&gt;&lt;u&gt;API&lt;/u&gt;&lt;/a&gt;, or &lt;a href=\&quot;https://developers.cloudflare.com/workers/wrangler/commands/#secrets-store-secret\&quot;&gt;&lt;u&gt;Wrangler&lt;/u&gt;&lt;/a&gt; by using the new &lt;b&gt;AI Gateway&lt;/b&gt; &lt;b&gt;scope&lt;/b&gt;. Scoping your secrets to AI Gateway ensures that only this specific service will be able to access your keys, meaning that secret could not be used in a Workers binding or anywhere else on Cloudflareâs platform.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6hiSSQi2lQGWQnGYe4e9p1/dadc4fde865010d9e263badb75847992/2.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;1605\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;You can pass your AI provider keys without including them directly in the request header. Instead of including the actual value, you can deploy the secret only using the Secrets Store reference:Â &lt;/p&gt;\n            &lt;pre class=\&quot;language-Shell\&quot;&gt;&lt;code class=\&quot;language-Shell\&quot;&gt;curl -X POST https://gateway.ai.cloudflare.com/v1/&amp;lt;ACCOUNT_ID&amp;gt;/my-gateway/anthropic/v1/messages \\\n --header &amp;#039;cf-aig-authorization: CLOUDFLARE_AI_GATEWAY_TOKEN \\\n --header &amp;#039;anthropic-version: 2023-06-01&amp;#039; \\\n --header &amp;#039;Content-Type: application/json&amp;#039; \\\n --data  &amp;#039;{&amp;quot;model&amp;quot;: &amp;quot;claude-3-opus-20240229&amp;quot;, &amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What is Cloudflare?&amp;quot;}]}&amp;#039;&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;Or, using Javascript:Â &lt;/p&gt;\n            &lt;pre class=\&quot;language-JavaScript\&quot;&gt;&lt;code class=\&quot;language-JavaScript\&quot;&gt;import Anthropic from &amp;#039;@anthropic-ai/sdk&amp;#039;;\n\n\nconst anthropic = new Anthropic({\n  apiKey: &amp;quot;CLOUDFLARE_AI_GATEWAY_TOKEN&amp;quot;,\n  baseURL: &amp;quot;https://gateway.ai.cloudflare.com/v1/&amp;lt;ACCOUNT_ID&amp;gt;/my-gateway/anthropic&amp;quot;,\n});\n\n\nconst message = await anthropic.messages.create({\n  model: &amp;#039;claude-3-opus-20240229&amp;#039;,\n  messages: [{role: &amp;quot;user&amp;quot;, content: &amp;quot;What is Cloudflare?&amp;quot;}],\n  max_tokens: 1024\n});&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;By using Secrets Store to deploy your secrets, you no longer need to give every developer access to every key â instead, you can rely on Secrets Storeâs &lt;a href=\&quot;https://developers.cloudflare.com/secrets-store/access-control/\&quot;&gt;&lt;u&gt;role-based access control&lt;/u&gt;&lt;/a&gt; to further lock down these sensitive values. For example, you might want your security administrators to have Secrets Store admin permissions so that they can create, update, and delete the keys when necessary. With Cloudflare &lt;a href=\&quot;https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/audit_logs/?cf_target_id=1C767B900C4419A313C249A5D99921FB\&quot;&gt;&lt;u&gt;audit logging&lt;/u&gt;&lt;/a&gt;, all such actions will be logged so you know exactly who did what and when. Your developers, on the other hand, might only need Deploy permissions, so they can reference the values in code, whether that is a Worker or AI Gateway or both. This way, you reduce the risk of the secret getting leaked accidentally or intentionally by a malicious actor. This also allows you to update your provider keys in one place and automatically propagate that value to any AI Gateway using those values, simplifying the management.Â &lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;unified-request-response\&quot;&gt;Unified Request/Response&lt;/h3&gt;\n      &lt;a href=\&quot;#unified-request-response\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;We made it super easy for people to try out different AI models â but the developer experience should match that as well. We found that each provider can have slight differences in how they expect people to send their requests, so weâre excited to launch an automatic translation layer between providers. When you send a request through AI Gateway, it just works â no matter what provider or model you use.&lt;/p&gt;\n            &lt;pre class=\&quot;language-JavaScript\&quot;&gt;&lt;code class=\&quot;language-JavaScript\&quot;&gt;import OpenAI from &amp;quot;openai&amp;quot;;\nconst client = new OpenAI({\n  apiKey: &amp;quot;YOUR_PROVIDER_API_KEY&amp;quot;, // Provider API key\n  // NOTE: the OpenAI client automatically adds /chat/completions to the end of the URL, you should not add it yourself.\n  baseURL:\n    &amp;quot;https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/compat&amp;quot;,\n});\n\nconst response = await client.chat.completions.create({\n  model: &amp;quot;google-ai-studio/gemini-2.0-flash&amp;quot;,\n  messages: [{ role: &amp;quot;user&amp;quot;, content: &amp;quot;What is Cloudflare?&amp;quot; }],\n});\n\nconsole.log(response.choices[0].message.content);&lt;/pre&gt;&lt;/code&gt;\n            \n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;dynamic-routes\&quot;&gt;Dynamic Routes&lt;/h2&gt;\n      &lt;a href=\&quot;#dynamic-routes\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;When we first launched &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/workers/\&quot;&gt;&lt;u&gt;Cloudflare Workers&lt;/u&gt;&lt;/a&gt;, it was an easy way for people to intercept HTTP requests and customize actions based on different attributes. We think the same customization is necessary for AI traffic, so weâre launching &lt;a href=\&quot;https://developers.cloudflare.com/ai-gateway/features/dynamic-routing/\&quot;&gt;&lt;u&gt;Dynamic Routes&lt;/u&gt;&lt;/a&gt; in AI Gateway.&lt;/p&gt;&lt;p&gt;Dynamic Routes allows you to define certain actions based on different request attributes. If you have free users, maybe you want to ratelimit them to a certain request per second (RPS) or a certain dollar spend. Or maybe you want to conduct an A/B test and split 50% of traffic to Model A and 50% of traffic to Model B. You could also want to chain several models in a row, like adding custom guardrails or enhancing a prompt before it goes to another model. All of this is possible with Dynamic Routes!&lt;/p&gt;&lt;p&gt;Weâve built a slick UI in the AI Gateway dashboard where you can define simple if/else interactions based on request attributes or a percentage split. Once you define a route, youâll use the route as the âmodelâ name in your input JSON and we will manage the traffic as you defined.Â &lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7qLp4KT8ASCLRv2pyM2kxR/3151e32afa4d8447ae07a5a8fb09a9b6/3.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;1426\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;\n            &lt;pre class=\&quot;language-JavaScript\&quot;&gt;&lt;code class=\&quot;language-JavaScript\&quot;&gt;import OpenAI from &amp;quot;openai&amp;quot;;\n\nconst cloudflareToken = &amp;quot;CF_AIG_TOKEN&amp;quot;;\nconst accountId = &amp;quot;{account_id}&amp;quot;;\nconst gatewayId = &amp;quot;{gateway_id}&amp;quot;;\nconst baseURL = `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}`;\n\nconst openai = new OpenAI({\n  apiKey: cloudflareToken,\n  baseURL,\n});\n\ntry {\n  const model = &amp;quot;dynamic/&amp;lt;your-dynamic-route-name&amp;gt;&amp;quot;;\n  const messages = [{ role: &amp;quot;user&amp;quot;, content: &amp;quot;What is a neuron?&amp;quot; }];\n  const chatCompletion = await openai.chat.completions.create({\n    model,\n    messages,\n  });\n  const response = chatCompletion.choices[0].message;\n  console.log(response);\n} catch (e) {\n  console.error(e);\n}&lt;/pre&gt;&lt;/code&gt;\n            \n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;built-in-security-with-firewall-in-ai-gateway\&quot;&gt;Built-in security with Firewall in AI Gateway&lt;/h2&gt;\n      &lt;a href=\&quot;#built-in-security-with-firewall-in-ai-gateway\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Earlier this year we announced &lt;a href=\&quot;https://developers.cloudflare.com/changelog/2025-02-26-guardrails/\&quot;&gt;&lt;u&gt;Guardrails&lt;/u&gt;&lt;/a&gt; in AI Gateway and now weâre expanding our security capabilities and include Data Loss Prevention (DLP) scanning in AI Gatewayâs Firewall. With this, you can select the DLP profiles you are interested in blocking or flagging, and we will scan requests for the matching content. DLP profiles include general categories like âFinancial Informationâ, âSocial Security, Insurance, Tax and Identifier Numbersâ that everyone has access to with a free Zero Trust account. If you would like to create a custom DLP profile to safeguard specific text, the upgraded Zero Trust plan allows you to create custom DLP profiles to catch sensitive data that is unique to your business.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5yti8oy4TF01EdZMtYN1If/d2f3bd804873644862fbd61b07d3574a/4.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1256\&quot; height=\&quot;1070\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;False positives and grey area situations happen, we give admins controls on whether to fully block or just alert on DLP matches. This allows administrators to monitor for potential issues without creating roadblocks for their users.. Each log on AI gateway now includes details about the DLP profiles matched on your request, and the action that was taken:&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2pDdqy8bVmsiyjm4sg2pkG/ff97d9069e200fb859c1dc2daed8e4fa/5.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1264\&quot; height=\&quot;657\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;more-coming-soon\&quot;&gt;More coming soonâ¦&lt;/h2&gt;\n      &lt;a href=\&quot;#more-coming-soon\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;If you think about the history of Cloudflare, youâll notice similar patterns that weâre following for the new vision for AI Gateway. We want developers of AI applications to be able to have simple interconnectivity, observability, security, customizable actions, and more â something that Cloudflare has a proven track record of accomplishing for global Internet traffic. We see AI Gateway as a natural extension of Cloudflareâs mission, and weâre excited to make it come to life.&lt;/p&gt;&lt;p&gt;Weâve got more launches up our sleeves, but we couldnât wait to get these first handful of features into your hands. Read up about it in our &lt;a href=\&quot;https://developers.cloudflare.com/ai-gateway/\&quot;&gt;&lt;u&gt;developer docs&lt;/u&gt;&lt;/a&gt;, &lt;a href=\&quot;https://developers.cloudflare.com/ai-gateway/get-started/\&quot;&gt;&lt;u&gt;give it a try&lt;/u&gt;&lt;/a&gt;, and let us know what you think. If you want to explore larger deployments, &lt;a href=\&quot;https://www.cloudflare.com/plans/enterprise/contact/?utm_medium=referral&amp;utm_source=blog&amp;utm_campaign=2025-q3-acq-gbl-connectivity-ge-ge-general-ai_week_blog\&quot;&gt;&lt;u&gt;reach out for a consultation &lt;/u&gt;&lt;/a&gt;with Cloudflare experts.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/LTpdSaZMBbdOzASW8ggoS/6610f437d955174d7f7f1212617a4365/6.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1199\&quot; height=\&quot;325\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-27T14:05+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-27T13:00:18.668Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/N8T4grj8IDVGwRVlhkKAt/5020ae08a3632df461525c056771bb91/image3.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;1GyUhE8o287lrdNSpdRUIe&quot;],&quot;name&quot;:[0,&quot;AI Gateway&quot;],&quot;slug&quot;:[0,&quot;ai-gateway&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Michelle Chen&quot;],&quot;slug&quot;:[0,&quot;michelle&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1hrcl3aVtUbBuCMeuXETWy/93dbfbc7d41c09ba35d863312dbde89d/michelle.jpg&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@_mchenco&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Abhishek Kankani&quot;],&quot;slug&quot;:[0,&quot;abhishek-kankani&quot;],&quot;bio&quot;:[0],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2hAolFysFaB0fboajbsNbR/1cc2166822e7cefe99689392086f92fe/Screenshot_2025-04-08_at_10.47.37_AM.png&quot;],&quot;location&quot;:[0],&quot;website&quot;:[0],&quot;twitter&quot;:[0],&quot;facebook&quot;:[0],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Mia Malden&quot;],&quot;slug&quot;:[0,&quot;mia&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2FY6694OH0p4nMADqEeg6b/49113dde85b295765b541197294fe244/Mia_Malden.jpg&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,null],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;AI Gateway simplifies AI app development with unified billing, secure key storage, and dynamic routing. Gain observability and control over costs, API keys, and traffic, connecting to major AI providers through a single endpoint.&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/ai-gateway-aug-2025-refresh&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;AI Gateway now gives you access to your favorite AI models, dynamic routing and more â through just one endpoint&quot;],&quot;description&quot;:[0,&quot;AI Gateway simplifies AI app development with unified billing, secure key storage, and dynamic routing. Gain observability and control over costs, API keys, and traffic, connecting to major AI providers through a single endpoint.&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3AKFMly47PtVeXSnFlbdRo/602a2247e6be226d8ad8d7e03d0e2e87/OG_Share_2024__86_.png&quot;]}],&quot;publicly_index&quot;:[0,true]}],[0,{&quot;id&quot;:[0,&quot;35N861jwJHF4GEiRCDxWP&quot;],&quot;title&quot;:[0,&quot;State-of-the-art image generation Leonardo models and text-to-speech Deepgram models now available in Workers AI&quot;],&quot;slug&quot;:[0,&quot;workers-ai-partner-models&quot;],&quot;excerpt&quot;:[0,&quot;We&#39;re expanding Workers AI with new partner models from Leonardo.Ai and Deepgram. Start using state-of-the-art image generation models from Leonardo and real-time TTS and STT models from Deepgram. &quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;When we first launched &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt;, we made a bet that AI models would get faster and smaller. We built our infrastructure around this hypothesis, adding specialized GPUs to our datacenters around the world that can serve inference to users as fast as possible. We created our platform to be as general as possible, but we also identified niche use cases that fit our infrastructure well, such as low-latency image generation or real-time audio voice agents. To lean in on those use cases, weâre bringing on some new models that will help make it easier to develop for these applications.&lt;/p&gt;&lt;p&gt;Today, weâre excited to announce that we are expanding our model catalog to include closed-source partner models that fit this use case. Weâve partnered with &lt;a href=\&quot;http://leonardo.ai\&quot;&gt;&lt;u&gt;Leonardo.Ai&lt;/u&gt;&lt;/a&gt; and &lt;a href=\&quot;https://deepgram.com/\&quot;&gt;&lt;u&gt;Deepgram&lt;/u&gt;&lt;/a&gt; to bring their latest and greatest models to Workers AI, hosted on Cloudflareâs infrastructure. Leonardo and Deepgram both have models with a great speed-to-performance ratio that suit the infrastructure of Workers AI. Weâre starting off with these great partners â but expect to expand our catalog to other partner models as well.&lt;/p&gt;&lt;p&gt;The benefits of using these models on Workers AI is that we donât only have a standalone inference service, we also have an entire suite of Developer products that allow you to build whole applications around AI. If youâre building an image generation platform, you could use Workers to host the application logic, Workers AI to generate the images, R2 for storage, and Images for serving and transforming media. If youâre building Realtime voice agents, we offer WebRTC and WebSocket support via Workers, speech-to-text, text-to-speech, and turn detection models via Workers AI, and an orchestration layer via Cloudflare Realtime. All in all, we want to lean into use cases that we think Cloudflare has a unique advantage in, with developer tools to back it up, and make it all available so that you can build the best AI applications on top of our holistic Developer Platform.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;leonardo-models\&quot;&gt;Leonardo Models&lt;/h2&gt;\n      &lt;a href=\&quot;#leonardo-models\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;&lt;a href=\&quot;https://www.leonardo.ai\&quot;&gt;&lt;u&gt;Leonardo.Ai&lt;/u&gt;&lt;/a&gt; is a generative AI media lab that trains their own models and hosts a platform for customers to create generative media. The Workers AI team has been working with Leonardo for a while now and have experienced the magic of their image generation models firsthand. Weâre excited to bring on two image generation models from Leonardo: @cf/leonardo/phoenix-1.0 and @cf/leonardo/lucid-origin.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;âWeâre excited to enable Cloudflare customers a new avenue to extend and use our image generation technology in creative ways such as creating character images for gaming, generating personalized images for websites, and a host of other uses... all through the Workers AI and the Cloudflare Developer Platform.â - &lt;/i&gt;&lt;b&gt;&lt;i&gt;Peter Runham&lt;/i&gt;&lt;/b&gt;&lt;i&gt;, CTO, &lt;/i&gt;&lt;a href=\&quot;http://leonardo.ai\&quot;&gt;&lt;i&gt;&lt;u&gt;Leonardo.AiÂ &lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The Phoenix model is trained from the ground up by Leonardo, excelling at things like text rendering and prompt coherence. The full image generation request took 4.89s end-to-end for a 25 step, 1024x1024 image.&lt;/p&gt;\n            &lt;pre class=\&quot;language-shell\&quot;&gt;&lt;code class=\&quot;language-shell\&quot;&gt;curl --request POST \\\n  --url https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/leonardo/phoenix-1.0 \\\n  --header &amp;#039;Authorization: Bearer {TOKEN}&amp;#039; \\\n  --header &amp;#039;Content-Type: application/json&amp;#039; \\\n  --data &amp;#039;{\n    &amp;quot;prompt&amp;quot;: &amp;quot;A 1950s-style neon diner sign glowing at night that reads &amp;#039;\\&amp;#039;&amp;#039;OPEN 24 HOURS&amp;#039;\\&amp;#039;&amp;#039; with chrome details and vintage typography.&amp;quot;,\n    &amp;quot;width&amp;quot;:1024,\n    &amp;quot;height&amp;quot;:1024,\n    &amp;quot;steps&amp;quot;: 25,\n    &amp;quot;seed&amp;quot;:1,\n    &amp;quot;guidance&amp;quot;: 4,\n    &amp;quot;negative_prompt&amp;quot;: &amp;quot;bad image, low quality, signature, overexposed, jpeg artifacts, undefined, unclear, Noisy, grainy, oversaturated, overcontrasted&amp;quot;\n}&amp;#039;\n&lt;/pre&gt;&lt;/code&gt;\n            \n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1q7ndHYrwLQqqAdX6kGEkl/96ece588cf82691fa8e8d11ece382672/BLOG-2903_2.png\&quot; alt=\&quot;BLOG-2903 2\&quot; class=\&quot;kg-image\&quot; width=\&quot;1024\&quot; height=\&quot;1024\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;The Lucid Origin model is a recent addition to Leonardoâs family of models and is great at generating photorealistic images. The image took 4.38s to generate end-to-end at 25 steps and a 1024x1024 image size.&lt;/p&gt;\n            &lt;pre class=\&quot;language-shell\&quot;&gt;&lt;code class=\&quot;language-shell\&quot;&gt;curl --request POST \\\n  --url https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/leonardo/lucid-origin \\\n  --header &amp;#039;Authorization: Bearer {TOKEN}&amp;#039; \\\n  --header &amp;#039;Content-Type: application/json&amp;#039; \\\n  --data &amp;#039;{\n    &amp;quot;prompt&amp;quot;: &amp;quot;A 1950s-style neon diner sign glowing at night that reads &amp;#039;\\&amp;#039;&amp;#039;OPEN 24 HOURS&amp;#039;\\&amp;#039;&amp;#039; with chrome details and vintage typography.&amp;quot;,\n    &amp;quot;width&amp;quot;:1024,\n    &amp;quot;height&amp;quot;:1024,\n    &amp;quot;steps&amp;quot;: 25,\n    &amp;quot;seed&amp;quot;:1,\n    &amp;quot;guidance&amp;quot;: 4,\n    &amp;quot;negative_prompt&amp;quot;: &amp;quot;bad image, low quality, signature, overexposed, jpeg artifacts, undefined, unclear, Noisy, grainy, oversaturated, overcontrasted&amp;quot;\n}&amp;#039;\n&lt;/pre&gt;&lt;/code&gt;\n            \n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/26VKWD8ua6Pe2awQWRnF7n/bb42c9612b08269af4ef38df39a2ed30/BLOG-2903_3.png\&quot; alt=\&quot;BLOG-2903 3\&quot; class=\&quot;kg-image\&quot; width=\&quot;1024\&quot; height=\&quot;1024\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;deepgram-models\&quot;&gt;Deepgram Models&lt;/h2&gt;\n      &lt;a href=\&quot;#deepgram-models\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Deepgram is a voice AI company that develops their own audio models, allowing users to interact with AI through a natural interface for humans: voice. Voice is an exciting interface because it carries higher bandwidth than text, because it has other speech signals like pacing, intonation, and more. The Deepgram models that weâre bringing on our platform are audio models which perform extremely fast speech-to-text and text-to-speech inference. Combined with the Workers AI infrastructure, the models showcase our unique infrastructure so customers can build low-latency voice agents and more.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;&amp;quot;By hosting our voice models on Cloudflare&amp;#39;s Workers AI, we&amp;#39;re enabling developers to create real-time, expressive voice agents with ultra-low latency. Cloudflare&amp;#39;s global network brings AI compute closer to users everywhere, so customers can now deliver lightning-fast conversational AI experiences without worrying about complex infrastructure.&amp;quot; - &lt;/i&gt;&lt;i&gt;&lt;b&gt;Adam Sypniewski&lt;/b&gt;&lt;/i&gt;&lt;i&gt;, CTO, Deepgram&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/nova-3\&quot;&gt;&lt;u&gt;@cf/deepgram/nova-3&lt;/u&gt;&lt;/a&gt; is a speech-to-text model that can quickly transcribe audio with high accuracy. &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/aura-1\&quot;&gt;&lt;u&gt;@cf/deepgram/aura-1&lt;/u&gt;&lt;/a&gt; is a text-to-speech model that is context aware and can apply natural pacing and expressiveness based on the input text. The newer Aura 2 model will be available on Workers AI soon. Weâve also improved the experience of sending binary mp3 files to Workers AI, so you donât have to convert it into an Uint8 array like you had to previously. Along with our Realtime announcements (coming soon!), these audio models are the key to enabling customers to build voice agents directly on Cloudflare.&lt;/p&gt;&lt;p&gt;With the AI binding, a call to the Nova 3 speech-to-text model would look like this:&lt;/p&gt;\n            &lt;pre class=\&quot;language-javascript\&quot;&gt;&lt;code class=\&quot;language-javascript\&quot;&gt;const URL = &amp;quot;https://www.some-website.com/audio.mp3&amp;quot;;\nconst mp3 = await fetch(URL);\n \nconst res = await env.AI.run(&amp;quot;@cf/deepgram/nova-3&amp;quot;, {\n    &amp;quot;audio&amp;quot;: {\n      body: mp3.body,\n      contentType: &amp;quot;audio/mpeg&amp;quot;\n    },\n    &amp;quot;detect_language&amp;quot;: true\n  });\n&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;With the REST API, it would look like this:&lt;/p&gt;\n            &lt;pre class=\&quot;language-shell\&quot;&gt;&lt;code class=\&quot;language-shell\&quot;&gt;curl --request POST \\\n  --url &amp;#039;https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/deepgram/nova-3?detect_language=true&amp;#039; \\\n  --header &amp;#039;Authorization: Bearer {TOKEN}&amp;#039; \\\n  --header &amp;#039;Content-Type: audio/mpeg&amp;#039; \\\n  --data-binary @/path/to/audio.mp3&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;As well, weâve added WebSocket support to the Deepgram models, which you can use to keep a connection to the inference server live and use it for bi-directional input and output. To use the Nova model with WebSocket support, check out our &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/nova-3\&quot;&gt;&lt;u&gt;Developer Docs&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;All the pieces work together so that you can:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Capture audio&lt;/b&gt; with Cloudflare Realtime from any WebRTC source&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Pipe it&lt;/b&gt; via WebSocket to your processing pipeline&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Transcribe&lt;/b&gt; with audio ML models Deepgram running on Workers AI&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Process&lt;/b&gt; with your LLM of choice through a model hosted on Workers AI or proxied via &lt;a href=\&quot;https://developers.cloudflare.com/ai-gateway/\&quot;&gt;&lt;u&gt;AI Gateway&lt;/u&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Orchestrate&lt;/b&gt; everything with Realtime Agents&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;try-these-models-out-today\&quot;&gt;Try these models out today&lt;/h2&gt;\n      &lt;a href=\&quot;#try-these-models-out-today\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Check out our&lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/\&quot;&gt;&lt;u&gt; developer docs&lt;/u&gt;&lt;/a&gt; for more details, pricing and how to get started with the newest partner models available on Workers AI.&lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-27T14:00+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-27T17:49:10.255Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/24eaEAFJQtC4seHoUKWFVl/e2691f5779435b8aef21c58c5db0c52c/BLOG-2903_1.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;]}],[0,{&quot;id&quot;:[0,&quot;3JAY3z7p7An94s6ScuSQPf&quot;],&quot;name&quot;:[0,&quot;Developer Platform&quot;],&quot;slug&quot;:[0,&quot;developer-platform&quot;]}],[0,{&quot;id&quot;:[0,&quot;4HIPcb68qM0e26fIxyfzwQ&quot;],&quot;name&quot;:[0,&quot;Developers&quot;],&quot;slug&quot;:[0,&quot;developers&quot;]}],[0,{&quot;id&quot;:[0,&quot;6hbkItfupogJP3aRDAq6v8&quot;],&quot;name&quot;:[0,&quot;Cloudflare Workers&quot;],&quot;slug&quot;:[0,&quot;workers&quot;]}],[0,{&quot;id&quot;:[0,&quot;1Wf1Dpb2AFicG44jpRT29y&quot;],&quot;name&quot;:[0,&quot;Workers AI&quot;],&quot;slug&quot;:[0,&quot;workers-ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Michelle Chen&quot;],&quot;slug&quot;:[0,&quot;michelle&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1hrcl3aVtUbBuCMeuXETWy/93dbfbc7d41c09ba35d863312dbde89d/michelle.jpg&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@_mchenco&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Nikhil Kothari&quot;],&quot;slug&quot;:[0,&quot;nikhil&quot;],&quot;bio&quot;:[0,&quot;Director, Strategic Partnerships&quot;],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7KZ3JdO5ODe3FLdN8ng1Nc/0dbaf23c66ad71fd5b338587ce057ae2/nikhil.jpeg&quot;],&quot;location&quot;:[0,&quot;San Francisco&quot;],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,null],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;We&#39;re expanding Workers AI with new partner models from Leonardo.Ai and Deepgram. Start using state-of-the-art image generation models from Leonardo (Phoenix, Lucid Origin) and real-time TTS and STT models from Deepgram (Nova 3, Aura 1). Build full-stack, low-latency AI applications, all hosted on Cloudflare&#39;s global network.\n&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/workers-ai-partner-models&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;State-of-the-art image generation Leonardo models and text-to-speech Deepgram models now available in Workers AI&quot;],&quot;description&quot;:[0,&quot;We&#39;re expanding Workers AI with new partner models from Leonardo.Ai and Deepgram. Start using state-of-the-art image generation models from Leonardo (Phoenix, Lucid Origin) and real-time TTS and STT models from Deepgram (Nova 3, Aura 1). Build full-stack, low-latency AI applications, all hosted on Cloudflare&#39;s global network.\n&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/32qiiUvYHhlCqWDaVDgSZm/267c07f4a2c664b7f4434103fbbf0c29/BLOG-2903_OG.png&quot;]}],&quot;publicly_index&quot;:[0,true]}],[0,{&quot;id&quot;:[0,&quot;KjxPspfQBaaHQ5K8ALjv8&quot;],&quot;title&quot;:[0,&quot;How Cloudflare runs more AI models on fewer GPUs: A technical deep-dive &quot;],&quot;slug&quot;:[0,&quot;how-cloudflare-runs-more-ai-models-on-fewer-gpus&quot;],&quot;excerpt&quot;:[0,&quot;Cloudflare built an internal platform called Omni. This platform uses lightweight isolation and memory over-commitment to run multiple AI models on a single GPU.&quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;As the demand for AI products grows, developers are creating and tuning a wider variety of models. While adding new models to our &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/\&quot;&gt;&lt;u&gt;growing catalog&lt;/u&gt;&lt;/a&gt; on Workers AI, we noticed that not all of them are used equally â leaving infrequently used models occupying valuable GPU space. Efficiency is a core value at Cloudflare, and with GPUs being the scarce commodity they are, we realized that we needed to build something to fully maximize our GPU usage.&lt;/p&gt;&lt;p&gt;Omni is an internal platform weâve built for running and managing AI models on Cloudflareâs edge nodes. It does so by spawning and managing multiple models on a single machine and GPU using lightweight isolation. Omni makes it easy and efficient to run many small and/or low-volume models, combining multiple capabilities by:Â Â &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Spawning multiple models from a single control plane,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Implementing lightweight process isolation, allowing models to spin up and down quickly,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Isolating the file system between models to easily manage per-model dependencies, and&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Over-committing GPU memory to run more models on a single GPU.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Cloudflare aims to place GPUs as close as we possibly can to people and applications that are using them. With Omni in place, weâre now able to run more models on every node in our network, improving model availability, minimizing latency, and reducing power consumed by idle GPUs.&lt;/p&gt;&lt;p&gt;Hereâs how.Â &lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;omnis-architecture-at-a-glance\&quot;&gt;Omniâs architecture â at a glance&lt;/h2&gt;\n      &lt;a href=\&quot;#omnis-architecture-at-a-glance\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;At a high level, Omni is a platform to run AI models. When an &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/inference-vs-training/\&quot;&gt;&lt;u&gt;inference&lt;/u&gt;&lt;/a&gt; request is made on Workers AI, we load the modelâs configuration from &lt;a href=\&quot;https://developers.cloudflare.com/kv/\&quot;&gt;&lt;u&gt;Workers KV&lt;/u&gt;&lt;/a&gt; and our routing layer forwards it to the closest Omni instance that has available capacity. For inferences using the &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/features/batch-api/\&quot;&gt;&lt;u&gt;Asynchronous Batch API&lt;/u&gt;&lt;/a&gt;, we route to an Omni instance that is idle, which is typically in a location where itâs night.&lt;/p&gt;&lt;p&gt;Omni runs a few checks on the inference request, runs model specific pre and post processing, then hands the request over to the model.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4zlObplZsGgpxPyUoD5NXe/ddd1cb8af444460d54fa5e0ab6e58c87/1.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;1938\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;elastic-scaling-by-spawning-multiple-models-from-a-single-control-plane\&quot;&gt;Elastic scaling by spawning multiple models from a single control plane&lt;/h2&gt;\n      &lt;a href=\&quot;#elastic-scaling-by-spawning-multiple-models-from-a-single-control-plane\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;If youâre developing an AI application, a typical setup is having a container or a VM dedicated to running a single model with a GPU attached to it. This is simple. But itâs also heavy-handed â because it requires managing the entire stack from provisioning the VM, installing GPU drivers, downloading model weights, and managing the Python environment. At scale, managing infrastructure this way is incredibly time consuming and often requires an entire team.Â &lt;/p&gt;&lt;p&gt;If youâre using Workers AI, we handle all of this for you. Omni uses a single control plane for running multiple models, called the scheduler, which automatically provisions models and spawns new instances as your traffic scales. When starting a new model instance, it downloads model weights, Python code, and any other dependencies. Omniâs scheduler provides fine-grained control and visibility over the modelâs lifecycle: it receives incoming inference requests and routes them to the corresponding model processes, being sure to distribute the load between multiple GPUs. It then makes sure the model processes are running, rolls out new versions as they are released, and restarts itself when detecting errors or failure states. It also collects metrics for billing and emits logs.&lt;/p&gt;&lt;p&gt;The inference itself is done by a per-model process, supervised by the scheduler. It receives the inference request and some metadata, then sends back a response. Depending on the model, the response can be various types; for instance, a JSON object or a SSE stream for text generation, or binary for image generation.&lt;/p&gt;&lt;p&gt;The scheduler and the child processes communicate by passing messages over Inter-Process Communication (IPC). Usually the inference request is buffered in the scheduler for applying features, like prompt templating or tool calling, before the request is passed to the child process. For potentially large binary requests, the scheduler hands over the underlying TCP connection to the child process for consuming the request body directly.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;implementing-lightweight-process-and-python-isolation\&quot;&gt;Implementing lightweight process and Python isolation&lt;/h2&gt;\n      &lt;a href=\&quot;#implementing-lightweight-process-and-python-isolation\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Typically, deploying a model requires its own dedicated container, but we want to colocate more models on a single container to conserve memory and GPU capacity. In order to do so, we needed finer-grained controls over CPU memory and the ability to isolate a model from its dependencies and environment. We deploy Omni in two configurations; a container running multiple models or bare metal running a single model. In both cases, process isolation and Python virtual environments allow us to isolate models with different dependencies by creating namespaces and are limited by &lt;a href=\&quot;https://en.wikipedia.org/wiki/Cgroups\&quot;&gt;&lt;u&gt;cgroups&lt;/u&gt;&lt;/a&gt;.Â &lt;/p&gt;&lt;p&gt;Python doesnât take into account cgroups memory limits for memory allocations, which can lead to OOM errors. Many AI Python libraries rely on &lt;a href=\&quot;https://pypi.org/project/psutil/\&quot;&gt;&lt;u&gt;psutil&lt;/u&gt;&lt;/a&gt; for pre-allocating CPU memory. psutil reads /proc/meminfo to determine how much memory is available. Since in Omni each model has its own configurable memory limits, we need psutil to reflect the current usage and limits for a given model, not for the entire system.&lt;/p&gt;&lt;p&gt;The solution for us was to create a virtual file system, using &lt;a href=\&quot;https://en.wikipedia.org/wiki/Filesystem_in_Userspace\&quot;&gt;&lt;u&gt;fuse&lt;/u&gt;&lt;/a&gt;, to mount our own version of /proc/meminfo which reflects the modelâs current usage and limits.&lt;/p&gt;&lt;p&gt;To illustrate this, hereâs an Omni instance running a model (running as pid 8). If we enter the mount namespace and look at /proc/meminfo it will reflect the modelâs configuration:&lt;/p&gt;\n            &lt;pre class=\&quot;language-Rust\&quot;&gt;&lt;code class=\&quot;language-Rust\&quot;&gt;# Enter the mount (file system) namespace of a child process\n$ nsenter -t 8 -m\n\n$ mount\n...\nnone /proc/meminfo fuse ...\n\n$ cat /proc/meminfo\nMemTotal:     7340032 kB\nMemFree:     7316388 kB\nMemAvailable:     7316388 kB&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;In this case the model has 7Gib of memory available and the entire container 15Gib. If the model tries to allocate more than 7Gib of memory, it will be OOM killed and restarted by the schedulerâs process manager, without causing any problems to the other models.&lt;/p&gt;&lt;p&gt;For isolating Python and some system dependencies, each model runs in a Python virtual environment, managed by &lt;a href=\&quot;https://docs.astral.sh/uv/\&quot;&gt;&lt;u&gt;uv&lt;/u&gt;&lt;/a&gt;. Dependencies are cached on the machine and, if possible, shared between models (uv uses symbolic links between its cache and virtual environments).&lt;/p&gt;&lt;p&gt;Also separated processes for models allows to have different CUDA contexts and isolation for error recovery.Â &lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;over-committing-memory-to-run-more-models-on-a-single-gpu\&quot;&gt;Over-committing memory to run more models on a single GPU&lt;/h2&gt;\n      &lt;a href=\&quot;#over-committing-memory-to-run-more-models-on-a-single-gpu\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Some models donât receive enough traffic to fully utilize a GPU, and with Omni we can pack more models on a single GPU, freeing up capacity for other workloads. When it comes to GPU memory management, Omni has two main jobs: safely over-commit GPU memory, so that more models than normal can share a single GPU, and enforce memory limits, to prevent any single model from running out of memory while running.Â Â Â Â Â Â &lt;/p&gt;&lt;p&gt;Over-committing memory means allocating more memory than is physically available to the device.Â &lt;/p&gt;&lt;p&gt;For example, if a GPU has 10 Gib of memory, Omni would allow 2 models of 10Gib each on that GPU.&lt;/p&gt;&lt;p&gt;Right now, Omni is configured to run 13 models and is allocating about 400% GPU memory on a single GPU, saving up 4 GPUs. Omni does this by injecting a CUDA stub library that intercepts CUDA memory allocations (cuMalloc* or cudaMalloc*) calls and forces memory allocations to be performed in &lt;a href=\&quot;https://developer.nvidia.com/blog/unified-memory-in-cuda-6/\&quot;&gt;&lt;u&gt;unified memory mode&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In Unified memory mode CUDA shares the same memory address space for both the GPU and the CPU:&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2G5zd0TDi15ZeFAmcJy812/1b292429140ec2c4bd0a81bee4954150/2.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;588\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;&lt;sup&gt;&lt;i&gt;CUDAâs &lt;/i&gt;&lt;/sup&gt;&lt;a href=\&quot;https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/\&quot;&gt;&lt;sup&gt;&lt;i&gt;&lt;u&gt;unified memory mode&lt;/u&gt;&lt;/i&gt;&lt;/sup&gt;&lt;/a&gt;&lt;sup&gt;&lt;i&gt;Â &lt;/i&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;In practice this is what memory over-commitment looks like: imagine 3 models (A, B and C). Models A+B fit in the GPUâs memory but C takes up the entire memory.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Models A+B are loaded first and are in GPU memory, while model C is in CPU memory&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/xU141x0PaZRp83XlF6hWz/527915ee03309f619a64e6b43c62cd92/3.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1220\&quot; height=\&quot;450\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Omni receives a request for model C so models A+B are swapped out and C is swapped in.\n&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4fD3Y2xyyawGmo1gpdLsQz/1cd36ebaed6b7f9e95b3d31ead1c1098/4.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1220\&quot; height=\&quot;468\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Omni receives a request for model B, so model C is partly swapped out and model B is swapped back in.\n&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2v5JjDW0NCkVUfEBXwIgpL/62009bc970b0967a850cb31ef87be44b/5.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1220\&quot; height=\&quot;450\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Omni receives a request for model A, so model A is swapped back in and model C is completely swapped out.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3cWGbEGgv3QckT7jgUIs9d/2c500a432be451a83dce0c71ccdcb89f/6.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1220\&quot; height=\&quot;450\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;The trade-off is added latency: if performing an inference requires memory that is currently on the host system, it must be transferred to the GPU. For smaller models, this latency is minimal, because with PCIe 4.0, the physical bus between your GPU and system, provides 32 GB/sec of bandwidth. On the other hand, if a model need to be âcold startedâ i.e. itâs been swapped out because it hasnât been used in a while, the system may need to swap back the entire model âÂ a larger sized model, for example, might use 5Gib of GPU memory for weights and caches, and would take ~156ms to be swapped back into the GPU. Naturally, over time, inactive models are put into CPU memory, while active models stay hot in the GPU.&lt;/p&gt;&lt;p&gt;Rather than allowing the model to choose how much GPU memory it uses, AI frameworks tend to pre-allocate as much GPU memory as possible for performance reasons, making co-locating models more complicated. Omni allows us to control how much memory is actually exposed to any given model to prevent a greedy model from over-using the GPU allocated to it. We do this by overriding the CUDA runtime and driver APIs (&lt;a href=\&quot;https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g376b97f5ab20321ca46f7cfa9511b978\&quot;&gt;&lt;u&gt;cudaMemGetInfo&lt;/u&gt;&lt;/a&gt; and &lt;a href=\&quot;https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1g808f555540d0143a331cc42aa98835c0\&quot;&gt;&lt;u&gt;cuMemGetInfo&lt;/u&gt;&lt;/a&gt;). Instead of exposing the entire GPU memory, we only expose a subset of memory to each model.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;how-omni-runs-multiple-models-for-workers-ai\&quot;&gt;How Omni runs multiple models for Workers AIÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#how-omni-runs-multiple-models-for-workers-ai\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;AI models can run in a variety of inference engines or backends: &lt;a href=\&quot;https://github.com/vllm-project/vllm\&quot;&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt;, Python, and now our very own inference engine, &lt;a href=\&quot;http://blog.cloudflare.com/cloudflares-most-efficient-ai-inference-engine/\&quot;&gt;&lt;u&gt;Infire&lt;/u&gt;&lt;/a&gt;. While models have different capabilities, each model needs to support &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI features&lt;/u&gt;&lt;/a&gt;, like batching and function calling. Omni acts as a unified layer for integrating these systems. It integrates into our internal routing and scheduling systems, and provides a Python API for our engineering team to add new models more easily. Letâs take a closer look at how Omni does this in practice:&lt;/p&gt;\n            &lt;pre class=\&quot;language-Python\&quot;&gt;&lt;code class=\&quot;language-Python\&quot;&gt;from omni import Response\nimport cowsay\n\n\ndef handle_request(request, context):\n    try:\n        json = request.body.json\n        text = json[&amp;quot;text&amp;quot;]\n    except Exception as err:\n        return Response.error(...)\n\n    return cowsay.get_output_string(&amp;#039;cow&amp;#039;, text)&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;Similar to how a JavaScript Worker works, Omni calls a request handler, running the modelâs logic and returning a response.Â &lt;/p&gt;&lt;p&gt;Omni installs Python dependencies at model startup. We run an internal Python registry and mirror the public registry. In either case we declare dependencies in requirements.txt:&lt;/p&gt;\n            &lt;pre class=\&quot;language-Rust\&quot;&gt;&lt;code class=\&quot;language-Rust\&quot;&gt;cowsay==6.1&lt;/pre&gt;&lt;/code&gt;\n            &lt;p&gt;The handle_request function can be async and return different Python types, including &lt;a href=\&quot;https://docs.pydantic.dev/latest/\&quot;&gt;&lt;u&gt;pydantic&lt;/u&gt;&lt;/a&gt; objects. Omni will convert the return value into a Workers AI response for the eyeball.&lt;/p&gt;&lt;p&gt;A Python package is injected, named omni, containing all the Python APIs to interact with the request, the Workers AI systems, building Responses, error handling, etc. Internally we publish it as regular Python package to be used in standalone, for unit testing for instance:&lt;/p&gt;\n            &lt;pre class=\&quot;language-Rust\&quot;&gt;&lt;code class=\&quot;language-Rust\&quot;&gt;from omni import Context, Request\nfrom model import handle_request\n\n\ndef test_basic():\n    ctx = Context.inactive()\n    req = Request(json={&amp;quot;text&amp;quot;: &amp;quot;my dog is cooler than you!&amp;quot;})\n    out = handle_request(req, ctx)\n    assert out == &amp;quot;&amp;quot;&amp;quot;  __________________________\n| my dog is cooler than you! |\n  ==========================\n                          \\\\\n                           \\\\\n                             ^__^\n                             (oo)\\\\_______\n                             (__)\\\\       )\\\\/\\\\\n                                 ||----w |\n                                 ||     ||&amp;quot;&amp;quot;&amp;quot;&lt;/pre&gt;&lt;/code&gt;\n            \n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;whats-next\&quot;&gt;Whatâs nextÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#whats-next\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Omni allows us to run models more efficiently by spawning them from a single control plane and implementing lightweight process isolation. This enables quick starting and stopping of models, isolated file systems for managing Python and system dependencies, and over-committing GPU memory to run more models on a single GPU. This improves the performance for our entire Workers AI stack, reduces the cost of running GPUs, and allows us to ship new models and features quickly and safely.&lt;/p&gt;&lt;p&gt;Right now, Omni is running in production on a handful of models in the Workers AI catalog, and weâre adding more every week. Check out &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt; today to experience Omniâs performance benefits on your AI application.Â &lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-27T14:00+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-27T13:00:19.518Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2RpretWriqNKCyLuiMZeLp/37bb67184039c2958f3f56e97b06fe12/0.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;6Foe3R8of95cWVnQwe5Toi&quot;],&quot;name&quot;:[0,&quot;AI&quot;],&quot;slug&quot;:[0,&quot;ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Sven Sauleau&quot;],&quot;slug&quot;:[0,&quot;sven&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4AvybzTkWLlF4cdTgLrE3q/0d45d87578c7668f3d62e49095bb4409/sven.jpg&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@svensauleau&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Mari Galicer&quot;],&quot;slug&quot;:[0,&quot;mari&quot;],&quot;bio&quot;:[0,&quot;Product Manager, Consumer Privacy&quot;],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6Gh4G4hhni5rwz8W2Nj7Ok/06696413b61cc3f15c37281d9670a723/mari.png&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@mmvri&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;In order to support a growing catalog of AI models while maximizing GPU utilization, Cloudflare built an internal platform called Omni. This platform uses lightweight isolation and memory over-commitment to run multiple AI models on a single GPU, allowing us to serve inference requests closer to users and improve overall availability across our network.&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/how-cloudflare-runs-more-ai-models-on-fewer-gpus&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;How Cloudflare runs more AI models on fewer GPUs:  A technical deep-dive&quot;],&quot;description&quot;:[0,&quot;In order to support a growing catalog of AI models while maximizing GPU utilization, Cloudflare built an internal platform called Omni. This platform uses lightweight isolation and memory over-commitment to run multiple AI models on a single GPU, allowing us to serve inference requests closer to users and improve overall availability across our network.&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5NQfIUcoNbNaOSA0GcPNYN/a6587e75c76a10f10ba60cef68f18399/OG_Share_2024__88_.png&quot;]}],&quot;publicly_index&quot;:[0,true]}],[0,{&quot;id&quot;:[0,&quot;7Li4fkq9b4B8QlgwSmZrqE&quot;],&quot;title&quot;:[0,&quot;How we built the most efficient inference engine for Cloudflareâs network &quot;],&quot;slug&quot;:[0,&quot;cloudflares-most-efficient-ai-inference-engine&quot;],&quot;excerpt&quot;:[0,&quot;Infire is an LLM inference engine that employs a range of techniques to maximize resource utilization, allowing us to serve AI models more efficiently with better performance for Cloudflare workloads.&quot;],&quot;featured&quot;:[0,false],&quot;html&quot;:[0,&quot;&lt;p&gt;Inference powers some of todayâs most powerful AI products: chat bot replies, &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/what-is-agentic-ai/\&quot;&gt;&lt;u&gt;AI agents&lt;/u&gt;&lt;/a&gt;, autonomous vehicle decisions, and fraud detection. The problem is, if youâre building one of these products on top of a hyperscaler, youâll likely need to rent expensive GPUs from large centralized data centers to run your inference tasks. That model doesnât work for Cloudflare âÂ thereâs a mismatch between Cloudflareâs globally-distributed network and a typical centralized AI deployment using large multi-GPU nodes. As a company that operates our own compute on a lean, fast, and widely distributed network within 50ms of 95% of the worldâs Internet-connected population, we need to be running inference tasks more efficiently than anywhere else.&lt;/p&gt;&lt;p&gt;This is further compounded by the fact that AI models are getting larger and more complex. As we started to support these models, like the Llama 4 herd and gpt-oss, we realized that we couldnât just throw money at the scaling problems by buying more GPUs. We needed to utilize every bit of idle capacity and be agile with where each model is deployed.Â &lt;/p&gt;&lt;p&gt;After running most of our models on the widely used open source inference and serving engine &lt;a href=\&quot;https://github.com/vllm-project/vllm\&quot;&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt;, we figured out it didnât allow us to fully utilize the GPUs at the edge. Although it can run on a very wide range of hardware, from personal devices to data centers, it is best optimized for large data centers. When run as a dedicated inference server on powerful hardware serving a specific model, vLLM truly shines. However, it is much less optimized for dynamic workloads, distributed networks, and for the unique security constraints of running inference at the edge alongside other services.&lt;/p&gt;&lt;p&gt;Thatâs why we decided to build something that will be able to meet the needs of Cloudflare inference workloads for years to come. Infire is an LLM inference engine, written in Rust, that employs a range of techniques to maximize memory, network I/O, and GPU utilization. It can serve more requests with fewer GPUs and significantly lower CPU overhead, saving time, resources, and energy across our network.Â &lt;/p&gt;&lt;p&gt;Our initial benchmarking has shown that Infire completes inference tasks up to 7% faster than vLLM 0.10.0 on unloaded machines equipped with an H100 NVL GPU. On infrastructure under real load, it performs significantly better.Â &lt;/p&gt;&lt;p&gt;Currently, Infire is powering the Llama 3.1 8B model for &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt;, and you can test it out today at &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/llama-3.1-8b-instruct-fast/\&quot;&gt;&lt;u&gt;@cf/meta/llama-3.1-8b-instruct&lt;/u&gt;&lt;/a&gt;!&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;the-architectural-challenge-of-llm-inference-at-cloudflare\&quot;&gt;The Architectural Challenge of LLM Inference at CloudflareÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#the-architectural-challenge-of-llm-inference-at-cloudflare\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Thanks to industry efforts, inference has improved a lot over the past few years. vLLM has led the way here with the recent release of the vLLM V1 engine with features like an optimized KV cache, improved batching, and the implementation of Flash Attention 3. vLLM is great for most inference workloads â weâre currently using it for several of the models in our &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/models/\&quot;&gt;&lt;u&gt;Workers AI catalog&lt;/u&gt;&lt;/a&gt; âÂ but as our AI workloads and catalog has grown, so has our need to optimize inference for the exact hardware and performance requirements we have.Â &lt;/p&gt;&lt;p&gt;Cloudflare is writing much of our &lt;a href=\&quot;https://blog.cloudflare.com/rust-nginx-module/\&quot;&gt;&lt;u&gt;new infrastructure in Rust&lt;/u&gt;&lt;/a&gt;, and vLLM is written in Python. Although Python has proven to be a great language for prototyping ML workloads, to maximize efficiency we need to control the low-level implementation details. Implementing low-level optimizations through multiple abstraction layers and Python libraries adds unnecessary complexity and leaves a lot of CPU performance on the table, simply due to the inefficiencies of Python as an interpreted language.&lt;/p&gt;&lt;p&gt;We love to contribute to open-source projects that we use, but in this case our priorities may not fit the goals of the vLLM project, so we chose to write a server for our needs. For example, vLLM does not support co-hosting multiple models on the same GPU without using Multi-Instance GPU (MIG), and we need to be able to dynamically schedule multiple models on the same GPU to minimize downtime. We also have an in-house AI Research team exploring unique features that are difficult, if not impossible, to upstream to vLLM.Â &lt;/p&gt;&lt;p&gt;Finally, running code securely is our top priority across our platform and &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/workers-ai/\&quot;&gt;&lt;u&gt;Workers AI&lt;/u&gt;&lt;/a&gt; is no exception.Â We simply canât trust a 3rd party Python process to run on our edge nodes alongside the rest of our services without strong sandboxing. We are therefore forced to run vLLM via &lt;a href=\&quot;https://gvisor.dev\&quot;&gt;&lt;u&gt;gvisor&lt;/u&gt;&lt;/a&gt;. Having an extra virtualization layer adds an additional performance overhead to vLLM. More importantly, it also increases the startup and tear down time for vLLM instances â which are already pretty long. Under full load on our edge nodes, vLLM running via gvisor consumes as much as 2.5 CPU cores, and is forced to compete for CPU time with other crucial services, that in turn slows vLLM down and lowers GPU utilization as a result.&lt;/p&gt;&lt;p&gt;While developing Infire, weâve been incorporating the latest research in inference efficiency â letâs take a deeper look at what we actually built.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;how-infire-works-under-the-hood\&quot;&gt;How Infire works under the hoodÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#how-infire-works-under-the-hood\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Infire is composed of three major components: an OpenAI compatible HTTP server, a batcher, and the Infire engine itself.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3BypYSG9QFsPjPFhjlOEsa/6ef5d4ccaabcd96da03116b7a14e8439/image2.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1999\&quot; height=\&quot;676\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;&lt;i&gt;&lt;sup&gt;An overview of Infireâs architectureÂ &lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;platform-startup\&quot;&gt;Platform startup&lt;/h2&gt;\n      &lt;a href=\&quot;#platform-startup\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;When a model is first scheduled to run on a specific node in one of our data centers by our auto-scaling service, the first thing that has to happen is for the model weights to be fetched from our &lt;a href=\&quot;https://www.cloudflare.com/developer-platform/products/r2/\&quot;&gt;&lt;u&gt;R2 object storage&lt;/u&gt;&lt;/a&gt;. Once the weights are downloaded, they are cached on the edge node for future reuse.&lt;/p&gt;&lt;p&gt;As the weights become available either from cache or from R2, Infire can begin loading the model onto the GPU.Â &lt;/p&gt;&lt;p&gt;Model sizes vary greatly, but most of them are &lt;b&gt;large, &lt;/b&gt;so transferring them into GPU memory can be a time-consuming part of Infireâs startup process. For example, most non-quantized models store their weights in the BF16 floating point format. This format has the same dynamic range as the 32-bit floating format, but with reduced accuracy. It is perfectly suited for inference providing the sweet spot of size, performance and accuracy. As the name suggests, the BF16 format requires 16 bits, or 2 bytes per weight. The approximate in-memory size of a given model is therefore double the size of its parameters. For example, LLama3.1 8B has approximately 8B parameters, and its memory footprint is about 16GB. A larger model, like LLama4 Scout, has 109B parameters, and requires around 218GB of memory. Infire utilizes a combination of &lt;a href=\&quot;https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/#pinned_host_memory\&quot;&gt;&lt;u&gt;Page Locked&lt;/u&gt;&lt;/a&gt; memory with CUDA asynchronous copy mechanism over multiple streams to speed up model transfer into GPU memory.&lt;/p&gt;&lt;p&gt;While loading the model weights, Infire begins just-in-time compiling the required kernels based on the model&amp;#39;s parameters, and loads them onto the device. Parallelizing the compilation with model loading amortizes the latency of both processes. The startup time of Infire when loading the Llama-3-8B-Instruct model from disk is just under 4 seconds.Â &lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h3 id=\&quot;the-http-server\&quot;&gt;The HTTP server&lt;/h3&gt;\n      &lt;a href=\&quot;#the-http-server\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;The Infire server is built on top of &lt;a href=\&quot;https://docs.rs/hyper/latest/hyper/\&quot;&gt;&lt;u&gt;hyper&lt;/u&gt;&lt;/a&gt;, a high performance HTTP crate, which makes it possible to handle hundreds of connections in parallel â while consuming a modest amount of CPU time. Because of ChatGPTâs ubiquity, vLLM and many other services offer OpenAI compatible endpoints out of the box. Infire is no different in that regard. The server is responsible for handling communication with the client: accepting connections, handling prompts and returning responses. A prompt will usually consist of some text, or a &amp;quot;transcript&amp;quot; of a chat session along with extra parameters that affect how the response is generated. Some parameters that come with a prompt include the temperature, which affects the randomness of the response, as well as other parameters that affect the randomness and length of a possible response.&lt;/p&gt;&lt;p&gt;After a request is deemed valid, Infire will pass it to the tokenizer, which transforms the raw text into a series of tokens, or numbers that the model can consume. Different models use different kinds of tokenizers, but the most popular ones use byte-pair encoding. For tokenization, we use HuggingFace&amp;#39;s tokenizers crate. The tokenized prompts and params are then sent to the batcher, and scheduled for processing on the GPU, where they will be processed as vectors of numbers, called &lt;a href=\&quot;https://www.cloudflare.com/learning/ai/what-are-embeddings/\&quot;&gt;&lt;u&gt;embeddings&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;the-batcher\&quot;&gt;The batcher&lt;/h2&gt;\n      &lt;a href=\&quot;#the-batcher\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;The most important part of Infire is in how it does batching: by executing multiple requests in parallel. This makes it possible to better utilize memory bandwidth and caches.Â &lt;/p&gt;&lt;p&gt;In order to understand why batching is so important, we need to understand how the inference algorithm works. The weights of a model are essentially a bunch of two-dimensional matrices (also called tensors). The prompt represented as vectors is passed through a series of transformations that are largely dominated by one operation: vector-by-matrix multiplication. The model weights are so large, that the cost of the multiplication is dominated by the time it takes to fetch it from memory. In addition, modern GPUs have hardware units dedicated to matrix-by-matrix multiplications (called Tensor Cores on Nvidia GPUs). In order to amortize the cost of memory access and take advantage of the Tensor Cores, it is necessary to aggregate multiple operations into a larger matrix multiplication.&lt;/p&gt;&lt;p&gt;Infire utilizes two techniques to increase the size of those matrix operations. The first one is called prefill: this technique is applied to the prompt tokens. Because all of the prompt tokens are available in advance and do not require decoding, they can all be processed in parallel. This is one reason why input tokens are often cheaper (and faster) than output tokens.&lt;/p&gt;\n          &lt;figure class=\&quot;kg-card kg-image-card\&quot;&gt;\n          &lt;Image src=\&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1pqyNSzgWLcgrV3urpCvA0/e204ac477992d591a7368632c36e97eb/image1.png\&quot; alt=\&quot;\&quot; class=\&quot;kg-image\&quot; width=\&quot;1718\&quot; height=\&quot;1220\&quot; loading=\&quot;lazy\&quot;/&gt;\n          &lt;/figure&gt;&lt;p&gt;&lt;sup&gt;&lt;i&gt;How Infire enables larger matrix multiplications via batching&lt;/i&gt;&lt;/sup&gt;&lt;/p&gt;&lt;p&gt;The other technique is called batching: this technique aggregates multiple prompts into a single decode operation.&lt;/p&gt;&lt;p&gt;Infire mixes both techniques. It attempts to process as many prompts as possible in parallel, and fills the remaining slots in a batch with prefill tokens from incoming prompts. This is also known as continuous batching with chunked prefill.&lt;/p&gt;&lt;p&gt;As tokens get decoded by the Infire engine, the batcher is also responsible for retiring prompts that reach an End of Stream token, and sending tokens back to the decoder to be converted into text.Â &lt;/p&gt;&lt;p&gt;Another job the batcher has is handling the KV cache. One demanding operation in the inference process is called &lt;i&gt;attention&lt;/i&gt;. Attention requires going over the KV values computed for all of the tokens up to the current one. If we had to recompute those previously encountered KV values for every new token we decode, the runtime of the process would explode for longer context sizes. However, using a cache, we can store all of the previous values and re-read them for each consecutive token. Potentially the KV cache for a prompt can store KV values for as many tokens as the context window allows. In LLama 3, the maximal context window is 128K tokens. If we pre-allocated the KV cache for each prompt in advance, we would only have enough memory available to execute 4 prompts in parallel on H100 GPUs! The solution for this is paged KV cache. With paged KV caching, the cache is split into smaller chunks called pages. When the batcher detects that a prompt would exceed its KV cache, it simply assigns another page to that prompt. Since most prompts rarely hit the maximum context window, this technique allows for essentially unlimited parallelism under typical load.&lt;/p&gt;&lt;p&gt;Finally, the batcher drives the Infire forward pass by scheduling the needed kernels to run on the GPU.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;cuda-kernels\&quot;&gt;CUDA kernels&lt;/h2&gt;\n      &lt;a href=\&quot;#cuda-kernels\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Developing Infire gives us the luxury of focusing on the exact hardware we use, which is currently Nvidia Hopper GPUs. This allowed us to improve performance of specific compute kernels using low-level PTX instructions for this specific architecture.&lt;/p&gt;&lt;p&gt;Infire just-in-time compiles its kernel for the specific model it is running, optimizing for the modelâs parameters, such as the hidden state size, dictionary size and the GPU it is running on. For some operations, such as large matrix multiplications, Infire will utilize the high performance cuBLASlt library, if it would deem it faster.&lt;/p&gt;&lt;p&gt;Infire also makes use of very fine-grained CUDA graphs, essentially creating a dedicated CUDA graph for every possible batch size on demand. It then stores it for future launch. Conceptually, a CUDA graph is another form of just-in-time compilation: the CUDA driver replaces a series of kernel launches with a single construct (the graph) that has a significantly lower amortized kernel launch cost, thus kernels executed back to back will execute faster when launched as a single graph as opposed to individual launches.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;how-infire-performs-in-the-wild\&quot;&gt;How Infire performs in the wildÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#how-infire-performs-in-the-wild\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;We ran synthetic benchmarks on one of our edge nodes with an H100 NVL GPU.&lt;/p&gt;&lt;p&gt;The benchmark we ran was on the widely used ShareGPT v3 dataset. We ran the benchmark on a set of 4,000 prompts with a concurrency of 200. We then compared Infire and vLLM running on bare metal as well as vLLM running under gvisor, which is the way we currently run in production. In a production traffic scenario, an edge node would be competing for resources with other traffic. To simulate this, we benchmarked vLLM running in gvisor with only one CPU available.&lt;/p&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;\n&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;requests/s&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;tokens/s&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;CPU load&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Infire&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;40.91&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;17224.21&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;25%&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;vLLM 0.10.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;38.38&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;16164.41&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;140%&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;vLLM under gvisor&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;37.13&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;15637.32&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;250%&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;vLLM under gvisor with CPU constraints&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;22.04&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;9279.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;100%&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;As evident from the benchmarks we achieved our initial goal of matching and even slightly surpassing vLLM performance, but more importantly, weâve done so at a significantly lower CPU usage, in large part because we can run Infire as a trusted bare-metal process. Inference no longer takes away precious resources from our other services and we see GPU utilization upward of 80%, reducing our operational costs.&lt;/p&gt;&lt;p&gt;This is just the beginning. There are still multiple proven performance optimizations yet to be implemented in Infire â for example, weâre integrating Flash Attention 3, and most of our kernels donât utilize kernel fusion. Those and other optimizations will allow us to unlock even faster inference in the near future.&lt;/p&gt;\n    &lt;div class=\&quot;flex anchor relative\&quot;&gt;\n      &lt;h2 id=\&quot;whats-next\&quot;&gt;Whatâs nextÂ &lt;/h2&gt;\n      &lt;a href=\&quot;#whats-next\&quot; aria-hidden=\&quot;true\&quot; class=\&quot;relative sm:absolute sm:-left-5\&quot;&gt;\n        &lt;svg width=\&quot;16\&quot; height=\&quot;16\&quot; viewBox=\&quot;0 0 24 24\&quot;&gt;&lt;path fill=\&quot;currentcolor\&quot; d=\&quot;m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z\&quot;&gt;&lt;/path&gt;&lt;/svg&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n    &lt;p&gt;Running AI inference presents novel challenges and demands to our infrastructure. Infire is how weâre running AI efficiently â close to users around the world. By building upon techniques like continuous batching, a paged KV-cache, and low-level optimizations tailored to our hardware, Infire maximizes GPU utilization while minimizing overhead. Infire completes inference tasks faster and with a fraction of the CPU load of our previous vLLM-based setup, especially under the strict security constraints we require. This allows us to serve more requests with fewer resources, making requests served via Workers AI faster and more efficient.&lt;/p&gt;&lt;p&gt;However, this is just our first iteration â weâre excited to build in multi-GPU support for larger models, quantization, and true multi-tenancy into the next version of Infire. This is part of our goal to make Cloudflare the best possible platform for developers to build AI applications.&lt;/p&gt;&lt;p&gt;Want to see if your AI workloads are faster on Cloudflare? &lt;a href=\&quot;https://developers.cloudflare.com/workers-ai/\&quot;&gt;&lt;u&gt;Get started&lt;/u&gt;&lt;/a&gt; with Workers AI today.Â &lt;/p&gt;&quot;],&quot;published_at&quot;:[0,&quot;2025-08-27T14:00+00:00&quot;],&quot;updated_at&quot;:[0,&quot;2025-08-27T13:00:16.881Z&quot;],&quot;feature_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4ktpxBoVmznE2J8B8F3Osq/2ebd55545ecaafd3ed66257234e0520e/image3.png&quot;],&quot;tags&quot;:[1,[[0,{&quot;id&quot;:[0,&quot;5XfXk7guhMbUfWq3t9LIib&quot;],&quot;name&quot;:[0,&quot;AI Week&quot;],&quot;slug&quot;:[0,&quot;ai-week&quot;]}],[0,{&quot;id&quot;:[0,&quot;6gMpGK5HugYKaxJbvTMOHp&quot;],&quot;name&quot;:[0,&quot;LLM&quot;],&quot;slug&quot;:[0,&quot;llm&quot;]}],[0,{&quot;id&quot;:[0,&quot;1Wf1Dpb2AFicG44jpRT29y&quot;],&quot;name&quot;:[0,&quot;Workers AI&quot;],&quot;slug&quot;:[0,&quot;workers-ai&quot;]}]]],&quot;relatedTags&quot;:[0],&quot;authors&quot;:[1,[[0,{&quot;name&quot;:[0,&quot;Vlad Krasnov&quot;],&quot;slug&quot;:[0,&quot;vlad-krasnov&quot;],&quot;bio&quot;:[0,null],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4aDp3XtkBoeuPj5b9e5oRE/084f9a39036700298e0a62b5e8c2aa3c/vlad-krasnov.jpg&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,null],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}],[0,{&quot;name&quot;:[0,&quot;Mari Galicer&quot;],&quot;slug&quot;:[0,&quot;mari&quot;],&quot;bio&quot;:[0,&quot;Product Manager, Consumer Privacy&quot;],&quot;profile_image&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6Gh4G4hhni5rwz8W2Nj7Ok/06696413b61cc3f15c37281d9670a723/mari.png&quot;],&quot;location&quot;:[0,null],&quot;website&quot;:[0,null],&quot;twitter&quot;:[0,&quot;@mmvri&quot;],&quot;facebook&quot;:[0,null],&quot;publiclyIndex&quot;:[0,true]}]]],&quot;meta_description&quot;:[0,&quot;Existing LLM serving engines arenât efficient enough when deployed across a globally distributed network, so we built our own, in Rust. Infire is an LLM inference engine that employs a range of techniques to maximize resource utilization, allowing us to serve AI models more efficiently and with better performance for Cloudflare workloads.&quot;],&quot;primary_author&quot;:[0,{}],&quot;localeList&quot;:[0,{&quot;name&quot;:[0,&quot;blog-english-only&quot;],&quot;enUS&quot;:[0,&quot;English for Locale&quot;],&quot;zhCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhHansCN&quot;:[0,&quot;No Page for Locale&quot;],&quot;zhTW&quot;:[0,&quot;No Page for Locale&quot;],&quot;frFR&quot;:[0,&quot;No Page for Locale&quot;],&quot;deDE&quot;:[0,&quot;No Page for Locale&quot;],&quot;itIT&quot;:[0,&quot;No Page for Locale&quot;],&quot;jaJP&quot;:[0,&quot;No Page for Locale&quot;],&quot;koKR&quot;:[0,&quot;No Page for Locale&quot;],&quot;ptBR&quot;:[0,&quot;No Page for Locale&quot;],&quot;esLA&quot;:[0,&quot;No Page for Locale&quot;],&quot;esES&quot;:[0,&quot;No Page for Locale&quot;],&quot;enAU&quot;:[0,&quot;No Page for Locale&quot;],&quot;enCA&quot;:[0,&quot;No Page for Locale&quot;],&quot;enIN&quot;:[0,&quot;No Page for Locale&quot;],&quot;enGB&quot;:[0,&quot;No Page for Locale&quot;],&quot;idID&quot;:[0,&quot;No Page for Locale&quot;],&quot;ruRU&quot;:[0,&quot;No Page for Locale&quot;],&quot;svSE&quot;:[0,&quot;No Page for Locale&quot;],&quot;viVN&quot;:[0,&quot;No Page for Locale&quot;],&quot;plPL&quot;:[0,&quot;No Page for Locale&quot;],&quot;arAR&quot;:[0,&quot;No Page for Locale&quot;],&quot;nlNL&quot;:[0,&quot;No Page for Locale&quot;],&quot;thTH&quot;:[0,&quot;No Page for Locale&quot;],&quot;trTR&quot;:[0,&quot;No Page for Locale&quot;],&quot;heIL&quot;:[0,&quot;No Page for Locale&quot;],&quot;lvLV&quot;:[0,&quot;No Page for Locale&quot;],&quot;etEE&quot;:[0,&quot;No Page for Locale&quot;],&quot;ltLT&quot;:[0,&quot;No Page for Locale&quot;]}],&quot;url&quot;:[0,&quot;https://blog.cloudflare.com/cloudflares-most-efficient-ai-inference-engine&quot;],&quot;metadata&quot;:[0,{&quot;title&quot;:[0,&quot;How we built the most efficient inference engine for Cloudflareâs network&quot;],&quot;description&quot;:[0,&quot;Existing LLM serving engines arenât efficient enough when deployed across a globally distributed network, so we built our own, in Rust. Infire is an LLM inference engine that employs a range of techniques to maximize resource utilization, allowing us to serve AI models more efficiently and with better performance for Cloudflare workloads.&quot;],&quot;imgPreview&quot;:[0,&quot;https://cf-assets.www.cloudflare.com/zkvhlag99gkb/612AdtnKXBlVmSe2mhRpsf/abcfae131a49e2bceae1a770708ac0bd/OG_Share_2024__87_.png&quot;]}],&quot;publicly_index&quot;:[0,true]}]]],&quot;locale&quot;:[0,&quot;en-us&quot;],&quot;translations&quot;:[0,{&quot;posts.by&quot;:[0,&quot;By&quot;],&quot;footer.gdpr&quot;:[0,&quot;GDPR&quot;],&quot;lang_blurb1&quot;:[0,&quot;This post is also available in {lang1}.&quot;],&quot;lang_blurb2&quot;:[0,&quot;This post is also available in {lang1} and {lang2}.&quot;],&quot;lang_blurb3&quot;:[0,&quot;This post is also available in {lang1}, {lang2} and {lang3}.&quot;],&quot;footer.press&quot;:[0,&quot;Press&quot;],&quot;header.title&quot;:[0,&quot;The Cloudflare Blog&quot;],&quot;search.clear&quot;:[0,&quot;Clear&quot;],&quot;search.filter&quot;:[0,&quot;Filter&quot;],&quot;search.source&quot;:[0,&quot;Source&quot;],&quot;footer.careers&quot;:[0,&quot;Careers&quot;],&quot;footer.company&quot;:[0,&quot;Company&quot;],&quot;footer.support&quot;:[0,&quot;Support&quot;],&quot;footer.the_net&quot;:[0,&quot;theNet&quot;],&quot;search.filters&quot;:[0,&quot;Filters&quot;],&quot;footer.our_team&quot;:[0,&quot;Our team&quot;],&quot;footer.webinars&quot;:[0,&quot;Webinars&quot;],&quot;page.more_posts&quot;:[0,&quot;More posts&quot;],&quot;posts.time_read&quot;:[0,&quot;{time} min read&quot;],&quot;search.language&quot;:[0,&quot;Language&quot;],&quot;footer.community&quot;:[0,&quot;Community&quot;],&quot;footer.resources&quot;:[0,&quot;Resources&quot;],&quot;footer.solutions&quot;:[0,&quot;Solutions&quot;],&quot;footer.trademark&quot;:[0,&quot;Trademark&quot;],&quot;header.subscribe&quot;:[0,&quot;Subscribe&quot;],&quot;footer.compliance&quot;:[0,&quot;Compliance&quot;],&quot;footer.free_plans&quot;:[0,&quot;Free plans&quot;],&quot;footer.impact_ESG&quot;:[0,&quot;Impact/ESG&quot;],&quot;posts.follow_on_X&quot;:[0,&quot;Follow on X&quot;],&quot;footer.help_center&quot;:[0,&quot;Help center&quot;],&quot;footer.network_map&quot;:[0,&quot;Network Map&quot;],&quot;header.please_wait&quot;:[0,&quot;Please Wait&quot;],&quot;page.related_posts&quot;:[0,&quot;Related posts&quot;],&quot;search.result_stat&quot;:[0,&quot;Results &lt;strong&gt;{search_range}&lt;/strong&gt; of &lt;strong&gt;{search_total}&lt;/strong&gt; for &lt;strong&gt;{search_keyword}&lt;/strong&gt;&quot;],&quot;footer.case_studies&quot;:[0,&quot;Case Studies&quot;],&quot;footer.connect_2024&quot;:[0,&quot;Connect 2024&quot;],&quot;footer.terms_of_use&quot;:[0,&quot;Terms of Use&quot;],&quot;footer.white_papers&quot;:[0,&quot;White Papers&quot;],&quot;footer.cloudflare_tv&quot;:[0,&quot;Cloudflare TV&quot;],&quot;footer.community_hub&quot;:[0,&quot;Community Hub&quot;],&quot;footer.compare_plans&quot;:[0,&quot;Compare plans&quot;],&quot;footer.contact_sales&quot;:[0,&quot;Contact Sales&quot;],&quot;header.contact_sales&quot;:[0,&quot;Contact Sales&quot;],&quot;header.email_address&quot;:[0,&quot;Email Address&quot;],&quot;page.error.not_found&quot;:[0,&quot;Page not found&quot;],&quot;footer.developer_docs&quot;:[0,&quot;Developer docs&quot;],&quot;footer.privacy_policy&quot;:[0,&quot;Privacy Policy&quot;],&quot;footer.request_a_demo&quot;:[0,&quot;Request a demo&quot;],&quot;page.continue_reading&quot;:[0,&quot;Continue reading&quot;],&quot;footer.analysts_report&quot;:[0,&quot;Analyst reports&quot;],&quot;footer.for_enterprises&quot;:[0,&quot;For enterprises&quot;],&quot;footer.getting_started&quot;:[0,&quot;Getting Started&quot;],&quot;footer.learning_center&quot;:[0,&quot;Learning Center&quot;],&quot;footer.project_galileo&quot;:[0,&quot;Project Galileo&quot;],&quot;pagination.newer_posts&quot;:[0,&quot;Newer Posts&quot;],&quot;pagination.older_posts&quot;:[0,&quot;Older Posts&quot;],&quot;posts.social_buttons.x&quot;:[0,&quot;Discuss on X&quot;],&quot;search.icon_aria_label&quot;:[0,&quot;Search&quot;],&quot;search.source_location&quot;:[0,&quot;Source/Location&quot;],&quot;footer.about_cloudflare&quot;:[0,&quot;About Cloudflare&quot;],&quot;footer.athenian_project&quot;:[0,&quot;Athenian Project&quot;],&quot;footer.become_a_partner&quot;:[0,&quot;Become a partner&quot;],&quot;footer.cloudflare_radar&quot;:[0,&quot;Cloudflare Radar&quot;],&quot;footer.network_services&quot;:[0,&quot;Network services&quot;],&quot;footer.trust_and_safety&quot;:[0,&quot;Trust &amp; Safety&quot;],&quot;header.get_started_free&quot;:[0,&quot;Get Started Free&quot;],&quot;page.search.placeholder&quot;:[0,&quot;Search Cloudflare&quot;],&quot;footer.cloudflare_status&quot;:[0,&quot;Cloudflare Status&quot;],&quot;footer.cookie_preference&quot;:[0,&quot;Cookie Preferences&quot;],&quot;header.valid_email_error&quot;:[0,&quot;Must be valid email.&quot;],&quot;search.result_stat_empty&quot;:[0,&quot;Results &lt;strong&gt;{search_range}&lt;/strong&gt; of &lt;strong&gt;{search_total}&lt;/strong&gt;&quot;],&quot;footer.connectivity_cloud&quot;:[0,&quot;Connectivity cloud&quot;],&quot;footer.developer_services&quot;:[0,&quot;Developer services&quot;],&quot;footer.investor_relations&quot;:[0,&quot;Investor relations&quot;],&quot;page.not_found.error_code&quot;:[0,&quot;Error Code: 404&quot;],&quot;search.autocomplete_title&quot;:[0,&quot;Insert a query. Press enter to send&quot;],&quot;footer.logos_and_press_kit&quot;:[0,&quot;Logos &amp; press kit&quot;],&quot;footer.application_services&quot;:[0,&quot;Application services&quot;],&quot;footer.get_a_recommendation&quot;:[0,&quot;Get a recommendation&quot;],&quot;posts.social_buttons.reddit&quot;:[0,&quot;Discuss on Reddit&quot;],&quot;footer.sse_and_sase_services&quot;:[0,&quot;SSE and SASE services&quot;],&quot;page.not_found.outdated_link&quot;:[0,&quot;You may have used an outdated link, or you may have typed the address incorrectly.&quot;],&quot;footer.report_security_issues&quot;:[0,&quot;Report Security Issues&quot;],&quot;page.error.error_message_page&quot;:[0,&quot;Sorry, we can&#39;t find the page you are looking for.&quot;],&quot;header.subscribe_notifications&quot;:[0,&quot;Subscribe to receive notifications of new posts:&quot;],&quot;footer.cloudflare_for_campaigns&quot;:[0,&quot;Cloudflare for Campaigns&quot;],&quot;header.subscription_confimation&quot;:[0,&quot;Subscription confirmed. Thank you for subscribing!&quot;],&quot;posts.social_buttons.hackernews&quot;:[0,&quot;Discuss on Hacker News&quot;],&quot;footer.diversity_equity_inclusion&quot;:[0,&quot;Diversity, equity &amp; inclusion&quot;],&quot;footer.critical_infrastructure_defense_project&quot;:[0,&quot;Critical Infrastructure Defense Project&quot;]}],&quot;localesAvailable&quot;:[1,[]],&quot;footerBlurb&quot;:[0,&quot;Cloudflare&#39;s connectivity cloud protects &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/network-services/&#39; rel=&#39;noreferrer&#39;&gt;entire corporate networks&lt;/a&gt;, helps customers build &lt;a target=&#39;_blank&#39; href=&#39;https://workers.cloudflare.com/&#39; rel=&#39;noreferrer&#39;&gt;Internet-scale applications efficiently&lt;/a&gt;, accelerates any &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/performance/accelerate-internet-applications/&#39; rel=&#39;noreferrer&#39;&gt;website or Internet application&lt;/a&gt;, &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/ddos/&#39; rel=&#39;noreferrer&#39;&gt;wards off DDoS attacks&lt;/a&gt;, keeps &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/application-security/&#39; rel=&#39;noreferrer&#39;&gt;hackers at bay&lt;/a&gt;, and can help you on &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/products/zero-trust/&#39; rel=&#39;noreferrer&#39;&gt;your journey to Zero Trust&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;Visit &lt;a target=&#39;_blank&#39; href=&#39;https://one.one.one.one/&#39; rel=&#39;noreferrer&#39;&gt;1.1.1.1&lt;/a&gt; from any device to get started with our free app that makes your Internet faster and safer.&lt;br/&gt;&lt;br/&gt;To learn more about our mission to help build a better Internet, &lt;a target=&#39;_blank&#39; href=&#39;https://www.cloudflare.com/learning/what-is-cloudflare/&#39; rel=&#39;noreferrer&#39;&gt;start here&lt;/a&gt;. If you&amp;apos;re looking for a new career direction, check out &lt;a target=&#39;_blank&#39; href=&#39;http://www.cloudflare.com/careers&#39; rel=&#39;noreferrer&#39;&gt;our open positions&lt;/a&gt;.&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;Post&quot;,&quot;value&quot;:true}" await-children><main id="post" class="flex flex-row flex-wrap items-center justify-center pt2 pt4-l"><article class="post-full mw-100 ph3 ph0-l fs-20px"><h1 class="f6 f7-l fw4 gray1 pt1 pt3-l mb1">Block unsafe prompts targeting your LLM endpoints with Firewall for AI</h1><p class="f3 fw5 gray5 db di-l mt2">2025-08-26</p><ul class="author-lists flex pl0 mt4"><li class="list flex items-center pr2 mb1-ns"><a href="/author/radwa/" class="static-avatar pr1"><img class="author-profile-image br-100 mr2" src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=64,height=64,gravity=face,fit=crop,zoom=0.5/https://cf-assets.www.cloudflare.com/zkvhlag99gkb/26ps7sSkjvnatLHbGNWuI9/42c3618ee2392f7d7a0dc5f335615fda/radwa.jpg" alt="Radwa Radwan" width="62" height="62"/></a><div class="author-name-tooltip"><a href="/author/radwa/" class="fw4 f3 no-underline black mr3">Radwa Radwan</a></div></li><li class="list flex items-center pr2 mb1-ns"><a href="/author/mathias-deschamps/" class="static-avatar pr1"><img class="author-profile-image br-100 mr2" src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=64,height=64,gravity=face,fit=crop,zoom=0.5/https://cf-assets.www.cloudflare.com/zkvhlag99gkb/43EtC0dFgs1vhmJxGLXQ5O/c625b7599d66afcf1890624b8108345a/Mathias_Deschamps.jpg" alt="Mathias Deschamps" width="62" height="62"/></a><div class="author-name-tooltip"><a href="/author/mathias-deschamps/" class="fw4 f3 no-underline black mr3">Mathias Deschamps</a></div></li></ul><section class="post-full-content"><div class="mb2 gray5">7 min read</div><img class="mr2" src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1VoeWz4uYyUO0N70CkENqB/eaf25c3382f436ba3d3e1b27da334cbc/image1.png" alt=""/><div class="post-content lh-copy gray1"><p>Security teams are racing to <a href="https://www.cloudflare.com/the-net/vulnerable-llm-ai/"><u>secure a new attack surface</u></a>: AI-powered applications. From chatbots to search assistants, LLMs are already shaping customer experience, but they also open the door to new risks. A single malicious prompt can exfiltrate sensitive data, <a href="https://www.cloudflare.com/learning/ai/data-poisoning/"><u>poison a model</u></a>, or inject toxic content into customer-facing interactions, undermining user trust. Without guardrails, even the best-trained model can be turned against the business.</p><p>Today, as part of AI Week, weâre expanding our AI security offerings by introducing unsafe content moderation, now integrated directly into Cloudflare <a href="https://developers.cloudflare.com/waf/detections/firewall-for-ai/"><u>Firewall for AI</u></a>. Built with Llama, this new feature allows customers to leverage their existing Firewall for AI engine for unified detection, analytics, and topic enforcement, providing real-time protection for <a href="https://www.cloudflare.com/learning/ai/what-is-large-language-model/"><u>Large Language Models (LLMs)</u></a> at the network level. Now with just a few clicks, security and application teams can detect and block harmful prompts or topics at the edge â eliminating the need to modify application code or infrastructure.

This feature is immediately available to current Firewall for AI users. Those not yet onboarded can contact their account team to participate in the beta program.</p>
    <div class="flex anchor relative">
      <h2 id="ai-protection-in-application-security">AI protection in application security</h2>
      <a href="#ai-protection-in-application-security" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>Cloudflare&#39;s Firewall for AI protects user-facing LLM applications from abuse and data leaks, addressing several of the <a href="https://www.cloudflare.com/learning/ai/owasp-top-10-risks-for-llms/"><u>OWASP Top 10 LLM risks</u></a> such as prompt injection, PII disclosure, and unbound consumption. It also extends protection to other risks such as unsafe or harmful content.</p><p>Unlike built-in controls that vary between model providers, Firewall for AI is model-agnostic. It sits in front of any model you choose, whether itâs from a third party like OpenAI or Gemini, one you run in-house, or a custom model you have built, and applies the same consistent protections.</p><p>Just like our origin-agnostic <a href="https://www.cloudflare.com/application-services/#application-services-case-products"><u>Application Security suite</u></a>, Firewall for AI enforces policies at scale across all your models, creating a unified security layer. That means you can define guardrails once and apply them everywhere. For example, a financial services company might require its LLM to only respond to finance-related questions, while blocking prompts about unrelated or sensitive topics, enforced consistently across every model in use.</p>
    <div class="flex anchor relative">
      <h2 id="unsafe-content-moderation-protects-businesses-and-users">Unsafe content moderation protects businesses and users</h2>
      <a href="#unsafe-content-moderation-protects-businesses-and-users" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>Effective AI moderation is more than blocking âbad wordsâ, itâs about setting boundaries that protect users, meeting legal obligations, and preserving brand integrity, without over-moderating in ways that silence important voices.</p><p>Because LLMs cannot be fully scripted, their interactions are inherently unpredictable. This flexibility enables rich user experiences but also opens the door to abuse.</p><p>Key risks from unsafe prompts include misinformation, biased or offensive content, and model poisoning, where repeated harmful prompts degrade the quality and safety of future outputs. Blocking these prompts aligns with the OWASP Top 10 for LLMs, preventing both immediate misuse and long-term degradation.</p><p>One example of this is<a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist"> <b><u>Microsoftâs Tay chatbot</u></b></a>. Trolls deliberately submitted toxic, racist, and offensive prompts, which Tay quickly began repeating. The failure was not only in Tayâs responses; it was in the lack of moderation on the inputs it accepted.</p>
    <div class="flex anchor relative">
      <h2 id="detecting-unsafe-prompts-before-reaching-the-model">Detecting unsafe prompts before reaching the model</h2>
      <a href="#detecting-unsafe-prompts-before-reaching-the-model" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>Cloudflare has integrated <a href="https://huggingface.co/meta-llama/Llama-Guard-3-8B"><u>Llama Guard</u></a> directly into Firewall for AI. This brings AI input moderation into the same rules engine our customers already use to protect their applications. It uses the same approach that we created for developers building with AI in our <a href="https://blog.cloudflare.com/guardrails-in-ai-gateway/"><u>AI Gateway</u></a> product.</p><p>Llama Guard analyzes prompts in real time and flags them across multiple safety categories, including hate, violence, sexual content, criminal planning, self-harm, and more.</p><p>With this integration, Firewall for AI not only <a href="https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#discovering-llm-powered-applications"><u>discovers LLM traffic</u></a> endpoints automatically, but also enables security and AI teams to take immediate action. Unsafe prompts can be blocked before they reach the model, while flagged content can be logged or reviewed for oversight and tuning. Content safety checks can also be combined with other Application Security protections, such as <a href="https://www.cloudflare.com/application-services/products/bot-management/"><u>Bot Management</u> </a>and <a href="https://www.cloudflare.com/application-services/products/rate-limiting/"><u>Rate Limiting</u></a>, to create layered defenses when protecting your model.</p><p>The result is a single, edge-native policy layer that enforces guardrails before unsafe prompts ever reach your infrastructure â without needing complex integrations.</p>
    <div class="flex anchor relative">
      <h2 id="how-it-works-under-the-hood">How it works under the hood</h2>
      <a href="#how-it-works-under-the-hood" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>Before diving into the architecture of Firewall for AI engine and how it fits within our previously mentioned module to detect <a href="https://blog.cloudflare.com/take-control-of-public-ai-application-security-with-cloudflare-firewall-for-ai/#using-workers-ai-to-deploy-presidio"><u>PII in the prompts</u></a>, letâs start with how we detect unsafe topics.</p>
    <div class="flex anchor relative">
      <h3 id="detection-of-unsafe-topics">Detection of unsafe topics</h3>
      <a href="#detection-of-unsafe-topics" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>A key challenge in building safety guardrails is balancing a good detection with model helpfulness. If detection is too broad, it can prevent a model from answering legitimate user questions, hurting its utility. This is especially difficult for topic detection because of the ambiguity and dynamic nature of human language, where context is fundamental to meaning.Â </p><p>Simple approaches like keyword blocklists are interesting for precise subjects â but insufficient. They are easily bypassed and fail to understand the context in which words are used, leading to poor recall. Older probabilistic models such as <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"><u>Latent Dirichlet Allocation (LDA)</u></a> were an improvement, but did not properly account for word ordering and other contextual nuances. 

Recent advancements in LLMs introduced a new paradigm. Their ability to perform zero-shot or few-shot classification is uniquely suited for the task of topic detection. For this reason, we chose <a href="https://huggingface.co/meta-llama/Llama-Guard-3-8B"><u>Llama Guard 3</u></a>, an open-source model based on the Llama architecture that is specifically fine-tuned for content safety classification. When it analyzes a prompt, it answers whether the text is safe or unsafe, and provides a specific category. We are showing the default categories, as listed <a href="https://developers.cloudflare.com/ruleset-engine/rules-language/fields/reference/cf.llm.prompt.unsafe_topic_categories/"><u>here</u></a>. Because Llama 3 has a fixed knowledge cutoff, certain categories â like defamation or elections â are time-sensitive. As a result, the model may not fully capture events or context that emerged after it was trained, and thatâs important to keep in mind when relying on it.</p><p>For now, we cover the 13 default categories. We plan to expand coverage in the future, leveraging the modelâs zero-shot capabilities.</p>
    <div class="flex anchor relative">
      <h3 id="a-scalable-architecture-for-future-detections">A scalable architecture for future detections</h3>
      <a href="#a-scalable-architecture-for-future-detections" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>We designed Firewall for AI to scale without adding noticeable latency, including Llama Guard, and this remains true even as we add new detection models.</p><p>To achieve this, we built a new asynchronous architecture. When a request is sent to an application protected by Firewall for AI, a Cloudflare Worker makes parallel, non-blocking requests to our different detection modules â one for PII, one for unsafe topics, and others as we add them.Â </p><p>Thanks to the Cloudflare network, this design scales to handle high request volumes out of the box, and latency does not increase as we add new detections. It will only be bounded by the slowest model used.Â </p>
          <figure class="kg-card kg-image-card">
          <Image src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4Y2gTP6teVR2263UIEWHc9/9a31fb394cee6c437c1d4af6f71d867c/image3.png" alt="" class="kg-image" width="1924" height="1300" loading="lazy"/>
          </figure><p>We optimize to keep the model utility at its maximum while keeping the guardrail detection broad enough.</p><p>Llama Guard is a rather large model, so running it at scale with minimal latency is a challenge. We deploy it on <a href="https://www.cloudflare.com/developer-platform/products/workers-ai/"><u>Workers AI</u></a>, leveraging our large fleet of high performance GPUs. This infrastructure ensures we can offer fast, reliable inference throughout our network.</p><p>To ensure the system remains fast and reliable as adoption grows, we ran extensive load tests simulating the requests per second (RPS) we anticipate, using a wide range of prompt sizes to prepare for real-world traffic. To handle this, the number of model instances deployed on our network scales automatically with the load. We employ concurrency to minimize latency and optimize for hardware utilization. We also enforce a hard 2-second threshold for each analysis; if this time limit is reached, we fall back to any detections already completed, ensuring your application&#39;s requests latency is never further impacted.</p>
    <div class="flex anchor relative">
      <h3 id="from-detection-to-security-rules-enforcement">From detection to security rules enforcement</h3>
      <a href="#from-detection-to-security-rules-enforcement" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>Firewall for AI follows the same familiar pattern as other Application Security features like Bot Management and WAF Attack Score, making it easy to adopt.</p><p>Once enabled, the <a href="https://developers.cloudflare.com/waf/detections/firewall-for-ai/#fields"><u>new fields</u></a> appear in <a href="https://developers.cloudflare.com/waf/analytics/security-analytics/"><u>Security Analytics</u></a> and expanded logs. From there, you can filter by unsafe topics, track trends over time, and drill into the results of individual requests to see all detection outcomes, for example: did we detect unsafe topics, and what are the categories. The request body itself (the prompt text) is not stored or exposed; only the results of the analysis are logged.</p>
          <figure class="kg-card kg-image-card">
          <Image src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/722JxyLvT6DFQxFpQhHMYP/3f1a6aa8ef1dafe4ad1a8277578fd7ae/image2.png" alt="" class="kg-image" width="1125" height="818" loading="lazy"/>
          </figure><p>After reviewing the analytics, you can enforce unsafe topic moderation by creating rules to log or block based on prompt categories in <a href="https://developers.cloudflare.com/waf/custom-rules/"><u>Custom rules</u></a>.</p><p>For example, you might log prompts flagged as sexual content or hate speech for review.Â </p><p>You can use this expression: 
<code>If (any(cf.llm.prompt.unsafe_topic_categories[*] in {&quot;S10&quot; &quot;S12&quot;})) then Log</code>

Or deploy the rule with the categories field in the dashboard as in the below screenshot.</p>
          <figure class="kg-card kg-image-card">
          <Image src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2CUsVjjpCEqv2UQMU6cMmt/5307235338c1b58856c0685585347537/image4.png" alt="" class="kg-image" width="1113" height="498" loading="lazy"/>
          </figure><p>You can also take a broader approach by blocking all unsafe prompts outright:
<code>If (cf.llm.prompt.unsafe_topic_detected)then Block</code></p>
          <figure class="kg-card kg-image-card">
          <Image src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3uRT9YlRlRPsL5bNyBFA3i/54eb171ecb48aaecc7876b972789bf15/image5.png" alt="" class="kg-image" width="1169" height="628" loading="lazy"/>
          </figure><p>These rules are applied automatically to all discovered HTTP requests containing prompts, ensuring guardrails are enforced consistently across your AI traffic.</p>
    <div class="flex anchor relative">
      <h2 id="whats-next">Whatâs Next</h2>
      <a href="#whats-next" aria-hidden="true" class="relative sm:absolute sm:-left-5">
        <svg width="16" height="16" viewBox="0 0 24 24"><path fill="currentcolor" d="m12.11 15.39-3.88 3.88a2.52 2.52 0 0 1-3.5 0 2.47 2.47 0 0 1 0-3.5l3.88-3.88a1 1 0 0 0-1.42-1.42l-3.88 3.89a4.48 4.48 0 0 0 6.33 6.33l3.89-3.88a1 1 0 1 0-1.42-1.42Zm8.58-12.08a4.49 4.49 0 0 0-6.33 0l-3.89 3.88a1 1 0 0 0 1.42 1.42l3.88-3.88a2.52 2.52 0 0 1 3.5 0 2.47 2.47 0 0 1 0 3.5l-3.88 3.88a1 1 0 1 0 1.42 1.42l3.88-3.89a4.49 4.49 0 0 0 0-6.33ZM8.83 15.17a1 1 0 0 0 1.1.22 1 1 0 0 0 .32-.22l4.92-4.92a1 1 0 0 0-1.42-1.42l-4.92 4.92a1 1 0 0 0 0 1.42Z"></path></svg>
      </a>
    </div>
    <p>In the coming weeks, Firewall for AI will expand to detect prompt injection and jailbreak attempts. We are also exploring how to add more visibility in the analytics and logs, so teams can better validate detection results. A major part of our roadmap is adding model response handling, giving you control over not only what goes into the LLM but also what comes out. Additional abuse controls, such as rate limiting on tokens and support for more safety categories, are also on the way.</p><p>Firewall for AI is available in beta today. If youâre new to Cloudflare and want to explore how to implement these AI protections, <a href="https://www.cloudflare.com/plans/enterprise/contact/?utm_medium=referral&utm_source=blog&utm_campaign=2025-q3-acq-gbl-connectivity-ge-ge-general-ai_week_blog"><u>reach out for a consultation</u></a>. If youâre already with Cloudflare, contact your account team to get access and start testing with real traffic.</p><p>Cloudflare is also opening up a user research program focused on AI security. If you are curious about previews of new functionality or want to help shape our roadmap, <a href="https://www.cloudflare.com/lp/ai-security-user-research-program-2025"><u>express your interest here</u></a>.</p></div></section><section class="post-full-content flex flex-row flex-wrap mw7 center mb4"><div class="post-content lh-copy w-100 gray1 bt b--gray8 pt4">Cloudflare's connectivity cloud protects <a target='_blank' href='https://www.cloudflare.com/network-services/' rel='noreferrer'>entire corporate networks</a>, helps customers build <a target='_blank' href='https://workers.cloudflare.com/' rel='noreferrer'>Internet-scale applications efficiently</a>, accelerates any <a target='_blank' href='https://www.cloudflare.com/performance/accelerate-internet-applications/' rel='noreferrer'>website or Internet application</a>, <a target='_blank' href='https://www.cloudflare.com/ddos/' rel='noreferrer'>wards off DDoS attacks</a>, keeps <a target='_blank' href='https://www.cloudflare.com/application-security/' rel='noreferrer'>hackers at bay</a>, and can help you on <a target='_blank' href='https://www.cloudflare.com/products/zero-trust/' rel='noreferrer'>your journey to Zero Trust</a>.<br/><br/>Visit <a target='_blank' href='https://one.one.one.one/' rel='noreferrer'>1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.<br/><br/>To learn more about our mission to help build a better Internet, <a target='_blank' href='https://www.cloudflare.com/learning/what-is-cloudflare/' rel='noreferrer'>start here</a>. If you&apos;re looking for a new career direction, check out <a target='_blank' href="https://www.cloudflare.com/careers" rel='noreferrer'>our open positions</a>.</div></section><div class="pv2 ph0-l mw7 center" id="social-buttons"><div class="mt5-l mt2 mb4 f2 flex flex-row-ns flex-column flex-wrap"><a id="social-button-hn" title="Discuss on Hacker News" href="https://news.ycombinator.com/submitlink?u=https://blog.cloudflare.com/block-unsafe-llm-prompts-with-firewall-for-ai" target="_blank" rel="noreferrer" class="mr2-ns mb0-l white link b pv3 ph3 mb3 " style="background-color:#0055DC"><svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" viewBox="0 0 512 512" class="mr2"><g><path d="M31,31v450h450V31H31z M270.1,287.6v94.9h-28.1v-94.9L165,143.5h31.9L256,254.3l59.1-110.8H347
C347,143.5,270.1,287.6,270.1,287.6z"></path></g></svg><span class="v-mid">Discuss on Hacker News</span></a></div></div><iframe sandbox="allow-scripts allow-popups allow-popups-to-escape-sandbox" title="cloudflare-tv-live-link" id="cloudflare-tv-embed" src="https://cloudflare.tv/embed/live.html" loading="lazy"></iframe><a href="/tag/ai-week/" class="dib pl2 pr2 pt1 pb1 mb2 bg-gray8 no-underline blue3 f2 mr1">AI Week</a><a href="/tag/security/" class="dib pl2 pr2 pt1 pb1 mb2 bg-gray8 no-underline blue3 f2 mr1">Security</a><a href="/tag/llm/" class="dib pl2 pr2 pt1 pb1 mb2 bg-gray8 no-underline blue3 f2 mr1">LLM</a><a href="/tag/waf/" class="dib pl2 pr2 pt1 pb1 mb2 bg-gray8 no-underline blue3 f2 mr1">WAF</a><a href="/tag/ai/" class="dib pl2 pr2 pt1 pb1 mb2 bg-gray8 no-underline blue3 f2 mr1">AI</a></article></main><div class="ph3 pv3"><div class=" flex flex-row flex-wrap mw7 center"><div class="w-100 bt b--gray8"><p class="black fw5 f4 mt4">Follow on X</p></div><div class="w-100 pb2"><span>Radwa Radwan</span><span class="ph1">|</span><a href="https://x.com/@RadwaRadwan__" class="no-underline">@RadwaRadwan__</a></div><div class="w-100 pb2"><span>Cloudflare</span><span class="ph2">|</span><a href="https://x.com/@cloudflare" class="no-underline">@cloudflare</a></div></div></div><div data-testid="related-posts-section" class="pv4 ph3 ph0-l flex flex-row flex-wrap mw7 center"><div class="w-100 bt b--gray8"><p class="orange fw5 f4 mt4 ttu">Related posts</p></div><article data-testid="related-posts-article" class="w-100 w-100-m w-50-l ph3 mb4"><p data-testid="related-posts-article-date" class="f3 fw5 gray5" data-iso-date="2025-08-27T14:05+00:00">August 27, 2025  2:05 PM</p><a data-testid="related-posts-article-title" href="/ai-gateway-aug-2025-refresh/" class="no-underline gray1 f4 fw5"><h2 class="gray1 f4 fw5 mt2">AI Gateway now gives you access to your favorite AI models, dynamic routing and more â through just one endpoint</h2></a><p data-testid="related-posts-article-excerpt" class="gray1 lh-copy">AI Gateway now gives you access to your favorite AI models, dynamic routing and more â through just one endpoint.<!-- -->...</p><ul class="flex pl0 fw6 f2 mb3 flex flex-wrap"><span>By<!-- -->Â </span><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/michelle/" class="fw5 f2 black no-underline">Michelle Chen</a><span class="fw5 f2 black no-underline">,Â </span></div></li><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/abhishek-kankani/" class="fw5 f2 black no-underline">Abhishek Kankani</a><span class="fw5 f2 black no-underline">,Â </span></div></li><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/mia/" class="fw5 f2 black no-underline">Mia Malden</a></div></li></ul><div data-testid="related-posts-article-tags" class="flex flex-row flex-wrap"><span><a href="/tag/ai-week/" class="no-underline f1 fw2 blue3 underline-hover">AI Week<!-- -->,</a>Â </span><span><a href="/tag/ai-gateway/" class="no-underline f1 fw2 blue3 underline-hover">AI Gateway<!-- -->,</a>Â </span><span><a href="/tag/ai/" class="no-underline f1 fw2 blue3 underline-hover">AI</a>Â </span></div></article><article data-testid="related-posts-article" class="w-100 w-100-m w-50-l ph3 mb4"><p data-testid="related-posts-article-date" class="f3 fw5 gray5" data-iso-date="2025-08-27T14:00+00:00">August 27, 2025  2:00 PM</p><a data-testid="related-posts-article-title" href="/workers-ai-partner-models/" class="no-underline gray1 f4 fw5"><h2 class="gray1 f4 fw5 mt2">State-of-the-art image generation Leonardo models and text-to-speech Deepgram models now available in Workers AI</h2></a><p data-testid="related-posts-article-excerpt" class="gray1 lh-copy">We&#x27;re expanding Workers AI with new partner models from Leonardo.Ai and Deepgram. Start using state-of-the-art image generation models from Leonardo and real-time TTS and STT models from Deepgram. <!-- -->...</p><ul class="flex pl0 fw6 f2 mb3 flex flex-wrap"><span>By<!-- -->Â </span><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/michelle/" class="fw5 f2 black no-underline">Michelle Chen</a><span class="fw5 f2 black no-underline">,Â </span></div></li><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/nikhil/" class="fw5 f2 black no-underline">Nikhil Kothari</a></div></li></ul><div data-testid="related-posts-article-tags" class="flex flex-row flex-wrap"><span><a href="/tag/ai-week/" class="no-underline f1 fw2 blue3 underline-hover">AI Week<!-- -->,</a>Â </span><span><a href="/tag/ai/" class="no-underline f1 fw2 blue3 underline-hover">AI<!-- -->,</a>Â </span><span><a href="/tag/developer-platform/" class="no-underline f1 fw2 blue3 underline-hover">Developer Platform<!-- -->,</a>Â </span><span><a href="/tag/developers/" class="no-underline f1 fw2 blue3 underline-hover">Developers<!-- -->,</a>Â </span><span><a href="/tag/workers/" class="no-underline f1 fw2 blue3 underline-hover">Cloudflare Workers<!-- -->,</a>Â </span><span><a href="/tag/workers-ai/" class="no-underline f1 fw2 blue3 underline-hover">Workers AI</a>Â </span></div></article><article data-testid="related-posts-article" class="w-100 w-100-m w-50-l ph3 mb4"><p data-testid="related-posts-article-date" class="f3 fw5 gray5" data-iso-date="2025-08-27T14:00+00:00">August 27, 2025  2:00 PM</p><a data-testid="related-posts-article-title" href="/how-cloudflare-runs-more-ai-models-on-fewer-gpus/" class="no-underline gray1 f4 fw5"><h2 class="gray1 f4 fw5 mt2">How Cloudflare runs more AI models on fewer GPUs: A technical deep-dive </h2></a><p data-testid="related-posts-article-excerpt" class="gray1 lh-copy">Cloudflare built an internal platform called Omni. This platform uses lightweight isolation and memory over-commitment to run multiple AI models on a single GPU.<!-- -->...</p><ul class="flex pl0 fw6 f2 mb3 flex flex-wrap"><span>By<!-- -->Â </span><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/sven/" class="fw5 f2 black no-underline">Sven Sauleau</a><span class="fw5 f2 black no-underline">,Â </span></div></li><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/mari/" class="fw5 f2 black no-underline">Mari Galicer</a></div></li></ul><div data-testid="related-posts-article-tags" class="flex flex-row flex-wrap"><span><a href="/tag/ai-week/" class="no-underline f1 fw2 blue3 underline-hover">AI Week<!-- -->,</a>Â </span><span><a href="/tag/ai/" class="no-underline f1 fw2 blue3 underline-hover">AI</a>Â </span></div></article><article data-testid="related-posts-article" class="w-100 w-100-m w-50-l ph3 mb4"><p data-testid="related-posts-article-date" class="f3 fw5 gray5" data-iso-date="2025-08-27T14:00+00:00">August 27, 2025  2:00 PM</p><a data-testid="related-posts-article-title" href="/cloudflares-most-efficient-ai-inference-engine/" class="no-underline gray1 f4 fw5"><h2 class="gray1 f4 fw5 mt2">How we built the most efficient inference engine for Cloudflareâs network </h2></a><p data-testid="related-posts-article-excerpt" class="gray1 lh-copy">Infire is an LLM inference engine that employs a range of techniques to maximize resource utilization, allowing us to serve AI models more efficiently with better performance for Cloudflare workloads.<!-- -->...</p><ul class="flex pl0 fw6 f2 mb3 flex flex-wrap"><span>By<!-- -->Â </span><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/vlad-krasnov/" class="fw5 f2 black no-underline">Vlad Krasnov</a><span class="fw5 f2 black no-underline">,Â </span></div></li><li class="list flex items-center" style="white-space:nowrap"><div class="author-name-tooltip"><a href="/author/mari/" class="fw5 f2 black no-underline">Mari Galicer</a></div></li></ul><div data-testid="related-posts-article-tags" class="flex flex-row flex-wrap"><span><a href="/tag/ai-week/" class="no-underline f1 fw2 blue3 underline-hover">AI Week<!-- -->,</a>Â </span><span><a href="/tag/llm/" class="no-underline f1 fw2 blue3 underline-hover">LLM<!-- -->,</a>Â </span><span><a href="/tag/workers-ai/" class="no-underline f1 fw2 blue3 underline-hover">Workers AI</a>Â </span></div></article></div><!--astro:end--></astro-island><footer class="pt4 pb4 pl1 pr1 main-footer"><div class="mw8 center dn db-l ph3"><div class="flex flex-row justify-between"><div class="main-footer__menu-group"><ul id="getting-started-menu" class="list pl0"><li class="pt1 pb1 f1 main-footer__menu-group__header js-toggle-footer-group" data-submenu="getting-started-menu">Getting Started<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/plans/free/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="free-plans" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Free plans</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/enterprise/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="enterprise" class="f1 blue3 no-underline underline-hover" rel="noreferrer">For enterprises</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/plans/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="compare-plans" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Compare plans</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/about-your-website/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="get-a-recommendation" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Get a recommendation</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/plans/enterprise/demo/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="request-a-demo" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Request a demo</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/plans/enterprise/contact/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="contact-sales" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Contact Sales</a></li></ul></div><div class="main-footer__menu-group"><ul id="company-menu" class="list pl0"><li class="pt1 pb1 f1" data-submenu="company-menu">Resources<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/learning/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="learning-center" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Learning Center</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/analysts/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="analysts-report" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Analyst reports</a></li><li class="pt1 pb1"><a href="https://radar.cloudflare.com/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="overview" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Cloudflare Radar</a></li><li class="pt1 pb1"><a href="https://cloudflare.tv/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="tv" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Cloudflare TV</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/case-studies/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="case-studies" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Case Studies</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/resource-hub/?resourcetype=Webinar" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="webinars" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Webinars</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/resource-hub/?resourcetype=Whitepaper" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="white-papers" class="f1 blue3 no-underline underline-hover" rel="noreferrer">White Papers</a></li><li class="pt1 pb1"><a href="https://developers.cloudflare.com" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="developer-docs" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Developer docs</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/the-net/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="theNet" class="f1 blue3 no-underline underline-hover" rel="noreferrer">theNet</a></li></ul></div><div class="main-footer__menu-group"><ul id="sales-menu" class="list pl0"><li class="pt1 pb1 f1 main-footer__menu-group__header js-toggle-footer-group" data-submenu="sales-menu">Solutions<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/connectivity-cloud/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="connectivity-cloud" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Connectivity cloud</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/zero-trust/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="zero-trust" class="f1 blue3 no-underline underline-hover" rel="noreferrer">SSE and SASE services</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/application-services/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="application-services" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Application services</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/network-services/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="network-services" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Network services</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/developer-platform/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="developer-services" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Developer services</a></li></ul></div><div class="main-footer__menu-group"><ul id="community-menu" class="list pl0"><li class="pt1 pb1 f1 main-footer__menu-group__header js-toggle-footer-group" data-submenu="community-menu">Community<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://community.cloudflare.com" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="community_hub" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Community Hub</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/galileo/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="galileo" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Project Galileo</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/athenian/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="athenian" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Athenian Project</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/campaigns/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="cloudflare-for-campaigns" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Cloudflare for Campaigns</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/partners/technology-partners/cidp/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="critical-infrastructure-defense-project" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Critical Infrastructure Defense Project</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/connect2024/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="connect-2024" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Connect 2024</a></li></ul></div><div class="main-footer__menu-group"><ul id="support-menu" class="list pl0"><li class="pt1 pb1 f1 main-footer__menu-group__header js-toggle-footer-group" data-submenu="support-menu">Support<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://support.cloudflare.com" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="help-center" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Help center</a></li><li class="pt1 pb1"><a href="https://www.cloudflarestatus.com" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="status" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Cloudflare Status</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/compliance/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="compliance" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Compliance</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/gdpr/introduction/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="gdpr" class="f1 blue3 no-underline underline-hover" rel="noreferrer">GDPR</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/trust-hub/abuse-approach/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="trust-and-safety" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Trust &amp; Safety</a></li></ul></div><div class="main-footer__menu-group"><ul id="company-menu" class="list pl0"><li class="pt1 pb1 f1 main-footer__menu-group__header js-toggle-footer-group" data-submenu="company-menu">Company<i class="icon-caret-down"></i></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/about-overview/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="overview" class="f1 blue3 no-underline underline-hover" rel="noreferrer">About Cloudflare</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/people/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="our_team" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Our team</a></li><li class="pt1 pb1"><a href="https://cloudflare.net/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="investor-relations" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Investor relations</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/press/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="press" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Press</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/careers/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="careers" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Careers</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/diversity-equity-and-inclusion/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="diversity-equity-inclusion" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Diversity, equity &amp; inclusion</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/impact/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="impact-ESG" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Impact/ESG</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/network/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="network_map" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Network Map</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/press-kit/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="press-kit" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Logos &amp; press kit</a></li><li class="pt1 pb1"><a href="https://www.cloudflare.com/partners/" target="_blank" data-tracking-category="footer" data-tracking-action="click" data-tracking-label="partners" class="f1 blue3 no-underline underline-hover" rel="noreferrer">Become a partner</a></li></ul></div></div></div><div class="mw8 center ph3"><div class="flex flex-row flex-wrap justify-center md:justify-between items-center pt4"><div class="flex flex-row space-x-4 items-start w-25-l pb4 pb0-l"><a target="_blank" rel="noreferrer" href="https://www.facebook.com/Cloudflare/" class="w-8"><img class="w-8" src="https://www.cloudflare.com/img/footer/facebook.svg" alt="facebook"/></a><a target=" _blank" rel="noreferrer" href="https://x.com/Cloudflare" class="w-8"><img class="w-8" src="https://www.cloudflare.com/img/footer/twitter.svg" alt="X"/></a><a target="_blank" rel="noreferrer" href="https://www.linkedin.com/company/cloudflare" class="w-8"><img class="w-8" src="https://www.cloudflare.com/img/footer/linkedin.svg" alt="linkedin"/></a><a target="_blank" rel="noreferrer" href="https://www.youtube.com/cloudflare" class="w-8"><img class="w-8" src="https://www.cloudflare.com/img/footer/youtube.svg" alt="youtube"/></a><a target="_blank" rel="noreferrer" href="https://www.instagram.com/cloudflare" class="w-8"><img class="w-8" src="https://www.cloudflare.com/img/footer/instagram.svg" alt="instagram"/></a></div><div class="w-70-l tr-l tl-ns"><div><span class="main-footer__copyright f1">Â© <!-- -->2025<!-- --> Cloudflare, Inc.<!-- --> </span><span class="main-footer__copyright f1">|</span><a href="https://www.cloudflare.com/privacypolicy/" target="_blank" class="main-footer__copyright f1 no-underline underline-hover" rel="noreferrer"> <!-- -->Privacy Policy<!-- --> </a><span class="main-footer__copyright f1">|</span><a href="https://www.cloudflare.com/website-terms/" target="_blank" class="main-footer__copyright f1 no-underline underline-hover" rel="noreferrer"> <!-- -->Terms of Use<!-- --> </a><span class="main-footer__copyright f1">|</span><a href="https://www.cloudflare.com/disclosure/" target="_blank" class="main-footer__copyright f1 no-underline underline-hover" rel="noreferrer"> <!-- -->Report Security Issues<!-- --> </a><span class="main-footer__copyright f1">|</span><img class="mw2 ph1" src="/images/privacy-options.svg" alt="Privacy Options"/><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings main-footer__copyright f1 no-underline underline-hover"><span class="brandGray5">Cookie Preferences</span> </a><span class="main-footer__copyright f1">|</span><a href="https://www.cloudflare.com/trademark/" target="_blank" class="main-footer__copyright f1 no-underline underline-hover" rel="noreferrer"> <!-- -->Trademark<!-- --> </a></div></div></div></div></footer></html><script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"rayId":"9760c2bb1826316d","version":"2025.8.0","serverTiming":{"name":{"cfExtPri":true,"cfEdge":true,"cfOrigin":true,"cfL4":true,"cfSpeedBrain":true,"cfCacheStatus":true}},"token":"2bc156e5f250476cb274d269511ffb57","b":1}' crossorigin="anonymous"></script>
